import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import pandas as pd
from utils import DataLoader, TextPreprocessor
from models import AutoencoderSummarizer
from nltk.tokenize import sent_tokenize

def test_autoencoder_train_debug():
    """Test de debug pour identifier le problÃ¨me dans la fonction train"""
    print("ðŸ” DEBUG DE LA FONCTION TRAIN DE L'AUTOENCODEUR")
    print("=" * 60)
    
    # 1. Charger les donnÃ©es
    dataset_path = r'D:\MES_Documents\Fac\NLP\app\data\enriched_dataset_paragraphs_2.csv'
    loader = DataLoader(dataset_path)
    texts, labels = loader.get_texts_and_labels()
    
    # 2. PrÃ©traitement
    preprocessor = TextPreprocessor()
    autoencoder_texts = preprocessor.transform_for_autoencoder(texts)
    
    print(f"ðŸ“ {len(autoencoder_texts)} textes prÃ©traitÃ©s")
    
    # 3. CrÃ©er l'autoencodeur
    autoencoder = AutoencoderSummarizer()
    
    # 4. Debug de la fonction train
    print("\nðŸ”§ DEBUG DE LA FONCTION TRAIN")
    print("-" * 40)
    
    # Test du tokenizer
    print("ðŸ“¤ Test du tokenizer...")
    if not autoencoder.tokenizer.is_fitted:
        print("ðŸ”„ EntraÃ®nement du tokenizer partagÃ©...")
        all_sentences_for_tokenizer = []
        for text in autoencoder_texts[:100]:  # Test sur 100 textes
            try:
                sentences = sent_tokenize(text)
                all_sentences_for_tokenizer.extend(sentences)
            except Exception as e:
                print(f"   âŒ Erreur tokenisation: {e}")
        
        print(f"   Phrases pour tokenizer: {len(all_sentences_for_tokenizer)}")
        
        if len(all_sentences_for_tokenizer) > 0:
            autoencoder.tokenizer.fit_on_texts(all_sentences_for_tokenizer)
            print(f"âœ… Tokenizer entraÃ®nÃ© sur {len(all_sentences_for_tokenizer)} phrases")
        else:
            print("âŒ Aucune phrase trouvÃ©e pour entraÃ®ner le tokenizer")
            return
    else:
        print("âœ… Tokenizer dÃ©jÃ  entraÃ®nÃ©")
    
    # Test du prÃ©traitement des phrases
    print("\nðŸ“ Test du prÃ©traitement des phrases...")
    all_sentences = []
    all_sentence_vectors = []
    
    for i, text in enumerate(autoencoder_texts[:10]):  # Test sur 10 textes
        print(f"   Texte {i+1}:")
        print(f"     Longueur: {len(text)} caractÃ¨res")
        
        try:
            sentence_vectors, original_sentences = autoencoder.preprocess_sentences(text)
            print(f"     Phrases originales: {len(original_sentences)}")
            print(f"     Phrases vectorisÃ©es: {len(sentence_vectors)}")
            
            # Debug de la condition problÃ©matique
            if len(sentence_vectors) > 0:
                print(f"     âœ… Condition len(sentence_vectors) > 0: VRAI")
                all_sentences.extend(original_sentences)
                all_sentence_vectors.extend(sentence_vectors)
            else:
                print(f"     âŒ Condition len(sentence_vectors) > 0: FAUX")
                
        except Exception as e:
            print(f"     âŒ Erreur lors du prÃ©traitement: {e}")
    
    print(f"\nðŸ“Š RÃ‰SULTATS DU DEBUG:")
    print(f"   Total phrases collectÃ©es: {len(all_sentences)}")
    print(f"   Total vecteurs collectÃ©s: {len(all_sentence_vectors)}")
    
    if len(all_sentences) >= 10:
        print("âœ… Suffisamment de phrases pour l'entraÃ®nement")
        print("   Le problÃ¨me n'est pas dans le prÃ©traitement")
    else:
        print("âŒ Pas assez de phrases pour l'entraÃ®nement")
        print("   Le problÃ¨me est dans le prÃ©traitement")

def test_text_preprocessing_difference():
    """Test pour vÃ©rifier la diffÃ©rence entre les prÃ©traitements"""
    print("\nðŸ” TEST DE LA DIFFÃ‰RENCE ENTRE PRÃ‰TRAITEMENTS")
    print("=" * 60)
    
    # 1. Charger les donnÃ©es
    dataset_path = r'D:\MES_Documents\Fac\NLP\app\data\enriched_dataset_paragraphs_2.csv'
    loader = DataLoader(dataset_path)
    texts, labels = loader.get_texts_and_labels()
    
    # 2. PrÃ©traitement normal
    preprocessor = TextPreprocessor()
    processed_texts = preprocessor.transform(texts)
    
    # 3. PrÃ©traitement pour autoencodeur
    autoencoder_texts = preprocessor.transform_for_autoencoder(texts)
    
    print(f"ðŸ“ {len(texts)} textes originaux")
    print(f"ðŸ“ {len(processed_texts)} textes prÃ©traitÃ©s normaux")
    print(f"ðŸ“ {len(autoencoder_texts)} textes prÃ©traitÃ©s pour autoencodeur")
    
    # Comparer les premiers textes
    print("\nðŸ“‹ COMPARAISON DES PRÃ‰TRAITEMENTS:")
    for i in range(3):
        print(f"\nðŸ“„ Texte {i+1}:")
        print(f"   Original: {texts.iloc[i][:100]}...")
        print(f"   Normal: {processed_texts[i][:100]}...")
        print(f"   Autoencoder: {autoencoder_texts[i][:100]}...")
        
        # Test de tokenisation
        try:
            sentences_normal = sent_tokenize(processed_texts[i])
            sentences_autoencoder = sent_tokenize(autoencoder_texts[i])
            print(f"   Phrases (normal): {len(sentences_normal)}")
            print(f"   Phrases (autoencoder): {len(sentences_autoencoder)}")
        except Exception as e:
            print(f"   âŒ Erreur tokenisation: {e}")

def test_autoencoder_train_exact():
    """Test qui reproduit exactement la fonction train de l'autoencodeur"""
    print("\nðŸ” TEST EXACT DE LA FONCTION TRAIN")
    print("=" * 60)
    
    # 1. Charger les donnÃ©es comme dans le script d'entraÃ®nement
    dataset_path = r'D:\MES_Documents\Fac\NLP\app\data\enriched_dataset_paragraphs_2.csv'
    loader = DataLoader(dataset_path)
    texts, labels = loader.get_texts_and_labels()
    
    print(f"ðŸ“ {len(texts)} textes originaux chargÃ©s")
    
    # 2. PrÃ©traitement spÃ©cial pour l'autoencodeur (comme dans train_autoencoder)
    print("\nðŸ”§ PrÃ©traitement spÃ©cial pour l'autoencodeur...")
    preprocessor = TextPreprocessor()
    autoencoder_texts = preprocessor.transform_for_autoencoder(texts)
    
    print(f"ðŸ“ {len(autoencoder_texts)} textes prÃ©traitÃ©s pour l'autoencodeur")
    
    # 3. CrÃ©er l'autoencodeur
    autoencoder = AutoencoderSummarizer()
    
    # 4. Reproduire exactement la fonction train
    print("\nðŸ”„ Reproduction de la fonction train...")
    print("ðŸ”„ PrÃ©paration des donnÃ©es pour l'autoencodeur...")
    
    # EntraÃ®ner le tokenizer partagÃ© si nÃ©cessaire
    if not autoencoder.tokenizer.is_fitted:
        print("ðŸ”„ EntraÃ®nement du tokenizer partagÃ©...")
        all_sentences_for_tokenizer = []
        for text in autoencoder_texts:
            sentences = sent_tokenize(text)
            all_sentences_for_tokenizer.extend(sentences)
        
        print(f"   Phrases pour tokenizer: {len(all_sentences_for_tokenizer)}")
        
        if len(all_sentences_for_tokenizer) > 0:
            autoencoder.tokenizer.fit_on_texts(all_sentences_for_tokenizer)
            print(f"âœ… Tokenizer entraÃ®nÃ© sur {len(all_sentences_for_tokenizer)} phrases")
        else:
            print("âŒ Aucune phrase trouvÃ©e pour entraÃ®ner le tokenizer")
            return
    else:
        print("âœ… Tokenizer dÃ©jÃ  entraÃ®nÃ©")
    
    # Collecter toutes les phrases
    all_sentences = []
    all_sentence_vectors = []
    
    print(f"\nðŸ“ Traitement de {len(autoencoder_texts)} textes...")
    
    for i, text in enumerate(autoencoder_texts):
        if i % 1000 == 0:  # Afficher le progrÃ¨s
            print(f"   Traitement du texte {i+1}/{len(autoencoder_texts)}")
        
        sentence_vectors, original_sentences = autoencoder.preprocess_sentences(text)
        
        # Debug de la condition problÃ©matique
        if len(sentence_vectors) > 0:
            all_sentences.extend(original_sentences)
            all_sentence_vectors.extend(sentence_vectors)
    
    print(f"\nðŸ“Š RÃ‰SULTATS FINAUX:")
    print(f"   Phrases trouvÃ©es: {len(all_sentences)}")
    print(f"   Textes analysÃ©s: {len(autoencoder_texts)}")
    
    if len(all_sentences) < 10:
        print("âš ï¸ Pas assez de phrases pour entraÃ®ner l'autoencodeur")
        return
    else:
        print("âœ… Suffisamment de phrases pour l'entraÃ®nement")
        print(f"   Forme des donnÃ©es: {len(all_sentence_vectors)} phrases")

if __name__ == "__main__":
    test_autoencoder_train_debug()
    test_text_preprocessing_difference()
    test_autoencoder_train_exact() 