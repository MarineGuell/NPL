#!/usr/bin/env python3
"""
Test de diagnostic pour l'autoencodeur - pourquoi pas assez de phrases ?
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import pandas as pd
from nltk.tokenize import sent_tokenize
import nltk

def test_dataset_analysis():
    """Analyse du dataset pour comprendre le probl√®me"""
    print("üîç Diagnostic du dataset pour l'autoencodeur")
    print("=" * 60)
    
    # Charger le dataset
    dataset_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), "data", "enriched_dataset_paragraphs.csv")
    
    if not os.path.exists(dataset_path):
        print(f"‚ùå Dataset non trouv√©: {dataset_path}")
        return
    
    print(f"üìÅ Chargement du dataset: {dataset_path}")
    df = pd.read_csv(dataset_path)
    
    print(f"üìä Taille du dataset: {len(df)} lignes")
    print(f"üìã Colonnes: {list(df.columns)}")
    
    # Analyser la colonne de texte
    text_column = None
    for col in df.columns:
        if 'text' in col.lower() or 'content' in col.lower() or 'paragraph' in col.lower():
            text_column = col
            break
    
    if text_column is None:
        print("‚ùå Colonne de texte non trouv√©e")
        print(f"   Colonnes disponibles: {list(df.columns)}")
        return
    
    print(f"üìù Colonne de texte identifi√©e: '{text_column}'")
    
    # Analyser les textes
    texts = df[text_column].dropna()
    print(f"üìù Textes non-null: {len(texts)}")
    
    # T√©l√©chargement automatique de punkt si n√©cessaire
    try:
        nltk.data.find('tokenizers/punkt')
    except LookupError:
        print("üì• T√©l√©chargement automatique de punkt...")
        nltk.download('punkt', quiet=True)
    
    # Analyser la tokenisation en phrases
    print("\nüîç ANALYSE DE LA TOKENISATION EN PHRASES")
    print("-" * 40)
    
    total_sentences = 0
    texts_with_sentences = 0
    sentence_lengths = []
    
    for i, text in enumerate(texts[:100]):  # Analyser les 100 premiers textes
        if pd.isna(text) or text == '':
            continue
            
        sentences = sent_tokenize(str(text))
        sentence_count = len(sentences)
        
        if sentence_count > 0:
            texts_with_sentences += 1
            total_sentences += sentence_count
            sentence_lengths.extend([len(s) for s in sentences])
        
        # Afficher quelques exemples
        if i < 5:
            print(f"\nüìÑ Texte {i+1}:")
            print(f"   Longueur: {len(str(text))} caract√®res")
            print(f"   Phrases d√©tect√©es: {sentence_count}")
            if sentence_count > 0:
                print(f"   Premi√®re phrase: {sentences[0][:100]}...")
            else:
                print(f"   Contenu: {str(text)[:100]}...")
    
    print(f"\nüìä STATISTIQUES:")
    print(f"   Textes avec phrases: {texts_with_sentences}/100")
    print(f"   Total phrases d√©tect√©es: {total_sentences}")
    print(f"   Moyenne phrases par texte: {total_sentences/texts_with_sentences if texts_with_sentences > 0 else 0:.2f}")
    
    if sentence_lengths:
        print(f"   Longueur moyenne des phrases: {sum(sentence_lengths)/len(sentence_lengths):.1f} caract√®res")
        print(f"   Longueur min/max des phrases: {min(sentence_lengths)}/{max(sentence_lengths)} caract√®res")
    
    # Analyser la ponctuation
    print("\nüîç ANALYSE DE LA PONCTUATION")
    print("-" * 40)
    
    punctuation_stats = {}
    for i, text in enumerate(texts[:50]):  # Analyser les 50 premiers textes
        if pd.isna(text) or text == '':
            continue
            
        text_str = str(text)
        for punct in ['.', '!', '?', ';', ':']:
            count = text_str.count(punct)
            punctuation_stats[punct] = punctuation_stats.get(punct, 0) + count
    
    print("üìä Occurrences de ponctuation:")
    for punct, count in punctuation_stats.items():
        print(f"   '{punct}': {count} occurrences")
    
    # Test avec le pr√©traitement de l'autoencodeur
    print("\nüîç TEST DU PR√âTRAITEMENT AUTOENCODEUR")
    print("-" * 40)
    
    from utils import TextPreprocessor
    preprocessor = TextPreprocessor()
    
    processed_texts = []
    for i, text in enumerate(texts[:20]):  # Tester les 20 premiers textes
        if pd.isna(text) or text == '':
            continue
            
        processed_text = preprocessor.clean_for_autoencoder(str(text))
        processed_texts.append(processed_text)
        
        if i < 3:
            print(f"\nüìÑ Texte {i+1} (apr√®s pr√©traitement):")
            print(f"   Original: {str(text)[:100]}...")
            print(f"   Trait√©: {processed_text[:100]}...")
            
            # Tokenisation apr√®s pr√©traitement
            sentences_after = sent_tokenize(processed_text)
            print(f"   Phrases apr√®s pr√©traitement: {len(sentences_after)}")
    
    # Analyser les textes pr√©trait√©s
    total_sentences_after = 0
    for text in processed_texts:
        sentences = sent_tokenize(text)
        total_sentences_after += len(sentences)
    
    print(f"\nüìä R√âSULTATS APR√àS PR√âTRAITEMENT:")
    print(f"   Total phrases apr√®s pr√©traitement: {total_sentences_after}")
    print(f"   Moyenne phrases par texte: {total_sentences_after/len(processed_texts):.2f}")

def test_autoencoder_preprocessing():
    """Test du pr√©traitement de l'autoencodeur"""
    print("\nüîç TEST DU PR√âTRAITEMENT DE L'AUTOENCODEUR")
    print("=" * 50)
    
    from models import AutoencoderSummarizer
    
    # Cr√©er une instance de l'autoencodeur
    autoencoder = AutoencoderSummarizer()
    
    # Test avec un texte simple
    test_text = "Premi√®re phrase. Deuxi√®me phrase. Troisi√®me phrase."
    print(f"üìÑ Test 1: '{test_text}'")
    
    sentence_vectors, original_sentences = autoencoder.preprocess_sentences(test_text)
    print(f"   Phrases originales: {len(original_sentences)}")
    print(f"   Phrases vectoris√©es: {len(sentence_vectors)}")
    if len(original_sentences) > 0:
        print(f"   Premi√®re phrase: {original_sentences[0]}")
    
    # Test avec un texte d'une seule phrase
    test_text2 = "Une seule phrase."
    print(f"\nüìÑ Test 2: '{test_text2}'")
    
    sentence_vectors2, original_sentences2 = autoencoder.preprocess_sentences(test_text2)
    print(f"   Phrases originales: {len(original_sentences2)}")
    print(f"   Phrases vectoris√©es: {len(sentence_vectors2)}")
    
    # Test avec un texte vide
    test_text3 = ""
    print(f"\nüìÑ Test 3: '{test_text3}'")
    
    sentence_vectors3, original_sentences3 = autoencoder.preprocess_sentences(test_text3)
    print(f"   Phrases originales: {len(original_sentences3)}")
    print(f"   Phrases vectoris√©es: {len(sentence_vectors3)}")

def test_sentence_tokenization():
    """Test de diagnostic pour la tokenisation des phrases"""
    print("üîç DIAGNOSTIC DE LA TOKENISATION DES PHRASES")
    print("=" * 50)
    
    # Charger le dataset
    try:
        df = pd.read_csv(r'D:\MES_Documents\Fac\NLP\app\data\enriched_dataset_paragraphs_2.csv')
        print(f"‚úÖ Dataset charg√©: {len(df)} textes")
    except Exception as e:
        print(f"‚ùå Erreur lors du chargement du dataset: {e}")
        return
    
    # T√©l√©charger punkt si n√©cessaire
    try:
        nltk.data.find('tokenizers/punkt')
        print("‚úÖ punkt d√©j√† disponible")
    except LookupError:
        print("üì• T√©l√©chargement de punkt...")
        nltk.download('punkt', quiet=True)
        print("‚úÖ punkt t√©l√©charg√©")
    
    # Tester sur quelques textes
    total_sentences = 0
    texts_with_sentences = 0
    
    for i, text in enumerate(df['text'].head(10)):
        print(f"\nüìù Texte {i+1}:")
        print(f"   Longueur: {len(text)} caract√®res")
        print(f"   D√©but: {text[:100]}...")
        
        try:
            sentences = sent_tokenize(text)
            print(f"   Phrases trouv√©es: {len(sentences)}")
            
            if len(sentences) > 0:
                texts_with_sentences += 1
                total_sentences += len(sentences)
                print(f"   Premi√®re phrase: {sentences[0][:50]}...")
            else:
                print(f"   ‚ö†Ô∏è Aucune phrase d√©tect√©e!")
                
        except Exception as e:
            print(f"   ‚ùå Erreur lors de la tokenisation: {e}")
    
    print(f"\nüìä R√âSULTATS DU DIAGNOSTIC:")
    print(f"   Textes avec phrases: {texts_with_sentences}/10")
    print(f"   Total phrases trouv√©es: {total_sentences}")
    
    # Tester sur l'ensemble du dataset
    print(f"\nüîÑ Test sur l'ensemble du dataset...")
    all_sentences = []
    for text in df['text']:
        try:
            sentences = sent_tokenize(text)
            all_sentences.extend(sentences)
        except Exception as e:
            continue
    
    print(f"   Total phrases dans le dataset: {len(all_sentences)}")
    print(f"   Textes analys√©s: {len(df)}")

if __name__ == "__main__":
    test_dataset_analysis()
    test_autoencoder_preprocessing()
    test_sentence_tokenization() 