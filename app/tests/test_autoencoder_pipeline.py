import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import pandas as pd
from utils import DataLoader, TextPreprocessor
from models import AutoencoderSummarizer

def test_complete_autoencoder_pipeline():
    """Test complet du pipeline de l'autoencodeur"""
    print("ğŸ” TEST COMPLET DU PIPELINE AUTOENCODEUR")
    print("=" * 60)
    
    # 1. Test du chargement des donnÃ©es
    print("\nğŸ“š 1. TEST DU CHARGEMENT DES DONNÃ‰ES")
    print("-" * 40)
    
    dataset_path = r'D:\MES_Documents\Fac\NLP\app\data\enriched_dataset_paragraphs_2.csv'
    
    try:
        loader = DataLoader(dataset_path)
        texts, labels = loader.get_texts_and_labels()
        print(f"âœ… Dataset chargÃ© avec succÃ¨s")
        print(f"   Textes: {len(texts)}")
        print(f"   CatÃ©gories: {len(set(labels))}")
        print(f"   CatÃ©gories: {list(set(labels))}")
    except Exception as e:
        print(f"âŒ Erreur lors du chargement: {e}")
        return
    
    # 2. Test du prÃ©traitement pour l'autoencodeur
    print("\nğŸ”§ 2. TEST DU PRÃ‰TRAITEMENT POUR L'AUTOENCODEUR")
    print("-" * 40)
    
    try:
        preprocessor = TextPreprocessor()
        autoencoder_texts = preprocessor.transform_for_autoencoder(texts)
        print(f"âœ… PrÃ©traitement rÃ©ussi")
        print(f"   Textes prÃ©traitÃ©s: {len(autoencoder_texts)}")
        
        # Afficher quelques exemples
        print("\nğŸ“‹ Exemples de textes prÃ©traitÃ©s:")
        for i, text in enumerate(autoencoder_texts[:3]):
            print(f"   {i+1}. {text[:100]}...")
            
    except Exception as e:
        print(f"âŒ Erreur lors du prÃ©traitement: {e}")
        return
    
    # 3. Test de la tokenisation des phrases
    print("\nğŸ”¤ 3. TEST DE LA TOKENISATION DES PHRASES")
    print("-" * 40)
    
    from nltk.tokenize import sent_tokenize
    import nltk
    
    # TÃ©lÃ©charger punkt si nÃ©cessaire
    try:
        nltk.data.find('tokenizers/punkt')
        print("âœ… punkt disponible")
    except LookupError:
        print("ğŸ“¥ TÃ©lÃ©chargement de punkt...")
        nltk.download('punkt', quiet=True)
        print("âœ… punkt tÃ©lÃ©chargÃ©")
    
    # Tester la tokenisation sur quelques textes
    total_sentences = 0
    texts_with_sentences = 0
    
    for i, text in enumerate(autoencoder_texts[:10]):
        try:
            sentences = sent_tokenize(text)
            if len(sentences) > 0:
                texts_with_sentences += 1
                total_sentences += len(sentences)
                if i < 3:
                    print(f"   Texte {i+1}: {len(sentences)} phrases")
        except Exception as e:
            print(f"   âŒ Erreur sur le texte {i+1}: {e}")
    
    print(f"âœ… Tokenisation testÃ©e")
    print(f"   Textes avec phrases: {texts_with_sentences}/10")
    print(f"   Total phrases trouvÃ©es: {total_sentences}")
    
    # 4. Test de l'autoencodeur
    print("\nğŸ¤– 4. TEST DE L'AUTOENCODEUR")
    print("-" * 40)
    
    try:
        autoencoder = AutoencoderSummarizer()
        print("âœ… Autoencodeur crÃ©Ã©")
        
        # Test du prÃ©traitement de l'autoencodeur sur un exemple
        test_text = autoencoder_texts[0]
        print(f"\nğŸ“ Test sur le premier texte:")
        print(f"   Longueur: {len(test_text)} caractÃ¨res")
        print(f"   DÃ©but: {test_text[:100]}...")
        
        sentence_vectors, original_sentences = autoencoder.preprocess_sentences(test_text)
        print(f"   Phrases vectorisÃ©es: {len(sentence_vectors)}")
        print(f"   Phrases originales: {len(original_sentences)}")
        
        if len(sentence_vectors) > 0:
            print(f"   Forme des vecteurs: {sentence_vectors.shape}")
            print("âœ… PrÃ©traitement de l'autoencodeur fonctionne")
        else:
            print("âš ï¸ Aucune phrase vectorisÃ©e (peut Ãªtre normal si < 2 phrases)")
            
    except Exception as e:
        print(f"âŒ Erreur lors du test de l'autoencodeur: {e}")
        return
    
    # 5. Test d'entraÃ®nement (optionnel, rapide)
    print("\nğŸ¯ 5. TEST D'ENTRAÃNEMENT RAPIDE")
    print("-" * 40)
    
    try:
        # Utiliser seulement les 100 premiers textes pour un test rapide
        test_texts = autoencoder_texts[:100]
        print(f"ğŸ“ Test d'entraÃ®nement sur {len(test_texts)} textes...")
        
        # Compter les phrases disponibles
        all_sentences = []
        for text in test_texts:
            try:
                sentences = sent_tokenize(text)
                if len(sentences) >= 2:  # Au moins 2 phrases par texte
                    all_sentences.extend(sentences)
            except:
                continue
        
        print(f"   Phrases disponibles pour l'entraÃ®nement: {len(all_sentences)}")
        
        if len(all_sentences) >= 10:
            print("âœ… Suffisamment de phrases pour l'entraÃ®nement")
            print("   (Test d'entraÃ®nement complet disponible)")
        else:
            print("âš ï¸ Pas assez de phrases pour l'entraÃ®nement")
            print(f"   NÃ©cessaire: 10, Disponible: {len(all_sentences)}")
            
    except Exception as e:
        print(f"âŒ Erreur lors du test d'entraÃ®nement: {e}")
    
    print("\n" + "=" * 60)
    print("ğŸ‰ TEST DU PIPELINE TERMINÃ‰")
    print("=" * 60)
    print("ğŸ’¡ Pour entraÃ®ner l'autoencodeur complet:")
    print("   python app/train_models.py --model autoencoder")

if __name__ == "__main__":
    test_complete_autoencoder_pipeline() 