#!/usr/bin/env python3
"""
Test pour analyser les contraintes de l'autoencodeur sur la longueur des textes.
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from models import AutoencoderSummarizer
from nltk.tokenize import sent_tokenize
import nltk

def test_autoencoder_constraints():
    """Test des contraintes de l'autoencodeur"""
    print("ğŸ” Test des contraintes de l'autoencodeur")
    print("=" * 50)
    
    # TÃ©lÃ©chargement automatique de punkt si nÃ©cessaire
    try:
        nltk.data.find('tokenizers/punkt')
    except LookupError:
        print("ğŸ“¥ TÃ©lÃ©chargement automatique de punkt...")
        nltk.download('punkt', quiet=True)
    
    autoencoder = AutoencoderSummarizer()
    
    # Test 1: Textes de diffÃ©rentes longueurs
    test_texts = [
        ("Texte trÃ¨s court", "Une seule phrase."),
        ("Texte court", "PremiÃ¨re phrase. DeuxiÃ¨me phrase."),
        ("Texte moyen", "PremiÃ¨re phrase. DeuxiÃ¨me phrase. TroisiÃ¨me phrase. QuatriÃ¨me phrase."),
        ("Texte long", "PremiÃ¨re phrase. DeuxiÃ¨me phrase. TroisiÃ¨me phrase. QuatriÃ¨me phrase. CinquiÃ¨me phrase. SixiÃ¨me phrase. SeptiÃ¨me phrase. HuitiÃ¨me phrase. NeuviÃ¨me phrase. DixiÃ¨me phrase."),
        ("Texte trÃ¨s long", "PremiÃ¨re phrase. DeuxiÃ¨me phrase. TroisiÃ¨me phrase. QuatriÃ¨me phrase. CinquiÃ¨me phrase. SixiÃ¨me phrase. SeptiÃ¨me phrase. HuitiÃ¨me phrase. NeuviÃ¨me phrase. DixiÃ¨me phrase. OnziÃ¨me phrase. DouziÃ¨me phrase. TreiziÃ¨me phrase. QuatorziÃ¨me phrase. QuinziÃ¨me phrase.")
    ]
    
    print("\n1. ANALYSE DES CONTRAINTES DE LONGUEUR")
    print("-" * 40)
    
    for name, text in test_texts:
        print(f"\nğŸ“„ Test: {name}")
        print(f"ğŸ“ Texte: {text[:50]}...")
        
        # Analyse du texte
        sentences = sent_tokenize(text)
        print(f"ğŸ“Š Nombre de phrases: {len(sentences)}")
        
        # Test de prÃ©processing
        sentence_vectors, original_sentences = autoencoder.preprocess_sentences(text)
        print(f"ğŸ”§ Phrases vectorisÃ©es: {len(sentence_vectors)}")
        
        # VÃ©rification des contraintes
        if len(sentences) < 2:
            print("âš ï¸  Contrainte: Moins de 2 phrases - pas de rÃ©sumÃ© possible")
        elif len(sentence_vectors) == 0:
            print("âš ï¸  Contrainte: Aucune phrase vectorisÃ©e")
        else:
            print("âœ… Texte valide pour l'autoencodeur")
    
    # Test 2: Contraintes d'entraÃ®nement
    print("\n2. CONTRAINTES D'ENTRAÃNEMENT")
    print("-" * 40)
    
    print("ğŸ“‹ Contraintes identifiÃ©es dans le code:")
    print("   â€¢ Minimum 10 phrases pour l'entraÃ®nement (ligne 800)")
    print("   â€¢ Minimum 2 phrases par texte pour le prÃ©processing (ligne 720)")
    print("   â€¢ Longueur maximale des phrases: 50 tokens (ligne 683)")
    print("   â€¢ Taille du vocabulaire: 5000 mots (ligne 683)")
    
    # Test 3: Simulation d'entraÃ®nement
    print("\n3. SIMULATION D'ENTRAÃNEMENT")
    print("-" * 40)
    
    # CrÃ©er des textes de test avec diffÃ©rentes longueurs
    short_texts = ["Phrase unique."] * 5
    medium_texts = ["PremiÃ¨re phrase. DeuxiÃ¨me phrase."] * 5
    long_texts = ["PremiÃ¨re phrase. DeuxiÃ¨me phrase. TroisiÃ¨me phrase. QuatriÃ¨me phrase. CinquiÃ¨me phrase."] * 5
    
    print("ğŸ“Š Test avec textes courts (1 phrase chacun):")
    total_sentences = sum(len(sent_tokenize(text)) for text in short_texts)
    print(f"   Total phrases: {total_sentences}")
    print(f"   RÃ©sultat: {'âŒ Ã‰CHEC' if total_sentences < 10 else 'âœ… SUCCÃˆS'}")
    
    print("ğŸ“Š Test avec textes moyens (2 phrases chacun):")
    total_sentences = sum(len(sent_tokenize(text)) for text in medium_texts)
    print(f"   Total phrases: {total_sentences}")
    print(f"   RÃ©sultat: {'âŒ Ã‰CHEC' if total_sentences < 10 else 'âœ… SUCCÃˆS'}")
    
    print("ğŸ“Š Test avec textes longs (5 phrases chacun):")
    total_sentences = sum(len(sent_tokenize(text)) for text in long_texts)
    print(f"   Total phrases: {total_sentences}")
    print(f"   RÃ©sultat: {'âŒ Ã‰CHEC' if total_sentences < 10 else 'âœ… SUCCÃˆS'}")
    
    # Test 4: Recommandations
    print("\n4. RECOMMANDATIONS")
    print("-" * 40)
    
    print("ğŸ¯ Pour un bon apprentissage de l'autoencodeur:")
    print("   â€¢ Minimum 10 phrases au total dans le dataset")
    print("   â€¢ IdÃ©alement 2-5 phrases par texte")
    print("   â€¢ Textes de longueur moyenne (pas trop courts, pas trop longs)")
    print("   â€¢ DiversitÃ© dans le contenu des phrases")
    print("   â€¢ Ponctuation correcte pour la tokenisation")
    
    print("\nâš ï¸  ProblÃ¨mes potentiels:")
    print("   â€¢ Textes trop courts (1 phrase) â†’ pas de rÃ©sumÃ© possible")
    print("   â€¢ Textes sans ponctuation â†’ pas de tokenisation en phrases")
    print("   â€¢ Dataset trop petit (< 10 phrases) â†’ Ã©chec d'entraÃ®nement")
    print("   â€¢ Phrases trop longues (> 50 tokens) â†’ troncature")

def test_autoencoder_usage():
    """Test de l'utilisation de l'autoencodeur"""
    print("\n5. TEST D'UTILISATION")
    print("-" * 40)
    
    autoencoder = AutoencoderSummarizer()
    
    # Test avec diffÃ©rents types de textes
    test_cases = [
        ("Texte court valide", "PremiÃ¨re phrase. DeuxiÃ¨me phrase. TroisiÃ¨me phrase."),
        ("Texte avec ponctuation incorrecte", "PremiÃ¨re phrase DeuxiÃ¨me phrase TroisiÃ¨me phrase"),
        ("Texte trÃ¨s court", "Une seule phrase."),
        ("Texte long", "PremiÃ¨re phrase. DeuxiÃ¨me phrase. TroisiÃ¨me phrase. QuatriÃ¨me phrase. CinquiÃ¨me phrase. SixiÃ¨me phrase. SeptiÃ¨me phrase. HuitiÃ¨me phrase.")
    ]
    
    for name, text in test_cases:
        print(f"\nğŸ“„ Test: {name}")
        print(f"ğŸ“ Texte: {text}")
        
        try:
            # Test de rÃ©sumÃ© (si le modÃ¨le est entraÃ®nÃ©)
            if autoencoder.model is not None:
                summary = autoencoder.summarize(text, num_sentences=2)
                print(f"ğŸ“‹ RÃ©sumÃ©: {summary}")
            else:
                print("âš ï¸  ModÃ¨le non entraÃ®nÃ©")
                
            # Test de prÃ©processing
            sentence_vectors, sentences = autoencoder.preprocess_sentences(text)
            print(f"ğŸ”§ Phrases dÃ©tectÃ©es: {len(sentences)}")
            print(f"ğŸ”§ Phrases vectorisÃ©es: {len(sentence_vectors)}")
            
        except Exception as e:
            print(f"âŒ Erreur: {e}")

if __name__ == "__main__":
    test_autoencoder_constraints()
    test_autoencoder_usage() 