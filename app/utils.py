"""
Module d'utilitaires pour le pr√©traitement et le chargement des donn√©es.
"""

import pandas as pd
import re
import string
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
from nltk.tag import pos_tag
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import numpy as np
import wikipedia

class DataLoader:
    """
    Classe pour charger et pr√©parer les donn√©es.
    """
    def __init__(self, filepath):
        """
        Initialise le chargeur de donn√©es.
        
        Args:
            filepath (str): Chemin vers le fichier CSV
        """
        self.data = pd.read_csv(filepath)
        self.clean_data()

    def clean_data(self):
        """
        Nettoie les donn√©es en supprimant les doublons et les valeurs manquantes.
        """
        # Suppression des doublons
        self.data = self.data.drop_duplicates()
        
        # Suppression des lignes avec valeurs manquantes
        self.data = self.data.dropna()
        
        # R√©initialisation de l'index
        self.data = self.data.reset_index(drop=True)

    def get_texts_and_labels(self):
        """
        Retourne les textes et les labels.
        
        Returns:
            tuple: (texts, labels)
        """
        return self.data['text'], self.data['category']

    def split_data(self, texts, labels, test_size=0.2, random_state=42):
        """
        Divise les donn√©es en ensembles d'entra√Ænement et de test.
        
        Args:
            texts: Les textes
            labels: Les labels
            test_size (float): Proportion des donn√©es de test
            random_state (int): Seed pour la reproductibilit√©
            
        Returns:
            tuple: (X_train, X_test, y_train, y_test)
        """
        return train_test_split(texts, labels, test_size=test_size, random_state=random_state)

class TextPreprocessor:
    """
    Classe pour le pr√©traitement des textes avec POS-tagging avanc√©.
    """
    def __init__(self):
        """
        Initialise le pr√©traiteur de texte.
        T√©l√©charge automatiquement les ressources NLTK n√©cessaires.
        """
        # T√©l√©chargement automatique des ressources NLTK avec gestion d'erreurs
        self._download_nltk_resources()
        
        self.stop_words = set(stopwords.words('english'))
        self.lemmatizer = WordNetLemmatizer()
        
        # Mapping des tags POS NLTK vers WordNet
        self.pos_mapping = {
            'J': 'a',  # Adjective
            'V': 'v',  # Verb
            'N': 'n',  # Noun
            'R': 'r'   # Adverb
        }

    def _download_nltk_resources(self):
        """
        T√©l√©charge automatiquement les ressources NLTK n√©cessaires.
        """
        resources = [
            'stopwords',
            'wordnet', 
            'punkt',
            'averaged_perceptron_tagger',
            'averaged_perceptron_tagger_eng'  # Version anglaise sp√©cifique
        ]
        
        for resource in resources:
            try:
                # V√©rifier si la ressource existe d√©j√†
                try:
                    if resource == 'stopwords':
                        stopwords.words('english')
                    elif resource == 'wordnet':
                        from nltk.corpus import wordnet
                        wordnet.synsets('test')
                    elif resource == 'punkt':
                        from nltk.tokenize import word_tokenize
                        word_tokenize('test')
                    elif resource in ['averaged_perceptron_tagger', 'averaged_perceptron_tagger_eng']:
                        from nltk.tag import pos_tag
                        pos_tag(['test'])
                except LookupError:
                    # La ressource n'existe pas, la t√©l√©charger
                    print(f"üì• T√©l√©chargement automatique de {resource}...")
                    nltk.download(resource, quiet=True)
                    print(f"‚úÖ {resource} t√©l√©charg√© avec succ√®s")
                    
            except Exception as e:
                print(f"‚ö†Ô∏è Erreur lors du t√©l√©chargement de {resource}: {e}")
                print("Tentative de t√©l√©chargement manuel...")
                try:
                    nltk.download(resource, quiet=True)
                except:
                    print(f"‚ùå Impossible de t√©l√©charger {resource}")
                    # Si c'est le POS tagger qui pose probl√®me, essayer l'autre version
                    if resource == 'averaged_perceptron_tagger_eng':
                        try:
                            print("üîÑ Tentative avec averaged_perceptron_tagger...")
                            nltk.download('averaged_perceptron_tagger', quiet=True)
                        except:
                            print("‚ùå Impossible de t√©l√©charger le POS tagger")

    def get_wordnet_pos(self, tag):
        """
        Convertit les tags POS de NLTK vers les tags WordNet.
        
        Args:
            tag (str): Tag POS de NLTK
            
        Returns:
            str: Tag POS pour WordNet
        """
        # Premier caract√®re du tag (plus g√©n√©ral)
        first_char = tag[0].upper()
        return self.pos_mapping.get(first_char, 'n')  # Par d√©faut: nom

    def clean(self, text):
        """
        Nettoie un texte de mani√®re approfondie avec POS-tagging.
        
        Args:
            text (str): Le texte √† nettoyer
            
        Returns:
            str: Le texte nettoy√©
        """
        # Basic cleaning
        text = text.strip()
        text = text.lower()
        text = ''.join(char for char in text if not char.isdigit())

        # Nettoyage des emails
        text = re.sub(r'From:.*?Subject:', '', text, flags=re.DOTALL)
        text = re.sub(r'\S+@\S+', '', text)

        # Suppression des mots avec 3+ lettres cons√©cutives identiques
        text = re.sub(r'\b\w*(\w)\1{2,}\w*\b', '', text)

        # Suppression des URLs
        text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)

        # Suppression de la ponctuation
        for punctuation in string.punctuation:
            text = text.replace(punctuation, '')

        # 1. ‚úÖ TOKENISATION
        tokenized_text = word_tokenize(text)
        
        # 2. ‚úÖ SUPPRESSION DES STOPWORDS
        tokenized_text_cleaned = [
            w for w in tokenized_text if not w in self.stop_words
        ]

        # 3. ‚úÖ POS-TAGGING AVEC GESTION D'ERREUR
        try:
            pos_tagged = pos_tag(tokenized_text_cleaned)
            
            # 4. ‚úÖ LEMMATISATION AVANC√âE AVEC POS-TAGGING
            lemmatized = []
            for word, tag in pos_tagged:
                # Conversion du tag POS pour WordNet
                wordnet_pos = self.get_wordnet_pos(tag)
                # Lemmatisation avec le bon POS
                lemmatized_word = self.lemmatizer.lemmatize(word, pos=wordnet_pos)
                lemmatized.append(lemmatized_word)
                
        except LookupError as e:
            # Fallback si le POS tagger n'est pas disponible
            print(f"‚ö†Ô∏è POS tagger non disponible: {e}")
            print("üîÑ Utilisation de la lemmatisation simple...")
            
            # Lemmatisation simple sans POS tagging
            lemmatized = []
            for word in tokenized_text_cleaned:
                lemmatized_word = self.lemmatizer.lemmatize(word)
                lemmatized.append(lemmatized_word)
                
        except Exception as e:
            # Autre erreur, retourner le texte nettoy√© sans lemmatisation
            print(f"‚ö†Ô∏è Erreur lors du POS tagging: {e}")
            print("üîÑ Retour du texte nettoy√© sans lemmatisation...")
            lemmatized = tokenized_text_cleaned

        # Reconstruction du texte
        cleaned_text = ' '.join(word for word in lemmatized)

        return cleaned_text

    def clean_with_pos_info(self, text):
        """
        Version avanc√©e qui retourne aussi les informations POS.
        
        Args:
            text (str): Le texte √† nettoyer
            
        Returns:
            tuple: (texte_nettoy√©, liste_des_pos_tags)
        """
        # Basic cleaning
        text = text.strip()
        text = text.lower()
        text = ''.join(char for char in text if not char.isdigit())

        # Nettoyage des emails et URLs
        text = re.sub(r'\S+@\S+', '', text)
        text = re.sub(r'http\S+|www\S+|https\S+', '', text)
        
        # Suppression de la ponctuation
        for punctuation in string.punctuation:
            text = text.replace(punctuation, '')

        # Tokenisation
        tokenized_text = word_tokenize(text)
        
        # Suppression des stopwords
        tokenized_text_cleaned = [
            w for w in tokenized_text if not w in self.stop_words
        ]

        # POS-tagging
        pos_tagged = pos_tag(tokenized_text_cleaned)

        # Lemmatisation avec POS
        lemmatized = []
        pos_info = []
        for word, tag in pos_tagged:
            wordnet_pos = self.get_wordnet_pos(tag)
            lemmatized_word = self.lemmatizer.lemmatize(word, pos=wordnet_pos)
            lemmatized.append(lemmatized_word)
            pos_info.append((lemmatized_word, tag, wordnet_pos))

        cleaned_text = ' '.join(word for word in lemmatized)
        
        return cleaned_text, pos_info

    def normalize(self, text):
        """
        Normalise un texte.
        
        Args:
            text (str): Le texte √† normaliser
            
        Returns:
            str: Le texte normalis√©
        """
        # Suppression des espaces multiples
        text = re.sub(r'\s+', ' ', text)
        
        # Suppression des espaces en d√©but et fin
        text = text.strip()
        
        return text

    def transform(self, texts):
        """
        Transforme une liste de textes.
        
        Args:
            texts: Les textes √† transformer
            
        Returns:
            Series: Les textes transform√©s
        """
        # Nettoyage avec POS-tagging
        cleaned_texts = texts.apply(self.clean)
        
        # Normalisation
        normalized_texts = cleaned_texts.apply(self.normalize)
        
        return normalized_texts

    def transform_with_pos_info(self, texts):
        """
        Transforme une liste de textes avec informations POS.
        
        Args:
            texts: Les textes √† transformer
            
        Returns:
            tuple: (textes_transform√©s, informations_pos_par_texte)
        """
        cleaned_texts = []
        pos_infos = []
        
        for text in texts:
            cleaned_text, pos_info = self.clean_with_pos_info(text)
            cleaned_texts.append(cleaned_text)
            pos_infos.append(pos_info)
        
        # Normalisation
        normalized_texts = [self.normalize(text) for text in cleaned_texts]
        
        return normalized_texts, pos_infos

    def get_pos_statistics(self, texts, sample_size=1000):
        """
        Calcule les statistiques des parties du discours dans un √©chantillon de textes.
        
        Args:
            texts: Les textes √† analyser
            sample_size (int): Taille de l'√©chantillon
            
        Returns:
            dict: Statistiques des POS
        """
        if len(texts) > sample_size:
            sample_texts = texts.sample(n=sample_size, random_state=42)
        else:
            sample_texts = texts
        
        pos_counts = {}
        total_words = 0
        
        for text in sample_texts:
            _, pos_info = self.clean_with_pos_info(text)
            for word, tag, wordnet_pos in pos_info:
                pos_counts[tag] = pos_counts.get(tag, 0) + 1
                total_words += 1
        
        # Calcul des pourcentages
        pos_stats = {}
        for tag, count in pos_counts.items():
            pos_stats[tag] = {
                'count': count,
                'percentage': (count / total_words) * 100
            }
        
        return pos_stats

    def extract_pos_words(self, text, pos_types=None):
        """
        Extrait les mots selon leur partie du discours (POS).
        
        Args:
            text (str): Le texte √† analyser
            pos_types (list): Liste des types POS √† extraire. Par d√©faut: ['NN', 'VB', 'JJ']
                            - 'NN': Noms (nouns)
                            - 'VB': Verbes (verbs) 
                            - 'JJ': Adjectifs (adjectives)
                            - 'RB': Adverbes (adverbs)
                            - 'PRP': Pronoms (pronouns)
                            - etc.
        
        Returns:
            dict: Dictionnaire avec les mots group√©s par type POS
        """
        if pos_types is None:
            pos_types = ['NN', 'VB', 'JJ']  # Noms, Verbes, Adjectifs par d√©faut
        
        # Nettoyage et POS-tagging
        _, pos_info = self.clean_with_pos_info(text)
        
        # Extraction des mots par type POS
        pos_words = {pos_type: [] for pos_type in pos_types}
        
        for word, tag, wordnet_pos in pos_info:
            # V√©rifier si le tag commence par le type POS recherch√©
            for pos_type in pos_types:
                if tag.startswith(pos_type):
                    pos_words[pos_type].append(word)
                    break
        
        return pos_words

    def extract_verbs_adjectives_nouns(self, text):
        """
        Extrait sp√©cifiquement les verbes, adjectifs et noms d'un texte.
        
        Args:
            text (str): Le texte √† analyser
            
        Returns:
            dict: Dictionnaire avec 'verbs', 'adjectives', 'nouns'
        """
        pos_words = self.extract_pos_words(text, ['NN', 'VB', 'JJ'])
        
        return {
            'verbs': pos_words.get('VB', []),
            'adjectives': pos_words.get('JJ', []),
            'nouns': pos_words.get('NN', [])
        }

    def get_pos_summary(self, text):
        """
        G√©n√®re un r√©sum√© des parties du discours dans un texte.
        
        Args:
            text (str): Le texte √† analyser
            
        Returns:
            dict: R√©sum√© avec statistiques et mots par type
        """
        # Extraction des mots par POS
        pos_words = self.extract_pos_words(text)
        
        # Statistiques
        total_words = sum(len(words) for words in pos_words.values())
        
        summary = {
            'total_words_analyzed': total_words,
            'pos_distribution': {},
            'words_by_pos': pos_words
        }
        
        # Calcul de la distribution
        for pos_type, words in pos_words.items():
            if total_words > 0:
                percentage = (len(words) / total_words) * 100
            else:
                percentage = 0
                
            summary['pos_distribution'][pos_type] = {
                'count': len(words),
                'percentage': percentage,
                'words': words
            }
        
        return summary

    def clean_for_autoencoder(self, text):
        """
        Pr√©traitement minimal pour l'autoencodeur.
        Pr√©serve les phrases compl√®tes et la ponctuation pour la tokenisation.
        
        Args:
            text (str): Le texte √† nettoyer
            
        Returns:
            str: Le texte nettoy√© pour l'autoencodeur
        """
        # Nettoyage de base tr√®s l√©ger
        text = text.strip()
        
        # Suppression des URLs et emails (peuvent perturber la tokenisation)
        text = re.sub(r'http\S+|www\S+|https\S+', '', text)
        text = re.sub(r'\S+@\S+', '', text)
        
        # Suppression des caract√®res de contr√¥le
        text = re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f\x7f-\x9f]', '', text)
        
        # Suppression des chiffres sauf ceux √† exactement 4 chiffres
        # Utilise une regex pour pr√©server les nombres √† 4 chiffres
        text = re.sub(r'\b(?!\d{4}\b)\d+\b', '', text)
        
        # Normalisation des espaces (mais pas suppression)
        text = re.sub(r'\s+', ' ', text)
        
        # Suppression des lignes vides multiples
        text = re.sub(r'\n\s*\n', '\n', text)
        
        return text.strip()

    def transform_for_autoencoder(self, texts):
        """
        Transforme une liste de textes pour l'autoencodeur.
        Utilise un pr√©traitement minimal pour pr√©server les phrases.
        
        Args:
            texts: Les textes √† transformer
            
        Returns:
            list: Les textes transform√©s pour l'autoencodeur
        """
        cleaned_texts = []
        
        for text in texts:
            cleaned_text = self.clean_for_autoencoder(text)
            if cleaned_text:  # Ne garder que les textes non vides
                cleaned_texts.append(cleaned_text)
        
        return cleaned_texts

def encode_labels(labels):
    """
    Encode les labels en utilisant LabelEncoder.
    
    Args:
        labels: Les labels √† encoder
        
    Returns:
        tuple: (encoded_labels, encoder)
    """
    encoder = LabelEncoder()
    encoded_labels = encoder.fit_transform(labels)
    return encoded_labels, encoder

def normalize_text(text):
    """
    Normalise un texte unique.
    
    Args:
        text (str): Le texte √† normaliser
        
    Returns:
        str: Le texte normalis√©
    """
    # Suppression des espaces multiples
    text = re.sub(r'\s+', ' ', text)
    
    # Suppression des espaces en d√©but et fin
    text = text.strip()
    
    return text

def search_wikipedia_smart(query):
    """
    Recherche intelligente sur Wikipedia avec gestion de l'ambigu√Øt√©.
    Retourne un dictionnaire avec le statut et les suggestions/pages trouv√©es.
    """
    wikipedia.set_lang("en")
    try:
        # Recherche de pages correspondant √† la requ√™te
        results = wikipedia.search(query, results=5)
        if not results:
            return {
                'status': 'error',
                'message': "Aucune page Wikipedia trouv√©e pour cette requ√™te, kero ! üê∏"
            }
        if len(results) == 1:
            # Succ√®s direct
            page = results[0]
            summary = wikipedia.summary(page, sentences=3)
            return {
                'status': 'success',
                'page': page,
                'summary': summary
            }
        else:
            # Ambigu√Øt√© : plusieurs pages possibles
            # On regroupe sous un seul mot-cl√© pour l'interface
            suggestions = {query: results}
            return {
                'status': 'ambiguous',
                'suggestions': suggestions
            }
    except Exception as e:
        return {
            'status': 'error',
            'message': f"Erreur lors de la recherche Wikipedia : {str(e)} kero üê∏"
        }