"""
# ============================================================================
# AUTOENCODEUR
# ============================================================================

AutoencoderSummarizer : R√©sum√© extractif par autoencodeur
   - D√©coupage en phrases ‚Üí Vectorisation ‚Üí Autoencodeur ‚Üí Erreur reconstruction
   - S√©lection des phrases avec erreur de reconstruction la plus faible
   - Architecture : Embedding ‚Üí LSTM ‚Üí Dense ‚Üí RepeatVector ‚Üí LSTM ‚Üí TimeDistributed
   - Sauvegarde automatique du tokenizer dans app/models/
"""

import os
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, RepeatVector, TimeDistributed
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.preprocessing.sequence import pad_sequences
from mon_tokenizer import SharedTokenizer

class AutoencoderSummarizer:
    """
    Mod√®le de r√©sum√© extractif bas√© sur un autoencodeur.
    Utilise le tokenizer partag√© pour la coh√©rence.
    - G√©n√©ration automatique des performances dans app/performances/
    - Early stopping avanc√©
    
    Processus d√©taill√© :
    1. D√©coupage du texte en phrases (tokenisation).
    2. Vectorisation de chaque phrase avec le tokenizer partag√©.
    3. Construction d'un autoencodeur s√©quentiel (Embedding + LSTM + Dense + RepeatVector + LSTM + TimeDistributed).
    4. Entra√Ænement de l'autoencodeur √† reconstruire les s√©quences de tokens des phrases.
    5. Pour le r√©sum√© :
       - On passe chaque phrase dans l'autoencodeur.
       - On calcule l'erreur de reconstruction (diff√©rence entre la phrase originale et la phrase reconstruite).
       - On s√©lectionne les phrases avec l'erreur la plus faible (les plus "repr√©sentatives").
    """
    MODEL_PATH = os.path.join(os.path.dirname(__file__), "models", "autoencoder_summarizer.h5")
    
    def __init__(self, max_words=5000, embedding_dim=128, max_sentence_length=50):
        """
        Initialise l'autoencodeur.
        
        Args:
            max_words (int): Taille maximale du vocabulaire
            embedding_dim (int): Dimension des embeddings
            max_sentence_length (int): Longueur maximale des phrases
        """
        self.max_words = max_words
        self.embedding_dim = embedding_dim
        self.max_sentence_length = max_sentence_length
        self.tokenizer = SharedTokenizer(max_words=max_words, max_len=max_sentence_length)
        self.model = None
        self.history = None
        
        # Chargement automatique si le mod√®le existe d√©j√†
        if os.path.exists(self.MODEL_PATH):
            from tensorflow.keras.models import load_model
            self.model = load_model(self.MODEL_PATH)
            print(f"‚úÖ Autoencodeur charg√© depuis {self.MODEL_PATH}")

    def preprocess_sentences(self, text):
        """
        D√©coupe le texte en phrases et vectorise chaque phrase.
        - Utilise NLTK pour la tokenisation en phrases.
        - Vectorise chaque phrase avec le tokenizer partag√©.
        Retourne :
            - sentence_vectors : matrice (nb_phrases, max_sentence_length)
            - original_sentences : phrases originales (pour le r√©sum√© final)
        """
        from nltk.tokenize import sent_tokenize
        
        # D√©coupage en phrases
        sentences = sent_tokenize(text)
        
        # Filtrage des phrases trop courtes ou trop longues
        filtered_sentences = []
        for sentence in sentences:
            if 5 <= len(sentence.split()) <= self.max_sentence_length:
                filtered_sentences.append(sentence)
        
        if len(filtered_sentences) == 0:
            return [], []
        
        # Vectorisation des phrases
        sequences = self.tokenizer.texts_to_sequences(filtered_sentences)
        sentence_vectors = pad_sequences(sequences, maxlen=self.max_sentence_length)
        
        return sentence_vectors, filtered_sentences

    def build_autoencoder(self):
        """
        Construit l'architecture de l'autoencodeur.
        """
        self.model = Sequential([
            # Encoder
            Embedding(self.max_words, self.embedding_dim, input_length=self.max_sentence_length),
            LSTM(64, return_sequences=True),
            LSTM(32),
            Dense(16, activation='relu'),
            
            # Decoder
            RepeatVector(self.max_sentence_length),
            LSTM(32, return_sequences=True),
            LSTM(64, return_sequences=True),
            TimeDistributed(Dense(self.max_words, activation='softmax'))
        ])
        
        self.model.compile(
            loss='sparse_categorical_crossentropy',
            optimizer='adam',
            metrics=['accuracy']
        )
        
        print("‚úÖ Architecture autoencodeur construite")

    def train(self, texts, epochs=15, batch_size=32):
        """
        Entra√Æne l'autoencodeur sur toutes les phrases du dataset avec early stopping.
        - D√©coupe chaque texte en phrases, vectorise.
        - Concat√®ne toutes les phrases pour former le jeu d'entra√Ænement.
        - Entra√Æne l'autoencodeur √† reconstruire chaque phrase.
        - Sauvegarde le mod√®le entra√Æn√©.
        """
        print("üîÑ Pr√©paration des donn√©es pour l'autoencodeur...")
        
        # Entra√Æner le tokenizer partag√© si n√©cessaire
        if not self.tokenizer.is_fitted:
            print("üîÑ Entra√Ænement du tokenizer partag√©...")
            # Extraire toutes les phrases pour entra√Æner le tokenizer
            all_sentences_for_tokenizer = []
            for text in texts:
                from nltk.tokenize import sent_tokenize
                sentences = sent_tokenize(text)
                all_sentences_for_tokenizer.extend(sentences)
            
            if len(all_sentences_for_tokenizer) > 0:
                self.tokenizer.fit_on_texts(all_sentences_for_tokenizer)
                print(f"‚úÖ Tokenizer entra√Æn√© sur {len(all_sentences_for_tokenizer)} phrases")
            else:
                print("‚ùå Aucune phrase trouv√©e pour entra√Æner le tokenizer")
                return
        
        all_sentences = []
        all_sentence_vectors = []
        for text in texts:
            sentence_vectors, original_sentences = self.preprocess_sentences(text)
            if len(sentence_vectors) > 0:
                all_sentences.extend(original_sentences)
                all_sentence_vectors.extend(sentence_vectors)
        
        if len(all_sentences) < 10:
            print("‚ö†Ô∏è Pas assez de phrases pour entra√Æner l'autoencodeur")
            print(f"   Phrases trouv√©es: {len(all_sentences)}")
            print(f"   Textes analys√©s: {len(texts)}")
            return
        
        X_train = np.array(all_sentence_vectors)
        print(f"Nombre total de phrases pour l'entra√Ænement : {X_train.shape[0]}")
        print(f"Forme des donn√©es d'entra√Ænement : {X_train.shape}")
        print("üîÑ Construction de l'autoencodeur...")
        self.build_autoencoder()
        print("üîÑ Entra√Ænement de l'autoencodeur avec early stopping...")
        
        early_stopping = EarlyStopping(
            monitor='val_loss',
            patience=5,
            restore_best_weights=True,
            verbose=1
        )
        
        reduce_lr = ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.5,
            patience=3,
            min_lr=1e-7,
            verbose=1
        )
        
        self.history = self.model.fit(
            X_train, X_train,
            epochs=epochs,
            batch_size=batch_size,
            validation_split=0.2,
            callbacks=[early_stopping, reduce_lr],
            verbose=1
        )
        self.model.save(self.MODEL_PATH)
        print(f"‚úÖ Autoencodeur sauvegard√© dans {self.MODEL_PATH}")
        
        # G√©n√©ration automatique des performances
        self._generate_performance_metrics()

    def _generate_performance_metrics(self):
        """
        G√©n√®re automatiquement les m√©triques de performance et les sauvegarde.
        """
        if self.history is None:
            print("‚ö†Ô∏è Pas d'historique d'entra√Ænement pour g√©n√©rer les m√©triques")
            return
            
        # Cr√©ation du dossier performances
        os.makedirs('app/performances', exist_ok=True)
        
        # 1. M√©triques finales
        final_loss = self.history.history['loss'][-1]
        final_val_loss = self.history.history['val_loss'][-1]
        final_accuracy = self.history.history['accuracy'][-1]
        final_val_accuracy = self.history.history['val_accuracy'][-1]
        
        # 2. Sauvegarde des m√©triques en CSV
        metrics_df = pd.DataFrame({
            'M√©trique': ['Loss finale (entra√Ænement)', 'Loss finale (validation)', 
                        'Accuracy finale (entra√Ænement)', 'Accuracy finale (validation)'],
            'Valeur': [final_loss, final_val_loss, final_accuracy, final_val_accuracy]
        })
        metrics_df.to_csv('app/performances/autoencoder_metrics.csv', index=False)
        
        # 3. Sauvegarde de l'historique d'entra√Ænement
        history_df = pd.DataFrame(self.history.history)
        history_df.to_csv('app/performances/autoencoder_training_history.csv', index=False)
        
        # 4. Courbes d'apprentissage
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
        
        # Loss
        ax1.plot(self.history.history['loss'], 'b-', linewidth=2, label='Entra√Ænement')
        ax1.plot(self.history.history['val_loss'], 'r-', linewidth=2, label='Validation')
        ax1.set_title('Loss de l\'Autoencodeur', fontweight='bold')
        ax1.set_xlabel('√âpoques')
        ax1.set_ylabel('Loss')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # Accuracy
        ax2.plot(self.history.history['accuracy'], 'b-', linewidth=2, label='Entra√Ænement')
        ax2.plot(self.history.history['val_accuracy'], 'r-', linewidth=2, label='Validation')
        ax2.set_title('Accuracy de l\'Autoencodeur', fontweight='bold')
        ax2.set_xlabel('√âpoques')
        ax2.set_ylabel('Accuracy')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig('app/performances/autoencoder_learning_curves.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print("‚úÖ M√©triques Autoencodeur g√©n√©r√©es dans app/performances/")

    def evaluate(self):
        """
        √âvalue l'autoencodeur et g√©n√®re les m√©triques de performance.
        Cette m√©thode est appel√©e automatiquement apr√®s l'entra√Ænement.
        """
        if self.history is None:
            print("‚ùå L'autoencodeur n'a pas encore √©t√© entra√Æn√©!")
            return
        
        print("üìä √âVALUATION DE L'AUTOENCODEUR")
        print("="*50)
        
        # M√©triques finales
        final_loss = self.history.history['loss'][-1]
        final_val_loss = self.history.history['val_loss'][-1]
        final_accuracy = self.history.history['accuracy'][-1]
        final_val_accuracy = self.history.history['val_accuracy'][-1]
        
        print(f"üìà Loss finale (entra√Ænement) : {final_loss:.4f}")
        print(f"üìà Loss finale (validation) : {final_val_loss:.4f}")
        print(f"üìà Accuracy finale (entra√Ænement) : {final_accuracy:.4f}")
        print(f"üìà Accuracy finale (validation) : {final_val_accuracy:.4f}")
        
        # G√©n√©ration des m√©triques de performance
        self._generate_performance_metrics()
        
        print("‚úÖ √âvaluation de l'autoencodeur termin√©e!")

    def summarize(self, text, num_sentences=3):
        """
        R√©sume un texte en s√©lectionnant les phrases les mieux reconstruites par l'autoencodeur.
        - D√©coupe et vectorise les phrases du texte.
        - Passe chaque phrase dans l'autoencodeur et calcule l'erreur de reconstruction.
        - S√©lectionne les phrases avec l'erreur la plus faible (les plus "centrales").
        - Retourne le r√©sum√© (concat√©nation des phrases s√©lectionn√©es dans l'ordre d'origine).
        """
        if self.model is None:
            raise RuntimeError("The autoencoder isn't trained yet! üê∏ Please run the training script first, kero!")
        sentence_vectors, original_sentences = self.preprocess_sentences(text)
        if len(sentence_vectors) == 0:
            return "*splashes water* üê∏ This text is too short to summarize, kero!"
        reconstruction_errors = []
        for i, sentence_vector in enumerate(sentence_vectors):
            reconstructed = self.model.predict(sentence_vector.reshape(1, -1), verbose=0)
            original_sequence = sentence_vector
            reconstructed_sequence = reconstructed[0].argmax(axis=-1)
            error = np.mean(np.abs(original_sequence - reconstructed_sequence))
            reconstruction_errors.append((i, error))
        reconstruction_errors.sort(key=lambda x: x[1])
        selected_indices = [idx for idx, _ in reconstruction_errors[:num_sentences]]
        selected_indices.sort()
        summary_sentences = [original_sentences[i] for i in selected_indices]
        summary = " ".join(summary_sentences)
        return summary 