"""
Module principal du chatbot.
Ce script impl√©mente la logique du chatbot, incluant la classification de texte,
la g√©n√©ration de r√©ponses et l'int√©gration avec diff√©rents mod√®les de langage.
"""

import os
import torch
import joblib
import numpy as np
from typing import Optional, Dict, Any
import time
from functools import lru_cache
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from transformers import AutoTokenizer, AutoModelForCausalLM, BertTokenizer, BertForSequenceClassification, pipeline
from sklearn.metrics import accuracy_score

from utils import summarize_text, search_wikipedia, preprocess_text
from models import SupervisedClassifier, DeepLearningClassifier, RNNTextClassifier, KerasTextClassifier

class Chatbot:
    """
    Classe principale du chatbot.
    G√®re l'initialisation des mod√®les, la classification de texte et la g√©n√©ration de r√©ponses.
    """
    
    def __init__(self):
        """Initialise le chatbot avec les mod√®les n√©cessaires."""
        self.model_name = "microsoft/DialoGPT-medium"  # Mod√®le de langage utilis√©
        self.tokenizer = None  # Tokenizer pour le mod√®le de langage
        self.model = None  # Mod√®le de langage
        self.classifier_model = None  # Mod√®le de classification ML
        self.vectorizer = None  # Vectoriseur pour la classification ML
        self.bert_tokenizer = None  # Tokenizer BERT
        self.bert_model = None  # Mod√®le BERT pour la classification
        self.optimized_model = None  # Mod√®le ML optimis√©
        self.bert_labels = [
            "accueil", "m√©t√©o", "technologie", "cuisine", "actualit√©s",
            "√©ducation", "histoire", "sport"
        ]
        self.classifier_ml = SupervisedClassifier()
        self.classifier_dl = DeepLearningClassifier()
        self.classifier_rnn = RNNTextClassifier()
        self.classifier_keras = KerasTextClassifier()
        self.sentiment_analyzer = pipeline("sentiment-analysis")
        self.initialize_models()

    def optimize_ml_model(self, texts, labels):
        """Optimise le mod√®le ML avec GridSearchCV"""
        try:
            # Pr√©traitement des textes
            processed_texts = [self.preprocess_text(text) for text in texts]
            
            # Cr√©ation du pipeline
            pipeline = Pipeline([
                ('tfidf', TfidfVectorizer(max_features=5000)),
                ('clf', MultinomialNB())
            ])
            
            # D√©finition des param√®tres √† tester
            param_grid = {
                'tfidf__max_features': [3000, 5000, 7000],
                'tfidf__ngram_range': [(1, 1), (1, 2)],
                'clf__alpha': [0.1, 0.5, 1.0]
            }
            
            # Calcul du nombre optimal de plis
            n_samples = len(processed_texts)
            n_splits = min(5, n_samples // 2)  # Utilise le minimum entre 5 et la moiti√© du nombre d'√©chantillons
            if n_splits < 2:
                n_splits = 2  # Minimum 2 plis pour la validation crois√©e
            
            # Cr√©ation du GridSearchCV avec le nombre de plis adapt√©
            grid_search = GridSearchCV(
                pipeline,
                param_grid,
                cv=n_splits,
                n_jobs=-1,
                verbose=1
            )
            
            # Division des donn√©es en ensembles d'entra√Ænement et de test
            X_train, X_test, y_train, y_test = train_test_split(
                processed_texts, labels,
                test_size=0.2,
                random_state=42,
                stratify=labels
            )
            
            # Entra√Ænement du mod√®le
            grid_search.fit(X_train, y_train)
            
            # Sauvegarde du meilleur mod√®le
            self.ml_model = grid_search.best_estimator_
            
            # √âvaluation du mod√®le
            y_pred = self.ml_model.predict(X_test)
            accuracy = accuracy_score(y_test, y_pred)
            print(f"Meilleure pr√©cision : {accuracy:.2f}")
            print(f"Meilleurs param√®tres : {grid_search.best_params_}")
            
            return accuracy
            
        except Exception as e:
            print(f"Erreur lors de l'optimisation du mod√®le ML : {str(e)}")
            return None

    def create_base_model(self):
        """
        Cr√©e un mod√®le de base avec des donn√©es d'exemple.
        """
        try:
            print("üîÑ Cr√©ation d'un mod√®le de base...")
            
            # Donn√©es d'exemple pour l'entra√Ænement
            texts = [
                "Bonjour, comment puis-je vous aider ?",
                "Quel temps fait-il aujourd'hui ?",
                "Je cherche des informations sur l'intelligence artificielle",
                "Comment faire une recette de g√¢teau au chocolat ?",
                "Quelles sont les derni√®res actualit√©s ?",
                "Je cherche des cours de math√©matiques",
                "Parlez-moi de l'histoire de France",
                "Quels sont les r√©sultats du match de football ?"
            ]
            
            labels = [
                "accueil",
                "m√©t√©o",
                "technologie",
                "cuisine",
                "actualit√©s",
                "√©ducation",
                "histoire",
                "sport"
            ]
            
            # Pr√©traitement des textes
            processed_texts = [preprocess_text(text) for text in texts]
            
            # Liste de mots vides en fran√ßais
            french_stop_words = [
                'le', 'la', 'les', 'un', 'une', 'des', 'et', 'ou', 'mais', 'donc', 'car', 'ni',
                'ce', 'cet', 'cette', 'ces', 'mon', 'ton', 'son', 'notre', 'votre', 'leur',
                'qui', 'que', 'quoi', 'dont', 'o√π', 'comment', 'pourquoi', 'quand',
                'je', 'tu', 'il', 'elle', 'nous', 'vous', 'ils', 'elles',
                '√™tre', 'avoir', 'faire', 'dire', 'aller', 'voir', 'venir',
                'de', 'du', 'des', '√†', 'au', 'aux', 'en', 'dans', 'sur', 'sous',
                'par', 'pour', 'avec', 'sans', 'vers', 'chez', 'entre', 'parmi'
            ]
            
            # Cr√©ation du vectoriseur avec les mots vides en fran√ßais
            self.vectorizer = TfidfVectorizer(
                max_features=5000,
                ngram_range=(1, 2),
                stop_words=french_stop_words
            )
            
            # Transformation des textes
            X = self.vectorizer.fit_transform(processed_texts)
            
            # Cr√©ation et entra√Ænement du classificateur
            self.classifier_model = MultinomialNB(alpha=0.1)
            self.classifier_model.fit(X, labels)
            
            # Sauvegarde des mod√®les
            model_dir = "app"
            if not os.path.exists(model_dir):
                os.makedirs(model_dir)
            
            joblib.dump(self.classifier_model, os.path.join(model_dir, "model.joblib"))
            joblib.dump(self.vectorizer, os.path.join(model_dir, "vectorizer.joblib"))
            
            print("‚úÖ Mod√®le de base cr√©√© et sauvegard√© avec succ√®s !")
            
        except Exception as e:
            print(f"‚ùå Erreur lors de la cr√©ation du mod√®le de base : {str(e)}")
            raise

    def initialize_models(self):
        """
        Initialise tous les mod√®les n√©cessaires au fonctionnement du chatbot.
        Charge les mod√®les existants ou en cr√©e de nouveaux si n√©cessaire.
        """
        try:
            print("üîÑ Chargement des mod√®les...")
            
            # Chargement du mod√®le DialoGPT pour la g√©n√©ration de r√©ponses
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            self.model = AutoModelForCausalLM.from_pretrained(self.model_name)
            
            # Chargement des mod√®les de classification ML
            model_path = "app/model.joblib"
            vectorizer_path = "app/vectorizer.joblib"
            
            if os.path.exists(model_path) and os.path.exists(vectorizer_path):
                print("üîÑ Chargement des mod√®les ML existants...")
                self.classifier_model = joblib.load(model_path)
                self.vectorizer = joblib.load(vectorizer_path)
                
                # V√©rification que les mod√®les sont correctement charg√©s
                if self.classifier_model is None or self.vectorizer is None:
                    raise ValueError("Les mod√®les ML n'ont pas √©t√© correctement charg√©s")
                
                # Test du mod√®le avec un texte simple
                test_text = "Test d'initialisation du mod√®le ML"
                test_processed = preprocess_text(test_text)
                test_vector = self.vectorizer.transform([test_processed])
                test_pred = self.classifier_model.predict(test_vector)
                
                if test_pred is None:
                    raise ValueError("Le mod√®le ML ne produit pas de pr√©dictions")
                
                print("‚úÖ Mod√®les ML charg√©s avec succ√®s !")
            else:
                print("‚ö†Ô∏è Mod√®les de classification non trouv√©s. Cr√©ation d'un mod√®le de base...")
                self.create_base_model()
            
            # Chargement du mod√®le BERT pour la classification DL
            print("üîÑ Chargement du mod√®le BERT...")
            self.bert_tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
            self.bert_model = BertForSequenceClassification.from_pretrained(
                "bert-base-uncased",
                num_labels=len(self.bert_labels),
                output_hidden_states=True
            )
            
            # V√©rification que le mod√®le BERT est bien charg√©
            if self.bert_model is None or self.bert_tokenizer is None:
                raise ValueError("Le mod√®le BERT n'a pas √©t√© correctement initialis√©")
            
            # Test du mod√®le BERT avec un texte simple
            test_text = "Test d'initialisation du mod√®le BERT"
            test_inputs = self.bert_tokenizer(test_text, return_tensors="pt", truncation=True, padding=True)
            test_outputs = self.bert_model(**test_inputs)
            
            if test_outputs is None:
                raise ValueError("Le mod√®le BERT ne produit pas de sorties")
            
            print("‚úÖ Tous les mod√®les ont √©t√© charg√©s avec succ√®s !")
            
        except Exception as e:
            print(f"‚ùå Erreur lors de l'initialisation des mod√®les: {str(e)}")
            raise

    def classify_with_bert(self, text: str) -> dict:
        """
        Classifie un texte avec BERT et retourne la cat√©gorie pr√©dite et la confiance.
        """
        try:
            if self.bert_model is None or self.bert_tokenizer is None:
                raise ValueError("Le mod√®le BERT n'est pas initialis√©")
            
            # Pr√©traitement du texte
            inputs = self.bert_tokenizer(
                text,
                return_tensors="pt",
                truncation=True,
                padding=True,
                max_length=128
            )
            
            # D√©sactivation du gradient pour l'inf√©rence
            with torch.no_grad():
                outputs = self.bert_model(**inputs)
            
            # V√©rification des sorties
            if outputs is None or not hasattr(outputs, 'logits'):
                raise ValueError("Le mod√®le BERT n'a pas produit de sorties valides")
            
            # Calcul des probabilit√©s
            logits = outputs.logits
            probs = torch.nn.functional.softmax(logits, dim=-1)
            probs = probs.detach().numpy()[0]
            
            # Obtention de la pr√©diction
            pred_idx = np.argmax(probs)
            confidence = float(probs[pred_idx])
            
            # Obtention des embeddings si disponibles
            embeddings = None
            if hasattr(outputs, 'hidden_states') and outputs.hidden_states is not None:
                embeddings = outputs.hidden_states[-1][0][0].detach().numpy().tolist()
            
            return {
                "category": self.bert_labels[pred_idx] if pred_idx < len(self.bert_labels) else "autre",
                "confidence": confidence,
                "embeddings": embeddings
            }
            
        except Exception as e:
            print(f"‚ùå Erreur lors de la classification BERT: {str(e)}")
            return {"category": "erreur", "confidence": 0.0, "embeddings": None}

    def search_wikipedia(self, query: str) -> str:
        """
        Recherche des informations sur Wikipedia.
        
        Args:
            query (str): La requ√™te de recherche
            
        Returns:
            str: Les informations trouv√©es
        """
        try:
            return search_wikipedia(query)
        except Exception as e:
            return f"Erreur lors de la recherche: {str(e)}"

    def summarize_text(self, text: str, use_dl: bool = True) -> str:
        """
        R√©sume un texte.
        
        Args:
            text (str): Le texte √† r√©sumer
            use_dl (bool): Si True, utilise le mod√®le de deep learning
            
        Returns:
            str: Le r√©sum√© g√©n√©r√©
        """
        try:
            return summarize_text(text, use_dl)
        except Exception as e:
            return f"Erreur lors du r√©sum√©: {str(e)}"

    @lru_cache(maxsize=100)
    def generate_response(self, text: str, use_dl: bool = False, model_type: str = "bert") -> str:
        """
        G√©n√®re une r√©ponse en fonction du texte d'entr√©e.
        
        Args:
            text (str): Le texte √† analyser
            use_dl (bool): Si True, utilise le mod√®le de deep learning
            model_type (str): Type de mod√®le DL ("bert", "rnn" ou "keras")
            
        Returns:
            str: La r√©ponse g√©n√©r√©e
        """
        try:
            if use_dl:
                if model_type == "bert":
                    sentiment = self.sentiment_analyzer(text)[0]
                    return f"Sentiment: {sentiment['label']} (Confiance: {sentiment['score']:.2f})"
                elif model_type == "rnn":
                    predictions, probs = self.classifier_rnn.predict([text])
                    sentiment = "POSITIVE" if predictions[0] == 1 else "NEGATIVE"
                    confidence = probs[0][predictions[0]]
                    return f"Sentiment: {sentiment} (Confiance: {confidence:.2f})"
                else:  # keras
                    predictions, probs = self.classifier_keras.predict([text])
                    sentiment = "POSITIVE" if predictions[0] == 1 else "NEGATIVE"
                    confidence = probs[0][predictions[0]]
                    return f"Sentiment: {sentiment} (Confiance: {confidence:.2f})"
            else:
                sentiment = self.classifier_ml.predict([text])[0]
                return f"Sentiment: {sentiment}"
        except Exception as e:
            return f"Erreur lors de l'analyse: {str(e)}"

    def classify_text(self, text: str, use_dl: bool = False, model_type: str = "bert") -> str:
        """
        Classifie un texte en utilisant soit le mod√®le ML soit le mod√®le DL.
        
        Args:
            text (str): Le texte √† classifier
            use_dl (bool): Si True, utilise le mod√®le de deep learning
            model_type (str): Type de mod√®le DL ("bert", "rnn" ou "keras")
            
        Returns:
            str: La cat√©gorie pr√©dite et la confiance
        """
        try:
            if use_dl:
                if model_type == "bert":
                    result = self.classify_with_bert(text)
                    return f"Cat√©gorie pr√©dite : {result['category']} (confiance : {result['confidence']:.2f})"
                elif model_type == "rnn":
                    result = self.classifier_rnn.predict(text)
                    return f"Cat√©gorie pr√©dite : {result['category']} (confiance : {result['confidence']:.2f})"
                elif model_type == "keras":
                    result = self.classifier_keras.predict(text)
                    return f"Cat√©gorie pr√©dite : {result['category']} (confiance : {result['confidence']:.2f})"
                else:
                    return "Type de mod√®le DL non support√©"
            else:
                # Utilisation du mod√®le ML
                processed_text = preprocess_text(text)
                if self.classifier_model and self.vectorizer:
                    X = self.vectorizer.transform([processed_text])
                    prediction = self.classifier_model.predict(X)[0]
                    probabilities = self.classifier_model.predict_proba(X)[0]
                    confidence = max(probabilities)
                    return f"Cat√©gorie pr√©dite : {prediction} (confiance : {confidence:.2f})"
                else:
                    return "Mod√®le ML non initialis√©"
        except Exception as e:
            return f"Erreur lors de la classification : {str(e)}"

    def cleanup(self):
        """
        Nettoie les ressources utilis√©es par le chatbot.
        Lib√®re la m√©moire GPU si disponible.
        """
        if self.model:
            del self.model
        if self.tokenizer:
            del self.tokenizer
        if self.classifier_model:
            del self.classifier_model
        if self.vectorizer:
            del self.vectorizer
        if self.bert_tokenizer:
            del self.bert_tokenizer
        if self.bert_model:
            del self.bert_model
        if self.optimized_model:
            del self.optimized_model
        torch.cuda.empty_cache() if torch.cuda.is_available() else None

if __name__ == "__main__":
    # Test simple du chatbot en mode console
    chatbot = Chatbot()
    print("Chatbot initialis√© ! Tapez 'quit' pour quitter.")
    
    while True:
        user_input = input("Vous : ")
        if user_input.lower() in ["quit", "exit", "bye"]:
            print("Chatbot : Au revoir !")
            chatbot.cleanup()
            break
            
        response = chatbot.generate_response(user_input)
        print("Chatbot :", response)
