"""
Module principal du chatbot.
Ce script impl√©mente la logique du chatbot, incluant la classification de texte,
la g√©n√©ration de r√©ponses et l'int√©gration avec diff√©rents mod√®les de langage.
"""

import os
import torch
import joblib
import numpy as np
from typing import Optional, Dict, Any
import time
from sklearn.model_selection import GridSearchCV
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from transformers import AutoTokenizer, AutoModelForCausalLM, BertTokenizer, BertForSequenceClassification

from utils import summarize_text, search_wikipedia, preprocess_text

class Chatbot:
    """
    Classe principale du chatbot.
    G√®re l'initialisation des mod√®les, la classification de texte et la g√©n√©ration de r√©ponses.
    """
    
    def __init__(self):
        """Initialise le chatbot avec les mod√®les n√©cessaires."""
        self.model_name = "microsoft/DialoGPT-medium"  # Mod√®le de langage utilis√©
        self.tokenizer = None  # Tokenizer pour le mod√®le de langage
        self.model = None  # Mod√®le de langage
        self.classifier_model = None  # Mod√®le de classification ML
        self.vectorizer = None  # Vectoriseur pour la classification ML
        self.bert_tokenizer = None  # Tokenizer BERT
        self.bert_model = None  # Mod√®le BERT pour la classification
        self.optimized_model = None  # Mod√®le ML optimis√©
        self.bert_labels = [
            "accueil", "m√©t√©o", "technologie", "cuisine", "actualit√©s",
            "√©ducation", "histoire", "sport"
        ]
        self.initialize_models()

    def optimize_ml_model(self, X_train, y_train):
        """
        Optimise les hyperparam√®tres du mod√®le ML (Naive Bayes) avec GridSearchCV.
        Args:
            X_train: Donn√©es d'entra√Ænement
            y_train: Labels d'entra√Ænement
        """
        try:
            print("üîÑ Optimisation des hyperparam√®tres du mod√®le ML...")
            
            # Cr√©ation du pipeline
            pipeline = Pipeline([
                ('tfidf', TfidfVectorizer()),
                ('clf', MultinomialNB())
            ])
            
            # Param√®tres √† optimiser
            param_grid = {
                'tfidf__max_features': [5000, 10000, 15000],
                'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],
                'tfidf__min_df': [1, 2, 3],
                'clf__alpha': [0.1, 0.5, 1.0]
            }
            
            # GridSearchCV
            grid_search = GridSearchCV(
                pipeline,
                param_grid,
                cv=5,
                scoring='accuracy',
                n_jobs=-1,
                verbose=1
            )
            
            # Entra√Ænement
            grid_search.fit(X_train, y_train)
            
            # Sauvegarde du meilleur mod√®le
            self.optimized_model = grid_search.best_estimator_
            self.vectorizer = self.optimized_model.named_steps['tfidf']
            self.classifier_model = self.optimized_model.named_steps['clf']
            
            print(f"‚úÖ Optimisation termin√©e ! Meilleurs param√®tres : {grid_search.best_params_}")
            print(f"Score de validation : {grid_search.best_score_:.3f}")
            
            # Sauvegarde du mod√®le optimis√©
            joblib.dump(self.optimized_model, "app/optimized_model.joblib")
            
        except Exception as e:
            print(f"‚ùå Erreur lors de l'optimisation du mod√®le : {str(e)}")
            raise

    def create_base_model(self):
        """
        Cr√©e un mod√®le de base avec des donn√©es d'exemple.
        """
        try:
            print("üîÑ Cr√©ation d'un mod√®le de base...")
            
            # Donn√©es d'exemple pour l'entra√Ænement
            texts = [
                "Bonjour, comment puis-je vous aider ?",
                "Quel temps fait-il aujourd'hui ?",
                "Je cherche des informations sur l'intelligence artificielle",
                "Comment faire une recette de g√¢teau au chocolat ?",
                "Quelles sont les derni√®res actualit√©s ?",
                "Je cherche des cours de math√©matiques",
                "Parlez-moi de l'histoire de France",
                "Quels sont les r√©sultats du match de football ?"
            ]
            
            labels = [
                "accueil",
                "m√©t√©o",
                "technologie",
                "cuisine",
                "actualit√©s",
                "√©ducation",
                "histoire",
                "sport"
            ]
            
            # Pr√©traitement des textes
            processed_texts = [preprocess_text(text) for text in texts]
            
            # Cr√©ation et entra√Ænement du mod√®le optimis√©
            self.optimize_ml_model(processed_texts, labels)
            
            print("‚úÖ Mod√®le de base cr√©√© avec succ√®s !")
            
        except Exception as e:
            print(f"‚ùå Erreur lors de la cr√©ation du mod√®le de base : {str(e)}")
            raise

    def initialize_models(self):
        """
        Initialise tous les mod√®les n√©cessaires au fonctionnement du chatbot.
        Charge les mod√®les existants ou en cr√©e de nouveaux si n√©cessaire.
        """
        try:
            print("üîÑ Chargement des mod√®les...")
            
            # Chargement du mod√®le DialoGPT pour la g√©n√©ration de r√©ponses
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            self.model = AutoModelForCausalLM.from_pretrained(self.model_name)
            
            # Chargement des mod√®les de classification ML
            model_path = "app/model.joblib"
            vectorizer_path = "app/vectorizer.joblib"
            
            if os.path.exists(model_path) and os.path.exists(vectorizer_path):
                self.classifier_model = joblib.load(model_path)
                self.vectorizer = joblib.load(vectorizer_path)
            else:
                print("‚ö†Ô∏è Mod√®les de classification non trouv√©s. Cr√©ation d'un mod√®le de base...")
                self.create_base_model()
            
            # Chargement du mod√®le BERT pour la classification DL
            self.bert_tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
            self.bert_model = BertForSequenceClassification.from_pretrained(
                "bert-base-uncased",
                num_labels=len(self.bert_labels),
                ignore_mismatched_sizes=True
            )
            
            print("‚úÖ Mod√®les charg√©s avec succ√®s !")
        except Exception as e:
            print(f"‚ùå Erreur lors de l'initialisation des mod√®les: {str(e)}")
            raise

    def classify_with_bert(self, text: str) -> dict:
        """
        Classifie un texte avec BERT et retourne la cat√©gorie pr√©dite et la confiance.
        """
        inputs = self.bert_tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=128)
        outputs = self.bert_model(**inputs)
        logits = outputs.logits.detach().numpy()[0]
        probs = np.exp(logits) / np.sum(np.exp(logits))
        pred_idx = np.argmax(probs)
        return {
            "category": self.bert_labels[pred_idx] if pred_idx < len(self.bert_labels) else "autre",
            "confidence": float(probs[pred_idx]),
            "embeddings": outputs.hidden_states[-1][0][0].detach().numpy().tolist() if hasattr(outputs, 'hidden_states') else None
        }

    def search_wikipedia(self, query: str) -> str:
        """
        Effectue une recherche Wikipedia et retourne un r√©sum√©.
        Args:
            query (str): La requ√™te de recherche
        Returns:
            str: Le r√©sum√© de l'article Wikipedia
        """
        try:
            return search_wikipedia(query)
        except Exception as e:
            print(f"‚ùå Erreur lors de la recherche Wikipedia: {str(e)}")
            return "D√©sol√©, je n'ai pas pu trouver d'informations sur ce sujet dans Wikipedia."

    def summarize_text(self, text: str, use_dl: bool = True) -> str:
        """
        R√©sume un texte en utilisant soit BART (DL) soit TF-IDF (ML).
        Args:
            text (str): Le texte √† r√©sumer
            use_dl (bool): Si True, utilise BART (DL), sinon utilise TF-IDF (ML)
        Returns:
            str: Le r√©sum√© du texte
        """
        try:
            if use_dl:
                return summarize_text(text)  # Utilise BART (DL)
            else:
                # M√©thode ML avec TF-IDF
                from nltk.tokenize import sent_tokenize
                import numpy as np
                from sklearn.feature_extraction.text import TfidfVectorizer
                
                # D√©coupage du texte en phrases
                sentences = sent_tokenize(text)
                
                # Cr√©ation du vectoriseur TF-IDF
                vectorizer = TfidfVectorizer(stop_words='english')
                tfidf_matrix = vectorizer.fit_transform(sentences)
                
                # Calcul des scores pour chaque phrase
                sentence_scores = np.sum(tfidf_matrix.toarray(), axis=1)
                
                # S√©lection des phrases les plus importantes
                num_sentences = min(3, len(sentences))  # Limite √† 3 phrases
                top_indices = sentence_scores.argsort()[-num_sentences:][::-1]
                top_indices.sort()  # Garde l'ordre original
                
                # Construction du r√©sum√©
                summary = [sentences[i] for i in top_indices]
                return " ".join(summary)
                
        except Exception as e:
            print(f"‚ùå Erreur lors du r√©sum√© du texte: {str(e)}")
            return "D√©sol√©, je n'ai pas pu r√©sumer ce texte."

    def generate_response(self, user_input: str, use_dl: bool = False) -> Dict[str, Any]:
        """
        G√©n√®re une r√©ponse bas√©e sur l'entr√©e de l'utilisateur.
        Combine la classification et la g√©n√©ration de texte.
        Args:
            user_input (str): Le message de l'utilisateur
            use_dl (bool): Si True, utilise BERT pour la classification
        Returns:
            Dict[str, Any]: La r√©ponse g√©n√©r√©e avec sa cat√©gorie et sa confiance
        """
        try:
            response = {
                "text": "",
                "category": None,
                "confidence": None,
                "embeddings": None
            }
            
            # Classification du texte
            if use_dl and self.bert_model and self.bert_tokenizer:
                bert_result = self.classify_with_bert(user_input)
                response["category"] = bert_result["category"]
                response["confidence"] = bert_result["confidence"]
                response["embeddings"] = bert_result["embeddings"]
            elif self.optimized_model:  # Utilisation du mod√®le optimis√© si disponible
                clean_text = preprocess_text(user_input)
                prediction = self.optimized_model.predict([clean_text])[0]
                confidence = self.optimized_model.predict_proba([clean_text]).max()
                response["category"] = prediction
                response["confidence"] = float(confidence)
            elif self.classifier_model and self.vectorizer:  # Fallback sur le mod√®le non optimis√©
                clean_text = preprocess_text(user_input)
                vect = self.vectorizer.transform([clean_text])
                prediction = self.classifier_model.predict(vect)[0]
                confidence = self.classifier_model.predict_proba(vect).max()
                response["category"] = prediction
                response["confidence"] = float(confidence)
            
            # G√©n√©ration de r√©ponse avec DialoGPT
            input_ids = self.tokenizer.encode(user_input + self.tokenizer.eos_token, return_tensors='pt')
            output = self.model.generate(
                input_ids,
                max_length=1000,
                pad_token_id=self.tokenizer.eos_token_id,
                no_repeat_ngram_size=3,
                do_sample=True,
                top_k=100,
                top_p=0.7,
                temperature=0.8
            )
            response["text"] = self.tokenizer.decode(output[:, input_ids.shape[-1]:][0], skip_special_tokens=True)
            return response
            
        except Exception as e:
            print(f"‚ùå Erreur lors de la g√©n√©ration de la r√©ponse: {str(e)}")
            return {
                "text": "D√©sol√©, une erreur s'est produite. Veuillez r√©essayer.",
                "category": None,
                "confidence": None,
                "embeddings": None
            }

    def cleanup(self):
        """
        Nettoie les ressources utilis√©es par le chatbot.
        Lib√®re la m√©moire GPU si disponible.
        """
        if self.model:
            del self.model
        if self.tokenizer:
            del self.tokenizer
        if self.classifier_model:
            del self.classifier_model
        if self.vectorizer:
            del self.vectorizer
        if self.bert_tokenizer:
            del self.bert_tokenizer
        if self.bert_model:
            del self.bert_model
        if self.optimized_model:
            del self.optimized_model
        torch.cuda.empty_cache() if torch.cuda.is_available() else None

if __name__ == "__main__":
    # Test simple du chatbot en mode console
    chatbot = Chatbot()
    print("Chatbot initialis√© ! Tapez 'quit' pour quitter.")
    
    while True:
        user_input = input("Vous : ")
        if user_input.lower() in ["quit", "exit", "bye"]:
            print("Chatbot : Au revoir !")
            chatbot.cleanup()
            break
            
        response = chatbot.generate_response(user_input)
        print("Chatbot :", response)
