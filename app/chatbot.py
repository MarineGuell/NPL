"""
Module principal du chatbot.
Ce script impl√©mente la logique du chatbot, incluant la classification de texte,
la g√©n√©ration de r√©ponses et l'int√©gration avec diff√©rents mod√®les de langage.
"""

import os
import torch
import joblib
import numpy as np
from typing import Optional, Dict, Any
import time
from functools import lru_cache
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from transformers import AutoTokenizer, AutoModelForCausalLM, BertTokenizer, BertForSequenceClassification, pipeline
from sklearn.metrics import accuracy_score

from utils import summarize_text, search_wikipedia, preprocess_text
from models import SupervisedClassifier, DeepLearningClassifier, RNNTextClassifier, KerasTextClassifier

class Chatbot:
    """
    Classe principale du chatbot.
    G√®re l'initialisation des mod√®les, la classification de texte et la g√©n√©ration de r√©ponses.
    """
    
    def __init__(self):
        """Initialise le chatbot avec les mod√®les n√©cessaires."""
        self.model_name = "microsoft/DialoGPT-medium"  # Mod√®le de langage utilis√©
        self.tokenizer = None  # Tokenizer pour le mod√®le de langage
        self.model = None  # Mod√®le de langage
        self.classifier_model = None  # Mod√®le de classification ML
        self.vectorizer = None  # Vectoriseur pour la classification ML
        self.bert_tokenizer = None  # Tokenizer BERT
        self.bert_model = None  # Mod√®le BERT pour la classification
        self.optimized_model = None  # Mod√®le ML optimis√©
        self.bert_labels = [
            "accueil", "m√©t√©o", "technologie", "cuisine", "actualit√©s",
            "√©ducation", "histoire", "sport"
        ]
        self.classifier_ml = SupervisedClassifier()
        self.classifier_dl = DeepLearningClassifier()
        self.classifier_rnn = RNNTextClassifier()
        self.classifier_keras = KerasTextClassifier()
        self.sentiment_analyzer = pipeline("sentiment-analysis")
        self.initialize_models()

    def optimize_ml_model(self, texts, labels):
        """Optimise le mod√®le ML avec GridSearchCV"""
        try:
            # Pr√©traitement des textes
            processed_texts = [self.preprocess_text(text) for text in texts]
            
            # Cr√©ation du pipeline
            pipeline = Pipeline([
                ('tfidf', TfidfVectorizer(max_features=5000)),
                ('clf', MultinomialNB())
            ])
            
            # D√©finition des param√®tres √† tester
            param_grid = {
                'tfidf__max_features': [3000, 5000, 7000],
                'tfidf__ngram_range': [(1, 1), (1, 2)],
                'clf__alpha': [0.1, 0.5, 1.0]
            }
            
            # Calcul du nombre optimal de plis
            n_samples = len(processed_texts)
            n_splits = min(5, n_samples // 2)  # Utilise le minimum entre 5 et la moiti√© du nombre d'√©chantillons
            if n_splits < 2:
                n_splits = 2  # Minimum 2 plis pour la validation crois√©e
            
            # Cr√©ation du GridSearchCV avec le nombre de plis adapt√©
            grid_search = GridSearchCV(
                pipeline,
                param_grid,
                cv=n_splits,
                n_jobs=-1,
                verbose=1
            )
            
            # Division des donn√©es en ensembles d'entra√Ænement et de test
            X_train, X_test, y_train, y_test = train_test_split(
                processed_texts, labels,
                test_size=0.2,
                random_state=42,
                stratify=labels
            )
            
            # Entra√Ænement du mod√®le
            grid_search.fit(X_train, y_train)
            
            # Sauvegarde du meilleur mod√®le
            self.ml_model = grid_search.best_estimator_
            
            # √âvaluation du mod√®le
            y_pred = self.ml_model.predict(X_test)
            accuracy = accuracy_score(y_test, y_pred)
            print(f"Meilleure pr√©cision : {accuracy:.2f}")
            print(f"Meilleurs param√®tres : {grid_search.best_params_}")
            
            return accuracy
            
        except Exception as e:
            print(f"Erreur lors de l'optimisation du mod√®le ML : {str(e)}")
            return None

    def create_base_model(self):
        """
        Cr√©e un mod√®le de base avec des donn√©es d'exemple.
        """
        try:
            print("üîÑ Cr√©ation d'un mod√®le de base...")
            
            # Donn√©es d'exemple pour l'entra√Ænement
            texts = [
                "Bonjour, comment puis-je vous aider ?",
                "Quel temps fait-il aujourd'hui ?",
                "Je cherche des informations sur l'intelligence artificielle",
                "Comment faire une recette de g√¢teau au chocolat ?",
                "Quelles sont les derni√®res actualit√©s ?",
                "Je cherche des cours de math√©matiques",
                "Parlez-moi de l'histoire de France",
                "Quels sont les r√©sultats du match de football ?"
            ]
            
            labels = [
                "accueil",
                "m√©t√©o",
                "technologie",
                "cuisine",
                "actualit√©s",
                "√©ducation",
                "histoire",
                "sport"
            ]
            
            # Pr√©traitement des textes
            processed_texts = [preprocess_text(text) for text in texts]
            
            # Cr√©ation et entra√Ænement du mod√®le optimis√©
            self.optimize_ml_model(processed_texts, labels)
            
            print("‚úÖ Mod√®le de base cr√©√© avec succ√®s !")
            
        except Exception as e:
            print(f"‚ùå Erreur lors de la cr√©ation du mod√®le de base : {str(e)}")
            raise

    def initialize_models(self):
        """
        Initialise tous les mod√®les n√©cessaires au fonctionnement du chatbot.
        Charge les mod√®les existants ou en cr√©e de nouveaux si n√©cessaire.
        """
        try:
            print("üîÑ Chargement des mod√®les...")
            
            # Chargement du mod√®le DialoGPT pour la g√©n√©ration de r√©ponses
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            self.model = AutoModelForCausalLM.from_pretrained(self.model_name)
            
            # Chargement des mod√®les de classification ML
            model_path = "app/model.joblib"
            vectorizer_path = "app/vectorizer.joblib"
            
            if os.path.exists(model_path) and os.path.exists(vectorizer_path):
                self.classifier_model = joblib.load(model_path)
                self.vectorizer = joblib.load(vectorizer_path)
            else:
                print("‚ö†Ô∏è Mod√®les de classification non trouv√©s. Cr√©ation d'un mod√®le de base...")
                self.create_base_model()
            
            # Chargement du mod√®le BERT pour la classification DL
            self.bert_tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
            self.bert_model = BertForSequenceClassification.from_pretrained(
                "bert-base-uncased",
                num_labels=len(self.bert_labels),
                ignore_mismatched_sizes=True
            )
            
            print("‚úÖ Mod√®les charg√©s avec succ√®s !")
        except Exception as e:
            print(f"‚ùå Erreur lors de l'initialisation des mod√®les: {str(e)}")
            raise

    def classify_with_bert(self, text: str) -> dict:
        """
        Classifie un texte avec BERT et retourne la cat√©gorie pr√©dite et la confiance.
        """
        try:
            inputs = self.bert_tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=128)
            with torch.no_grad():
                outputs = self.bert_model(**inputs)
            logits = outputs.logits.detach().numpy()[0]
            probs = np.exp(logits) / np.sum(np.exp(logits))
            pred_idx = np.argmax(probs)
            return {
                "category": self.bert_labels[pred_idx] if pred_idx < len(self.bert_labels) else "autre",
                "confidence": float(probs[pred_idx]),
                "embeddings": outputs.hidden_states[-1][0][0].detach().numpy().tolist() if hasattr(outputs, 'hidden_states') else None
            }
        except Exception as e:
            print(f"‚ùå Erreur lors de la classification BERT: {str(e)}")
            return {"category": "erreur", "confidence": 0.0, "embeddings": None}

    def search_wikipedia(self, query: str) -> str:
        """
        Recherche des informations sur Wikipedia.
        
        Args:
            query (str): La requ√™te de recherche
            
        Returns:
            str: Les informations trouv√©es
        """
        try:
            return search_wikipedia(query)
        except Exception as e:
            return f"Erreur lors de la recherche: {str(e)}"

    def summarize_text(self, text: str, use_dl: bool = True) -> str:
        """
        R√©sume un texte.
        
        Args:
            text (str): Le texte √† r√©sumer
            use_dl (bool): Si True, utilise le mod√®le de deep learning
            
        Returns:
            str: Le r√©sum√© g√©n√©r√©
        """
        try:
            return summarize_text(text, use_dl)
        except Exception as e:
            return f"Erreur lors du r√©sum√©: {str(e)}"

    @lru_cache(maxsize=100)
    def generate_response(self, text: str, use_dl: bool = False, model_type: str = "bert") -> str:
        """
        G√©n√®re une r√©ponse en fonction du texte d'entr√©e.
        
        Args:
            text (str): Le texte √† analyser
            use_dl (bool): Si True, utilise le mod√®le de deep learning
            model_type (str): Type de mod√®le DL ("bert", "rnn" ou "keras")
            
        Returns:
            str: La r√©ponse g√©n√©r√©e
        """
        try:
            if use_dl:
                if model_type == "bert":
                    sentiment = self.sentiment_analyzer(text)[0]
                    return f"Sentiment: {sentiment['label']} (Confiance: {sentiment['score']:.2f})"
                elif model_type == "rnn":
                    predictions, probs = self.classifier_rnn.predict([text])
                    sentiment = "POSITIVE" if predictions[0] == 1 else "NEGATIVE"
                    confidence = probs[0][predictions[0]]
                    return f"Sentiment: {sentiment} (Confiance: {confidence:.2f})"
                else:  # keras
                    predictions, probs = self.classifier_keras.predict([text])
                    sentiment = "POSITIVE" if predictions[0] == 1 else "NEGATIVE"
                    confidence = probs[0][predictions[0]]
                    return f"Sentiment: {sentiment} (Confiance: {confidence:.2f})"
            else:
                sentiment = self.classifier_ml.predict([text])[0]
                return f"Sentiment: {sentiment}"
        except Exception as e:
            return f"Erreur lors de l'analyse: {str(e)}"

    def cleanup(self):
        """
        Nettoie les ressources utilis√©es par le chatbot.
        Lib√®re la m√©moire GPU si disponible.
        """
        if self.model:
            del self.model
        if self.tokenizer:
            del self.tokenizer
        if self.classifier_model:
            del self.classifier_model
        if self.vectorizer:
            del self.vectorizer
        if self.bert_tokenizer:
            del self.bert_tokenizer
        if self.bert_model:
            del self.bert_model
        if self.optimized_model:
            del self.optimized_model
        torch.cuda.empty_cache() if torch.cuda.is_available() else None

if __name__ == "__main__":
    # Test simple du chatbot en mode console
    chatbot = Chatbot()
    print("Chatbot initialis√© ! Tapez 'quit' pour quitter.")
    
    while True:
        user_input = input("Vous : ")
        if user_input.lower() in ["quit", "exit", "bye"]:
            print("Chatbot : Au revoir !")
            chatbot.cleanup()
            break
            
        response = chatbot.generate_response(user_input)
        print("Chatbot :", response)
