"""
Module principal du chatbot.
Ce script impl√©mente la logique du chatbot, incluant la classification de texte,
la g√©n√©ration de r√©ponses et l'int√©gration avec diff√©rents mod√®les de langage.
"""

import joblib
import re
import string
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import os
from typing import Optional, Dict, Any
import numpy as np

from utils import summarize_text, search_wikipedia

# T√©l√©chargement des ressources NLTK n√©cessaires pour le traitement du texte
nltk.download('punkt')  # Pour la tokenization
nltk.download('stopwords')  # Pour la suppression des mots vides
nltk.download('wordnet')  # Pour la lemmatization

class Chatbot:
    """
    Classe principale du chatbot.
    G√®re l'initialisation des mod√®les, la classification de texte et la g√©n√©ration de r√©ponses.
    """
    
    def __init__(self):
        """Initialise le chatbot avec les mod√®les n√©cessaires."""
        self.model_name = "microsoft/DialoGPT-medium"  # Mod√®le de langage utilis√©
        self.tokenizer = None  # Tokenizer pour le mod√®le de langage
        self.model = None  # Mod√®le de langage
        self.classifier_model = None  # Mod√®le de classification
        self.vectorizer = None  # Vectoriseur pour la classification
        self.initialize_models()

    def create_base_model(self):
        """
        Cr√©e un mod√®le de classification de base avec des donn√©es d'exemple.
        Utilis√© si aucun mod√®le n'est trouv√© lors de l'initialisation.
        """
        print("üîÑ Cr√©ation d'un mod√®le de classification de base...")
        
        # Donn√©es d'exemple pour l'entra√Ænement initial
        texts = [
            "Bonjour, comment puis-je vous aider ?",
            "Quelle est la m√©t√©o aujourd'hui ?",
            "Pouvez-vous me donner des informations sur Python ?",
            "Je cherche des recettes de cuisine",
            "Comment fonctionne l'intelligence artificielle ?",
            "Quelles sont les derni√®res nouvelles ?",
            "Je voudrais apprendre √† programmer",
            "Pouvez-vous m'aider avec mes devoirs ?",
            "Je cherche des informations sur l'histoire",
            "Comment faire du sport √† la maison ?"
        ]
        
        # Cat√©gories correspondantes aux textes d'exemple
        categories = [
            "accueil",
            "m√©t√©o",
            "technologie",
            "cuisine",
            "technologie",
            "actualit√©s",
            "√©ducation",
            "√©ducation",
            "histoire",
            "sport"
        ]
        
        # Cr√©ation et entra√Ænement du vectoriseur TF-IDF
        self.vectorizer = TfidfVectorizer()
        X = self.vectorizer.fit_transform(texts)
        
        # Cr√©ation et entra√Ænement du mod√®le de classification Naive Bayes
        self.classifier_model = MultinomialNB()
        self.classifier_model.fit(X, categories)
        
        # Sauvegarde des mod√®les pour une utilisation future
        os.makedirs("app", exist_ok=True)
        joblib.dump(self.classifier_model, "app/model.joblib")
        joblib.dump(self.vectorizer, "app/vectorizer.joblib")
        
        print("‚úÖ Mod√®le de base cr√©√© et sauvegard√© !")

    def initialize_models(self):
        """
        Initialise tous les mod√®les n√©cessaires au fonctionnement du chatbot.
        Charge les mod√®les existants ou en cr√©e de nouveaux si n√©cessaire.
        """
        try:
            print("üîÑ Chargement des mod√®les...")
            
            # Chargement du mod√®le DialoGPT pour la g√©n√©ration de r√©ponses
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            self.model = AutoModelForCausalLM.from_pretrained(self.model_name)
            
            # Chargement des mod√®les de classification
            model_path = "app/model.joblib"
            vectorizer_path = "app/vectorizer.joblib"
            
            if os.path.exists(model_path) and os.path.exists(vectorizer_path):
                self.classifier_model = joblib.load(model_path)
                self.vectorizer = joblib.load(vectorizer_path)
            else:
                print("‚ö†Ô∏è Mod√®les de classification non trouv√©s. Cr√©ation d'un mod√®le de base...")
                self.create_base_model()
            
            print("‚úÖ Mod√®les charg√©s avec succ√®s !")
        except Exception as e:
            print(f"‚ùå Erreur lors de l'initialisation des mod√®les: {str(e)}")
            raise

    def preprocess_text(self, text: str) -> str:
        """
        Pr√©traite le texte pour la classification.
        Effectue la tokenization, la suppression des mots vides et la lemmatization.
        
        Args:
            text (str): Le texte √† pr√©traiter
            
        Returns:
            str: Le texte pr√©trait√©
        """
        text = text.lower()  # Conversion en minuscules
        text = text.translate(str.maketrans('', '', string.punctuation))  # Suppression de la ponctuation
        tokens = word_tokenize(text)  # Tokenization
        tokens = [word for word in tokens if word not in stopwords.words('english')]  # Suppression des mots vides
        tokens = [WordNetLemmatizer().lemmatize(word) for word in tokens]  # Lemmatization
        return " ".join(tokens)

    def generate_response(self, user_input: str) -> Dict[str, Any]:
        """
        G√©n√®re une r√©ponse bas√©e sur l'entr√©e de l'utilisateur.
        Combine la classification et la g√©n√©ration de texte.
        
        Args:
            user_input (str): Le message de l'utilisateur
            
        Returns:
            Dict[str, Any]: La r√©ponse g√©n√©r√©e avec sa cat√©gorie et sa confiance
        """
        try:
            response = {
                "text": "",
                "category": None,
                "confidence": None
            }
            
            # Classification du texte si le mod√®le est disponible
            if self.classifier_model and self.vectorizer:
                clean_text = self.preprocess_text(user_input)
                vect = self.vectorizer.transform([clean_text])
                prediction = self.classifier_model.predict(vect)[0]
                confidence = self.classifier_model.predict_proba(vect).max()
                response["category"] = prediction
                response["confidence"] = float(confidence)
            
            # G√©n√©ration de r√©ponse avec DialoGPT
            input_ids = self.tokenizer.encode(user_input + self.tokenizer.eos_token, 
                                            return_tensors='pt')
            
            # Configuration de la g√©n√©ration de texte
            output = self.model.generate(
                input_ids,
                max_length=1000,  # Longueur maximale de la r√©ponse
                pad_token_id=self.tokenizer.eos_token_id,
                no_repeat_ngram_size=3,  # √âvite la r√©p√©tition de phrases
                do_sample=True,  # Active la g√©n√©ration stochastique
                top_k=100,  # Limite le nombre de tokens consid√©r√©s
                top_p=0.7,  # Filtre les tokens par probabilit√© cumulative
                temperature=0.8  # Contr√¥le la cr√©ativit√© de la g√©n√©ration
            )
            
            # D√©codage de la r√©ponse g√©n√©r√©e
            response["text"] = self.tokenizer.decode(output[:, input_ids.shape[-1]:][0], 
                                                   skip_special_tokens=True)
            
            return response
            
        except Exception as e:
            print(f"‚ùå Erreur lors de la g√©n√©ration de la r√©ponse: {str(e)}")
            return {
                "text": "D√©sol√©, une erreur s'est produite. Veuillez r√©essayer.",
                "category": None,
                "confidence": None
            }

    def cleanup(self):
        """
        Nettoie les ressources utilis√©es par le chatbot.
        Lib√®re la m√©moire GPU si disponible.
        """
        if self.model:
            del self.model
        if self.tokenizer:
            del self.tokenizer
        if self.classifier_model:
            del self.classifier_model
        if self.vectorizer:
            del self.vectorizer
        torch.cuda.empty_cache() if torch.cuda.is_available() else None


if __name__ == "__main__":
    # Test simple du chatbot en mode console
    chatbot = Chatbot()
    print("ü§ñ Chatbot initialis√© ! Tapez 'quit' pour quitter.")
    
    while True:
        user_input = input("Vous : ")
        if user_input.lower() in ["quit", "exit", "bye"]:
            print("Chatbot : Au revoir !")
            chatbot.cleanup()
            break
            
        response = chatbot.generate_response(user_input)
        print("Chatbot :", response)
