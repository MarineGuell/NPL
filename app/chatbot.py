"""
Module Orchestrateur du Chatbot Kaeru - Pipeline Central de NLP

Ce module orchestre le pipeline complet du chatbot :
- Interface utilisateur (Streamlit) â†” Orchestrateur â†” ModÃ¨les
- PrÃ©traitement centralisÃ© (TextPreprocessor)
- Classification (ML : TF-IDF+Naive Bayes, DL : LSTM bidirectionnel)
- RÃ©sumÃ© (ML : similaritÃ© cosinus, DL : autoencodeur extractif)
- Recherche Wikipedia intelligente

Pipeline de donnÃ©es :
1. RÃ©ception texte utilisateur â†’ PrÃ©traitement (nettoyage, normalisation)
2. Transformation numÃ©rique (vectorisation TF-IDF ou tokenization)
3. PrÃ©diction avec modÃ¨le appropriÃ© (ML/DL selon fonction)
4. Formatage rÃ©ponse avec personnalitÃ© grenouille japonaise

Fonctions disponibles via l'interface :
- Classification ML : Pipeline optimisÃ© GridSearchCV
- Classification DL : LSTM bidirectionnel avec BatchNormalization  
- RÃ©sumÃ© ML : SimilaritÃ© cosinus TF-IDF (3 phrases les plus reprÃ©sentatives)
- RÃ©sumÃ© DL : Autoencodeur extractif (phrases les mieux reconstruites)
- Recherche Wikipedia : Extraction mots-clÃ©s + recherche intelligente

Tous les modÃ¨les sont automatiquement chargÃ©s depuis models/ et sauvegardÃ©s lors de l'entraÃ®nement.
"""

from utils import TextPreprocessor
from models import MLModel, DLModel, AutoencoderSummarizer
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.tokenize import sent_tokenize
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import nltk

# TÃ©lÃ©chargement automatique de punkt si nÃ©cessaire
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    print("ğŸ“¥ TÃ©lÃ©chargement automatique de punkt pour le chatbot...")
    try:
        nltk.download('punkt', quiet=True)
        print("âœ… punkt tÃ©lÃ©chargÃ© avec succÃ¨s")
    except Exception as e:
        print(f"âŒ Erreur lors du tÃ©lÃ©chargement de punkt: {e}")

class ChatbotOrchestrator:
    """
    Orchestre le pipeline complet :
    - Interface utilisateur (Streamlit)
    - PrÃ©traitement (TextPreprocessor)
    - Classification (ML et DL)
    - RÃ©sumÃ© (ML : TF-IDF/cosinus, DL : autoencodeur extractif)
    
    DÃ©tail du rÃ©sumÃ© DL (autoencodeur) :
    1. Lors de l'entraÃ®nement global, l'autoencodeur est entraÃ®nÃ© sur toutes les phrases du dataset.
    2. Pour rÃ©sumer un texte, on dÃ©coupe en phrases, on vectorise, on passe chaque phrase dans l'autoencodeur.
    3. On calcule l'erreur de reconstruction pour chaque phrase.
    4. On sÃ©lectionne les phrases avec l'erreur la plus faible (les plus "centrales").
    5. Le rÃ©sumÃ© est la concatÃ©nation de ces phrases dans l'ordre d'origine.
    """
    
    def __init__(self):
        """
        Initialise l'orchestrateur avec tous les composants nÃ©cessaires.
        """
        print("ğŸ¤– Initialisation de l'orchestrateur...")
        self.preprocessor = TextPreprocessor()
        self.ml_classifier = MLModel()
        self.dl_classifier = DLModel()
        
        # Le modÃ¨le de rÃ©sumÃ© DL (Autoencodeur) est chargÃ© ici
        print("ğŸ”„ Chargement du modÃ¨le de rÃ©sumÃ© (Autoencodeur)...")
        self.autoencoder_summarizer = AutoencoderSummarizer()
        
        # VÃ©rification automatique si les modÃ¨les sont entraÃ®nÃ©s
        self.is_trained = self._check_models_trained()
        print("âœ… Orchestrateur prÃªt.")

    def _check_models_trained(self):
        """
        VÃ©rifie si les modÃ¨les sont dÃ©jÃ  entraÃ®nÃ©s et disponibles.
        """
        ml_trained = (self.ml_classifier.model is not None and 
                     self.ml_classifier.vectorizer is not None)
        dl_trained = self.dl_classifier.model is not None
        autoencoder_trained = self.autoencoder_summarizer.model is not None
        
        if ml_trained and dl_trained and autoencoder_trained:
            print("âœ… Tous les modÃ¨les sont chargÃ©s et prÃªts Ã  l'utilisation")
            return True
        elif ml_trained and dl_trained:
            print("âœ… ModÃ¨les de classification chargÃ©s (autoencodeur non disponible)")
            return True
        else:
            print("âš ï¸ ModÃ¨les non entraÃ®nÃ©s - veuillez exÃ©cuter le script d'entraÃ®nement")
            return False

    def train_models(self, texts, labels):
        """
        EntraÃ®ne tous les modÃ¨les :
        - ML (TF-IDF + Naive Bayes)
        - DL (LSTM bidirectionnel)
        - Autoencodeur pour le rÃ©sumÃ© extractif
        
        Ã‰tapes dÃ©taillÃ©es :
        1. PrÃ©traitement des textes (nettoyage, normalisation)
        2. EntraÃ®nement du modÃ¨le ML + Ã©valuation
        3. PrÃ©paration et entraÃ®nement du modÃ¨le DL
        4. EntraÃ®nement de l'autoencodeur sur toutes les phrases du dataset
        """
        print("ğŸ”„ PrÃ©traitement des textes pour l'entraÃ®nement...")
        processed_texts = self.preprocessor.transform(texts)
        
        print("ğŸ”„ EntraÃ®nement du modÃ¨le ML...")
        self.ml_classifier.train(processed_texts, labels)
        self.ml_classifier.evaluate()
        
        print("ğŸ”„ PrÃ©paration et entraÃ®nement du modÃ¨le DL...")
        X_dl, y_dl = self.dl_classifier.prepare(processed_texts, labels)
        self.dl_classifier.train(X_dl, y_dl)
        
        # EntraÃ®nement de l'autoencodeur pour le rÃ©sumÃ©
        print("ğŸ”„ EntraÃ®nement de l'autoencodeur pour le rÃ©sumÃ©...")
        self.autoencoder_summarizer.train(texts.tolist())
        
        self.is_trained = True
        print("âœ… ModÃ¨les entraÃ®nÃ©s avec succÃ¨s.")

    def classify(self, text, model_type='ml'):
        """
        Classe un texte donnÃ© en utilisant le modÃ¨le spÃ©cifiÃ©.
        
        Args:
            text (str): Le texte brut Ã  classifier.
            model_type (str): 'ml' ou 'dl'.
            
        Returns:
            str: La prÃ©diction formatÃ©e.
        """
        if not self.is_trained:
            return "The models aren't trained yet! ğŸ¸ Please run the training script first, kero!"

        print(f"ğŸ”„ Classification with {model_type.upper()} model...")
        # PrÃ©traitement complet du texte d'entrÃ©e
        processed_text = self.preprocessor.normalize(self.preprocessor.clean(text))
        
        if model_type == 'ml':
            prediction = self.ml_classifier.predict([processed_text])[0]
            proba = self.ml_classifier.predict_proba([processed_text])[0].max()
        else: # 'dl'
            prediction = self.dl_classifier.predict([processed_text])[0]
            proba = self.dl_classifier.predict_proba([processed_text])[0].max()
        
        print(f"ğŸ¸ Debug - PrÃ©diction: {prediction}, ProbabilitÃ© max: {proba:.3f}")
        
        # RÃ©ponses personnalisÃ©es selon la confiance
        if proba > 0.8:
            response = f"*hops excitedly* ğŸ¸ This text is definitely about **{prediction}**! I'm {proba:.1%} confident, kero!"
        elif proba > 0.6:
            response = f"*tilts head thoughtfully* ğŸ¸ I think this text is about **{prediction}**. I'm {proba:.1%} sure, kero!"
        else:
            response = f"*croaks uncertainly* ğŸ¸ Hmm... I'm not very confident, but I'd say it's about **{prediction}** ({proba:.1%} sure). Maybe I need more training, kero!"
        
        print(f"ğŸ¸ Debug - RÃ©ponse gÃ©nÃ©rÃ©e: {response}")
        return response

    def summarize(self, text, model_type='dl'):
        """
        RÃ©sume un texte donnÃ© en utilisant la mÃ©thode spÃ©cifiÃ©e.
        - 'ml' : rÃ©sumÃ© extractif TF-IDF/cosinus (phrases les plus similaires au texte global)
        - 'dl' : rÃ©sumÃ© extractif autoencodeur (phrases les mieux reconstruites)
        
        DÃ©tail du rÃ©sumÃ© autoencodeur :
        1. DÃ©coupage du texte en phrases
        2. Vectorisation de chaque phrase
        3. Passage dans l'autoencodeur
        4. Calcul de l'erreur de reconstruction
        5. SÃ©lection des phrases avec l'erreur la plus faible
        6. Assemblage du rÃ©sumÃ©
        """
        print(f"ğŸ”„ Summarization with {model_type.upper()} method...")
        if model_type == 'ml':
            # === MÃ‰THODE EXTRACTIVE BASÃ‰E SUR LA SIMILARITÃ‰ COSINUS ===
            # 
            # Ã‰TAPE 1: Tokenisation du texte en phrases
            # DÃ©coupage du texte en phrases individuelles pour analyse
            sentences = sent_tokenize(text)
            if len(sentences) < 3:
                return "This text is too short to summarize! It's already quite concise."
            
            # Ã‰TAPE 2: Vectorisation TF-IDF du texte entier
            # - CrÃ©ation d'un vectorizer TF-IDF avec bigrammes (1-2 mots)
            # - Suppression des mots vides anglais pour se concentrer sur le contenu
            # - Le vectorizer "apprend" le vocabulaire du texte entier
            vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 2))
            text_vector = vectorizer.fit_transform([text])
            
            # Ã‰TAPE 3: Vectorisation de chaque phrase
            # - Utilisation du mÃªme vectorizer pour assurer la cohÃ©rence
            # - Chaque phrase est transformÃ©e en vecteur avec le mÃªme vocabulaire
            sentence_vectors = vectorizer.transform(sentences)
            
            # Ã‰TAPE 4: Calcul de la similaritÃ© cosinus
            # - Mesure de l'angle entre chaque phrase et le texte entier
            # - Valeur proche de 1 = trÃ¨s similaire, proche de 0 = trÃ¨s diffÃ©rent
            # - Les phrases avec la plus haute similaritÃ© sont les plus reprÃ©sentatives
            similarities = cosine_similarity(sentence_vectors, text_vector).flatten()
            
            # Ã‰TAPE 5: SÃ©lection des phrases les plus reprÃ©sentatives
            # - Tri des phrases par similaritÃ© dÃ©croissante
            # - SÃ©lection des 3 phrases les plus similaires au texte entier
            # - PrÃ©servation de l'ordre original pour maintenir la cohÃ©rence narrative
            num_sentences = min(3, len(sentences))
            top_indices = similarities.argsort()[-num_sentences:][::-1]
            top_indices.sort()  # Garder l'ordre original des phrases
            
            # Ã‰TAPE 6: Assemblage du rÃ©sumÃ©
            # - ConcatÃ©nation des phrases sÃ©lectionnÃ©es
            # - Maintien de la fluiditÃ© narrative
            summary = " ".join([sentences[i] for i in top_indices])
            return f"In short, your text says : {summary} Kero ğŸ¸"
        else: # 'dl'
            # === MÃ‰THODE EXTRACTIVE BASÃ‰E SUR L'AUTOENCODEUR ===
            try:
                summary = self.autoencoder_summarizer.summarize(text, num_sentences=3)
                return f"In short, your text says : {summary} Kero ğŸ¸"
            except Exception as e:
                return f"*croaks apologetically* ğŸ¸ Sorry, I couldn't summarize with the autoencoder: {str(e)}, kero!" 