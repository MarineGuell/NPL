"""
Module Orchestrateur du Chatbot.
Ce module connecte l'interface utilisateur, le pr√©traitement et les mod√®les.
"""

from utils import TextPreprocessor
from models import MLModel, DLModel, AutoencoderSummarizer
from transformers import pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.tokenize import sent_tokenize
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

class ChatbotOrchestrator:
    """
    Orchestre le pipeline complet :
    - Interface utilisateur (Streamlit)
    - Pr√©traitement (TextPreprocessor)
    - Classification (ML et DL)
    - R√©sum√© (ML : TF-IDF/cosinus, DL : autoencodeur extractif)
    
    D√©tail du r√©sum√© DL (autoencodeur) :
    1. Lors de l'entra√Ænement global, l'autoencodeur est entra√Æn√© sur toutes les phrases du dataset.
    2. Pour r√©sumer un texte, on d√©coupe en phrases, on vectorise, on passe chaque phrase dans l'autoencodeur.
    3. On calcule l'erreur de reconstruction pour chaque phrase.
    4. On s√©lectionne les phrases avec l'erreur la plus faible (les plus "centrales").
    5. Le r√©sum√© est la concat√©nation de ces phrases dans l'ordre d'origine.
    """
    
    def __init__(self):
        """
        Initialise l'orchestrateur avec tous les composants n√©cessaires.
        """
        print("ü§ñ Initialisation de l'orchestrateur...")
        self.preprocessor = TextPreprocessor()
        self.ml_classifier = MLModel()
        self.dl_classifier = DLModel()
        
        # Le mod√®le de r√©sum√© DL (Autoencodeur) est charg√© ici
        print("üîÑ Chargement du mod√®le de r√©sum√© (Autoencodeur)...")
        self.autoencoder_summarizer = AutoencoderSummarizer()
        self.is_trained = False
        print("‚úÖ Orchestrateur pr√™t.")

    def train_models(self, texts, labels):
        """
        Entra√Æne tous les mod√®les :
        - ML (TF-IDF + Naive Bayes)
        - DL (LSTM bidirectionnel)
        - Autoencodeur pour le r√©sum√© extractif
        
        √âtapes d√©taill√©es :
        1. Pr√©traitement des textes (nettoyage, normalisation)
        2. Entra√Ænement du mod√®le ML + √©valuation
        3. Pr√©paration et entra√Ænement du mod√®le DL
        4. Entra√Ænement de l'autoencodeur sur toutes les phrases du dataset
        """
        print("üîÑ Pr√©traitement des textes pour l'entra√Ænement...")
        processed_texts = self.preprocessor.transform(texts)
        
        print("üîÑ Entra√Ænement du mod√®le ML...")
        self.ml_classifier.train(processed_texts, labels)
        self.ml_classifier.evaluate()
        
        print("üîÑ Pr√©paration et entra√Ænement du mod√®le DL...")
        X_dl, y_dl = self.dl_classifier.prepare(processed_texts, labels)
        self.dl_classifier.train(X_dl, y_dl)
        
        # Entra√Ænement de l'autoencodeur pour le r√©sum√©
        print("üîÑ Entra√Ænement de l'autoencodeur pour le r√©sum√©...")
        self.autoencoder_summarizer.train(texts.tolist())
        
        self.is_trained = True
        print("‚úÖ Mod√®les entra√Æn√©s avec succ√®s.")

    def classify(self, text, model_type='ml'):
        """
        Classe un texte donn√© en utilisant le mod√®le sp√©cifi√©.
        
        Args:
            text (str): Le texte brut √† classifier.
            model_type (str): 'ml' ou 'dl'.
            
        Returns:
            str: La pr√©diction format√©e.
        """
        if not self.is_trained:
            return "The models aren't trained yet! üê∏ Please run the training script first, kero!"

        print(f"üîÑ Classification with {model_type.upper()} model...")
        # Pr√©traitement complet du texte d'entr√©e
        processed_text = self.preprocessor.normalize(self.preprocessor.clean(text))
        
        if model_type == 'ml':
            prediction = self.ml_classifier.predict([processed_text])[0]
            proba = self.ml_classifier.predict_proba([processed_text])[0].max()
        else: # 'dl'
            prediction = self.dl_classifier.predict([processed_text])[0]
            proba = self.dl_classifier.predict_proba([processed_text])[0].max()
        
        # R√©ponses personnalis√©es selon la confiance
        if proba > 0.8:
            return f"*hops excitedly* üê∏ This text is definitely about **{prediction}**! I'm {proba:.1%} confident, kero!"
        elif proba > 0.6:
            return f"*tilts head thoughtfully* üê∏ I think this text is about **{prediction}**. I'm {proba:.1%} sure, kero!"
        else:
            return f"*croaks uncertainly* üê∏ Hmm... I'm not very confident, but I'd say it's about **{prediction}** ({proba:.1%} sure). Maybe I need more training, kero!"

    def summarize(self, text, model_type='dl'):
        """
        R√©sume un texte donn√© en utilisant la m√©thode sp√©cifi√©e.
        - 'ml' : r√©sum√© extractif TF-IDF/cosinus (phrases les plus similaires au texte global)
        - 'dl' : r√©sum√© extractif autoencodeur (phrases les mieux reconstruites)
        
        D√©tail du r√©sum√© autoencodeur :
        1. D√©coupage du texte en phrases
        2. Vectorisation de chaque phrase
        3. Passage dans l'autoencodeur
        4. Calcul de l'erreur de reconstruction
        5. S√©lection des phrases avec l'erreur la plus faible
        6. Assemblage du r√©sum√©
        """
        print(f"üîÑ Summarization with {model_type.upper()} method...")
        if model_type == 'ml':
            # === M√âTHODE EXTRACTIVE BAS√âE SUR LA SIMILARIT√â COSINUS ===
            # 
            # √âTAPE 1: Tokenisation du texte en phrases
            # D√©coupage du texte en phrases individuelles pour analyse
            sentences = sent_tokenize(text)
            if len(sentences) < 3:
                return "*splashes water* üê∏ This text is too short to summarize, kero! It's already quite concise!"
            
            # √âTAPE 2: Vectorisation TF-IDF du texte entier
            # - Cr√©ation d'un vectorizer TF-IDF avec bigrammes (1-2 mots)
            # - Suppression des mots vides anglais pour se concentrer sur le contenu
            # - Le vectorizer "apprend" le vocabulaire du texte entier
            vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1, 2))
            text_vector = vectorizer.fit_transform([text])
            
            # √âTAPE 3: Vectorisation de chaque phrase
            # - Utilisation du m√™me vectorizer pour assurer la coh√©rence
            # - Chaque phrase est transform√©e en vecteur avec le m√™me vocabulaire
            sentence_vectors = vectorizer.transform(sentences)
            
            # √âTAPE 4: Calcul de la similarit√© cosinus
            # - Mesure de l'angle entre chaque phrase et le texte entier
            # - Valeur proche de 1 = tr√®s similaire, proche de 0 = tr√®s diff√©rent
            # - Les phrases avec la plus haute similarit√© sont les plus repr√©sentatives
            similarities = cosine_similarity(sentence_vectors, text_vector).flatten()
            
            # √âTAPE 5: S√©lection des phrases les plus repr√©sentatives
            # - Tri des phrases par similarit√© d√©croissante
            # - S√©lection des 3 phrases les plus similaires au texte entier
            # - Pr√©servation de l'ordre original pour maintenir la coh√©rence narrative
            num_sentences = min(3, len(sentences))
            top_indices = similarities.argsort()[-num_sentences:][::-1]
            top_indices.sort()  # Garder l'ordre original des phrases
            
            # √âTAPE 6: Assemblage du r√©sum√©
            # - Concat√©nation des phrases s√©lectionn√©es
            # - Maintien de la fluidit√© narrative
            summary = " ".join([sentences[i] for i in top_indices])
            return f"*jumps from lily pad to lily pad* üê∏ Here's what I found most representative, kero:\n\n{summary}"
        else: # 'dl'
            # === M√âTHODE EXTRACTIVE BAS√âE SUR L'AUTOENCODEUR ===
            try:
                summary = self.autoencoder_summarizer.summarize(text, num_sentences=3)
                return f"*dives deep into the pond of knowledge* üê∏ Here's my autoencoder summary, kero:\n\n{summary}"
            except Exception as e:
                return f"*croaks apologetically* üê∏ Sorry, I couldn't summarize with the autoencoder: {str(e)}, kero!" 