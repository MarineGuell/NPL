"""
Module principal du chatbot.
Ce script impl√©mente la logique du chatbot, incluant la classification de texte,
la g√©n√©ration de r√©ponses et l'int√©gration avec diff√©rents mod√®les de langage.
"""

import os
import torch
import joblib
import numpy as np
from typing import Optional, Dict, Any
import time
from functools import lru_cache
from sklearn.model_selection import GridSearchCV
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from transformers import AutoTokenizer, AutoModelForCausalLM, BertTokenizer, BertForSequenceClassification

from utils import summarize_text, search_wikipedia, preprocess_text

class Chatbot:
    """
    Classe principale du chatbot.
    G√®re l'initialisation des mod√®les, la classification de texte et la g√©n√©ration de r√©ponses.
    """
    
    def __init__(self):
        """Initialise le chatbot avec les mod√®les n√©cessaires."""
        self.model_name = "microsoft/DialoGPT-medium"  # Mod√®le de langage utilis√©
        self.tokenizer = None  # Tokenizer pour le mod√®le de langage
        self.model = None  # Mod√®le de langage
        self.classifier_model = None  # Mod√®le de classification ML
        self.vectorizer = None  # Vectoriseur pour la classification ML
        self.bert_tokenizer = None  # Tokenizer BERT
        self.bert_model = None  # Mod√®le BERT pour la classification
        self.optimized_model = None  # Mod√®le ML optimis√©
        self.bert_labels = [
            "accueil", "m√©t√©o", "technologie", "cuisine", "actualit√©s",
            "√©ducation", "histoire", "sport"
        ]
        self.initialize_models()

    def optimize_ml_model(self, X_train, y_train):
        """
        Optimise les hyperparam√®tres du mod√®le ML (Naive Bayes) avec GridSearchCV.
        Args:
            X_train: Donn√©es d'entra√Ænement
            y_train: Labels d'entra√Ænement
        """
        try:
            print("üîÑ Optimisation des hyperparam√®tres du mod√®le ML...")
            
            # Cr√©ation du pipeline
            pipeline = Pipeline([
                ('tfidf', TfidfVectorizer()),
                ('clf', MultinomialNB())
            ])
            
            # Param√®tres √† optimiser
            param_grid = {
                'tfidf__max_features': [5000, 10000, 15000],
                'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],
                'tfidf__min_df': [1, 2, 3],
                'clf__alpha': [0.1, 0.5, 1.0]
            }
            
            # GridSearchCV
            grid_search = GridSearchCV(
                pipeline,
                param_grid,
                cv=5,
                scoring='accuracy',
                n_jobs=-1,
                verbose=1
            )
            
            # Entra√Ænement
            grid_search.fit(X_train, y_train)
            
            # Sauvegarde du meilleur mod√®le
            self.optimized_model = grid_search.best_estimator_
            self.vectorizer = self.optimized_model.named_steps['tfidf']
            self.classifier_model = self.optimized_model.named_steps['clf']
            
            print(f"‚úÖ Optimisation termin√©e ! Meilleurs param√®tres : {grid_search.best_params_}")
            print(f"Score de validation : {grid_search.best_score_:.3f}")
            
            # Sauvegarde du mod√®le optimis√©
            joblib.dump(self.optimized_model, "app/optimized_model.joblib")
            
        except Exception as e:
            print(f"‚ùå Erreur lors de l'optimisation du mod√®le : {str(e)}")
            raise

    def create_base_model(self):
        """
        Cr√©e un mod√®le de base avec des donn√©es d'exemple.
        """
        try:
            print("üîÑ Cr√©ation d'un mod√®le de base...")
            
            # Donn√©es d'exemple pour l'entra√Ænement
            texts = [
                "Bonjour, comment puis-je vous aider ?",
                "Quel temps fait-il aujourd'hui ?",
                "Je cherche des informations sur l'intelligence artificielle",
                "Comment faire une recette de g√¢teau au chocolat ?",
                "Quelles sont les derni√®res actualit√©s ?",
                "Je cherche des cours de math√©matiques",
                "Parlez-moi de l'histoire de France",
                "Quels sont les r√©sultats du match de football ?"
            ]
            
            labels = [
                "accueil",
                "m√©t√©o",
                "technologie",
                "cuisine",
                "actualit√©s",
                "√©ducation",
                "histoire",
                "sport"
            ]
            
            # Pr√©traitement des textes
            processed_texts = [preprocess_text(text) for text in texts]
            
            # Cr√©ation et entra√Ænement du mod√®le optimis√©
            self.optimize_ml_model(processed_texts, labels)
            
            print("‚úÖ Mod√®le de base cr√©√© avec succ√®s !")
            
        except Exception as e:
            print(f"‚ùå Erreur lors de la cr√©ation du mod√®le de base : {str(e)}")
            raise

    def initialize_models(self):
        """
        Initialise tous les mod√®les n√©cessaires au fonctionnement du chatbot.
        Charge les mod√®les existants ou en cr√©e de nouveaux si n√©cessaire.
        """
        try:
            print("üîÑ Chargement des mod√®les...")
            
            # Chargement du mod√®le DialoGPT pour la g√©n√©ration de r√©ponses
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            self.model = AutoModelForCausalLM.from_pretrained(self.model_name)
            
            # Chargement des mod√®les de classification ML
            model_path = "app/model.joblib"
            vectorizer_path = "app/vectorizer.joblib"
            
            if os.path.exists(model_path) and os.path.exists(vectorizer_path):
                self.classifier_model = joblib.load(model_path)
                self.vectorizer = joblib.load(vectorizer_path)
            else:
                print("‚ö†Ô∏è Mod√®les de classification non trouv√©s. Cr√©ation d'un mod√®le de base...")
                self.create_base_model()
            
            # Chargement du mod√®le BERT pour la classification DL
            self.bert_tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
            self.bert_model = BertForSequenceClassification.from_pretrained(
                "bert-base-uncased",
                num_labels=len(self.bert_labels),
                ignore_mismatched_sizes=True
            )
            
            print("‚úÖ Mod√®les charg√©s avec succ√®s !")
        except Exception as e:
            print(f"‚ùå Erreur lors de l'initialisation des mod√®les: {str(e)}")
            raise

    def classify_with_bert(self, text: str) -> dict:
        """
        Classifie un texte avec BERT et retourne la cat√©gorie pr√©dite et la confiance.
        """
        try:
            inputs = self.bert_tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=128)
            with torch.no_grad():
                outputs = self.bert_model(**inputs)
            logits = outputs.logits.detach().numpy()[0]
            probs = np.exp(logits) / np.sum(np.exp(logits))
            pred_idx = np.argmax(probs)
            return {
                "category": self.bert_labels[pred_idx] if pred_idx < len(self.bert_labels) else "autre",
                "confidence": float(probs[pred_idx]),
                "embeddings": outputs.hidden_states[-1][0][0].detach().numpy().tolist() if hasattr(outputs, 'hidden_states') else None
            }
        except Exception as e:
            print(f"‚ùå Erreur lors de la classification BERT: {str(e)}")
            return {"category": "erreur", "confidence": 0.0, "embeddings": None}

    def search_wikipedia(self, query: str) -> str:
        """
        Effectue une recherche Wikipedia et retourne un r√©sum√©.
        Args:
            query (str): La requ√™te de recherche
        Returns:
            str: Le r√©sum√© de l'article Wikipedia
        """
        try:
            result = search_wikipedia(query)
            # Ajout du support LaTeX pour les formules math√©matiques
            result = result.replace("$", "$$")
            return result
        except Exception as e:
            print(f"‚ùå Erreur lors de la recherche Wikipedia: {str(e)}")
            return "D√©sol√©, je n'ai pas pu trouver d'informations sur ce sujet dans Wikipedia."

    def summarize_text(self, text: str, use_dl: bool = True) -> str:
        """
        R√©sume un texte en utilisant soit BART (DL) soit TF-IDF (ML).
        Args:
            text (str): Le texte √† r√©sumer
            use_dl (bool): Si True, utilise BART (DL), sinon utilise TF-IDF (ML)
        Returns:
            str: Le r√©sum√© du texte
        """
        try:
            if use_dl:
                # Utilise BART (DL) avec gestion des erreurs
                try:
                    return summarize_text(text)
                except Exception as e:
                    print(f"‚ùå Erreur BART, passage en mode ML: {str(e)}")
                    use_dl = False
            
            # M√©thode ML avec TF-IDF
            from nltk.tokenize import sent_tokenize
            import numpy as np
            from sklearn.feature_extraction.text import TfidfVectorizer
            
            # D√©coupage du texte en phrases
            sentences = sent_tokenize(text)
            
            if len(sentences) <= 3:
                return text
            
            # Cr√©ation du vectoriseur TF-IDF
            vectorizer = TfidfVectorizer(stop_words='english')
            tfidf_matrix = vectorizer.fit_transform(sentences)
            
            # Calcul des scores pour chaque phrase
            sentence_scores = np.sum(tfidf_matrix.toarray(), axis=1)
            
            # S√©lection des phrases les plus importantes
            num_sentences = min(3, len(sentences))  # Limite √† 3 phrases
            top_indices = sentence_scores.argsort()[-num_sentences:][::-1]
            top_indices.sort()  # Garde l'ordre original
            
            # Construction du r√©sum√©
            summary = [sentences[i] for i in top_indices]
            return " ".join(summary)
                
        except Exception as e:
            print(f"‚ùå Erreur lors du r√©sum√© du texte: {str(e)}")
            return "D√©sol√©, je n'ai pas pu r√©sumer ce texte."

    @lru_cache(maxsize=100)
    def generate_response(self, user_input: str, use_dl: bool = False) -> Dict[str, Any]:
        """
        G√©n√®re une r√©ponse bas√©e sur l'entr√©e de l'utilisateur.
        Combine la classification et la g√©n√©ration de texte.
        Args:
            user_input (str): Le message de l'utilisateur
            use_dl (bool): Si True, utilise BERT pour la classification
        Returns:
            dict: La r√©ponse g√©n√©r√©e avec la cat√©gorie et la confiance
        """
        try:
            # Classification du texte
            if use_dl:
                classification = self.classify_with_bert(user_input)
            else:
                # Classification ML
                processed_text = preprocess_text(user_input)
                if self.vectorizer and self.classifier_model:
                    features = self.vectorizer.transform([processed_text])
                    prediction = self.classifier_model.predict(features)[0]
                    confidence = self.classifier_model.predict_proba(features).max()
                    classification = {
                        "category": prediction,
                        "confidence": float(confidence),
                        "embeddings": None
                    }
                else:
                    raise Exception("Mod√®le ML non initialis√©")

            # G√©n√©ration de la r√©ponse avec DialoGPT
            input_ids = self.tokenizer.encode(user_input + self.tokenizer.eos_token, return_tensors='pt')
            response_ids = self.model.generate(
                input_ids,
                max_length=200,
                pad_token_id=self.tokenizer.eos_token_id,
                no_repeat_ngram_size=2,
                do_sample=True,
                top_k=50,
                top_p=0.9,
                temperature=0.7,
                early_stopping=True
            )
            response = self.tokenizer.decode(response_ids[:, input_ids.shape[-1]:][0], skip_special_tokens=True)

            return {
                "text": response,
                "category": classification["category"],
                "confidence": classification["confidence"],
                "embeddings": classification["embeddings"]
            }
        except Exception as e:
            print(f"‚ùå Erreur lors de la g√©n√©ration de la r√©ponse: {str(e)}")
            return {
                "text": "D√©sol√©, je n'ai pas pu traiter votre demande.",
                "category": "erreur",
                "confidence": 0.0,
                "embeddings": None
            }

    def cleanup(self):
        """
        Nettoie les ressources utilis√©es par le chatbot.
        Lib√®re la m√©moire GPU si disponible.
        """
        if self.model:
            del self.model
        if self.tokenizer:
            del self.tokenizer
        if self.classifier_model:
            del self.classifier_model
        if self.vectorizer:
            del self.vectorizer
        if self.bert_tokenizer:
            del self.bert_tokenizer
        if self.bert_model:
            del self.bert_model
        if self.optimized_model:
            del self.optimized_model
        torch.cuda.empty_cache() if torch.cuda.is_available() else None

if __name__ == "__main__":
    # Test simple du chatbot en mode console
    chatbot = Chatbot()
    print("Chatbot initialis√© ! Tapez 'quit' pour quitter.")
    
    while True:
        user_input = input("Vous : ")
        if user_input.lower() in ["quit", "exit", "bye"]:
            print("Chatbot : Au revoir !")
            chatbot.cleanup()
            break
            
        response = chatbot.generate_response(user_input)
        print("Chatbot :", response)
