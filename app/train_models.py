"""
Script Principal d'EntraÃ®nement - Chatbot Kaeru

Script principal pour entraÃ®ner tous les modÃ¨les du chatbot Kaeru.
Permet d'entraÃ®ner tous les modÃ¨les ou un modÃ¨le spÃ©cifique.
Sauvegarde automatique des modÃ¨les dans models/.
Ã‰valuation automatique des performances.

Usage :
    # EntraÃ®ner tous les modÃ¨les (recommandÃ©)
    python app/train_models.py --all
    
    # EntraÃ®ner un modÃ¨le spÃ©cifique
    python app/train_models.py --model ml
    python app/train_models.py --model dl
    python app/train_models.py --model autoencoder
    
    # EntraÃ®ner et Ã©valuer automatiquement
    python app/train_models.py --all --evaluate
    
    # Afficher l'aide
    python app/train_models.py --help

Pipeline d'EntraÃ®nement :
1. Chargement du dataset CSV (enriched_dataset_paragraphs.csv par dÃ©faut)
2. Nettoyage automatique (suppression doublons, valeurs manquantes)
3. PrÃ©traitement des textes (nettoyage, normalisation, POS-tagging)
4. EntraÃ®nement sÃ©quentiel :
   - ModÃ¨le ML : Pipeline TF-IDF + Naive Bayes optimisÃ© par GridSearchCV
   - ModÃ¨le DL : LSTM bidirectionnel avec tokenizer partagÃ©
   - Autoencodeur : EntraÃ®nement sur toutes les phrases du dataset
5. Sauvegarde automatique de tous les modÃ¨les dans models/
6. Ã‰valuation et visualisations automatiques

ModÃ¨les EntraÃ®nÃ©s :
- ml_model.joblib : Pipeline ML complet + vectorizer
- dl_model.h5 : ModÃ¨le LSTM + tokenizer partagÃ©
- dl_label_encoder.pkl : Label encoder pour le modÃ¨le DL
- autoencoder_summarizer.h5 : Autoencodeur + tokenizer partagÃ©
- shared_tokenizer.pkl : Tokenizer partagÃ© entre les modÃ¨les DL
"""

import os
import sys
import argparse
import pandas as pd
from datetime import datetime

# Ajout du chemin pour les imports
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from utils import DataLoader, TextPreprocessor
from models import MLModel, DLModel, AutoencoderSummarizer

def train_ml_model(texts, labels):
    """
    EntraÃ®ne le modÃ¨le ML.
    
    Args:
        texts: Les textes d'entraÃ®nement
        labels: Les labels d'entraÃ®nement
        
    Returns:
        MLModel: Le modÃ¨le entraÃ®nÃ©
    """
    print("\n" + "="*60)
    print("ğŸ¤– ENTRAÃNEMENT DU MODÃˆLE ML")
    print("="*60)
    
    ml_model = MLModel()
    ml_model.train(texts, labels)
    
    return ml_model

def train_dl_model(texts, labels):
    """
    EntraÃ®ne le modÃ¨le DL.
    
    Args:
        texts: Les textes d'entraÃ®nement
        labels: Les labels d'entraÃ®nement
        
    Returns:
        DLModel: Le modÃ¨le entraÃ®nÃ©
    """
    print("\n" + "="*60)
    print("ğŸ§  ENTRAÃNEMENT DU MODÃˆLE DL")
    print("="*60)
    
    dl_model = DLModel()
    X, y = dl_model.prepare(texts, labels)
    history, X_test, y_test = dl_model.train(X, y)
    
    return dl_model

def train_autoencoder(texts, labels):
    """
    EntraÃ®ne l'autoencodeur.
    
    Args:
        texts: Les textes d'entraÃ®nement
        labels: Les labels d'entraÃ®nement (non utilisÃ©s pour l'autoencodeur)
        
    Returns:
        AutoencoderSummarizer: L'autoencodeur entraÃ®nÃ©
    """
    print("\n" + "="*60)
    print("ğŸ”„ ENTRAÃNEMENT DE L'AUTOENCODEUR")
    print("="*60)
    
    # Afficher le chemin absolu du dataset
    dataset_path = os.path.abspath(os.path.join(os.path.dirname(__file__), args.dataset)) if 'args' in globals() else None
    if dataset_path:
        print(f"ğŸ“ Chemin absolu du dataset utilisÃ© : {dataset_path}")
    print(f"ğŸ“ Nombre de textes reÃ§us : {len(texts)}")
    
    # PrÃ©traitement spÃ©cial pour l'autoencodeur (prÃ©serve les phrases)
    print("ğŸ”§ PrÃ©traitement spÃ©cial pour l'autoencodeur...")
    preprocessor = TextPreprocessor()
    autoencoder_texts = preprocessor.transform_for_autoencoder(texts)
    print(f"ğŸ“ {len(autoencoder_texts)} textes prÃ©traitÃ©s pour l'autoencodeur")
    
    # Compter le nombre total de phrases
    from nltk.tokenize import sent_tokenize
    total_phrases = 0
    for text in autoencoder_texts:
        total_phrases += len(sent_tokenize(text))
    print(f"ğŸ“Š Nombre total de phrases trouvÃ©es dans le corpus : {total_phrases}")
    
    # Afficher quelques exemples pour vÃ©rification
    print("\nğŸ“‹ Exemples de textes prÃ©traitÃ©s pour l'autoencodeur:")
    for i, text in enumerate(autoencoder_texts[:3]):
        print(f"   {i+1}. {text[:100]}...")
    
    autoencoder = AutoencoderSummarizer()
    autoencoder.train(autoencoder_texts)
    
    # Ã‰valuation automatique de l'autoencodeur (comme pour ML et DL)
    print("\nğŸ“Š Ã‰VALUATION AUTOMATIQUE DE L'AUTOENCODEUR")
    print("="*60)
    autoencoder.evaluate()
    
    return autoencoder

def evaluate_models(ml_model=None, dl_model=None, autoencoder=None):
    """
    Ã‰value les modÃ¨les entraÃ®nÃ©s.
    
    Args:
        ml_model: Le modÃ¨le ML (optionnel)
        dl_model: Le modÃ¨le DL (optionnel)
        autoencoder: L'autoencodeur (optionnel) - dÃ©jÃ  Ã©valuÃ© automatiquement
    """
    print("\n" + "="*60)
    print("ğŸ“Š Ã‰VALUATION DES MODÃˆLES")
    print("="*60)
    
    if ml_model:
        ml_model.evaluate()
    
    if dl_model:
        dl_model.evaluate()
    
    # L'autoencodeur est dÃ©jÃ  Ã©valuÃ© automatiquement aprÃ¨s l'entraÃ®nement
    if autoencoder:
        print("âœ… Autoencodeur dÃ©jÃ  Ã©valuÃ© automatiquement aprÃ¨s l'entraÃ®nement")

def main():
    """Fonction principale."""
    parser = argparse.ArgumentParser(
        description="Script d'entraÃ®nement modulaire pour le chatbot Kaeru",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Exemples d'utilisation :
  python app/train_models.py --all                    # EntraÃ®ner tous les modÃ¨les
  python app/train_models.py --model ml               # EntraÃ®ner seulement le modÃ¨le ML
  python app/train_models.py --model dl               # EntraÃ®ner seulement le modÃ¨le DL
  python app/train_models.py --model autoencoder      # EntraÃ®ner seulement l'autoencodeur
  python app/train_models.py --all --evaluate         # EntraÃ®ner et Ã©valuer tous les modÃ¨les
        """
    )
    
    parser.add_argument(
        '--all',
        action='store_true',
        help='EntraÃ®ner tous les modÃ¨les (ML, DL, Autoencodeur)'
    )
    
    parser.add_argument(
        '--model',
        choices=['ml', 'dl', 'autoencoder'],
        help='EntraÃ®ner un modÃ¨le spÃ©cifique'
    )
    
    parser.add_argument(
        '--evaluate',
        action='store_true',
        help='Ã‰valuer automatiquement les modÃ¨les aprÃ¨s entraÃ®nement'
    )
    
    parser.add_argument(
        '--dataset',
        default='data/enriched_dataset_paragraphs_2.csv',
        help='Chemin vers le dataset (dÃ©faut: data/enriched_dataset_paragraphs_2.csv)'
    )
    
    args = parser.parse_args()
    
    # VÃ©rification des arguments
    if not args.all and not args.model:
        parser.error("Vous devez spÃ©cifier --all ou --model")
    
    if args.all and args.model:
        parser.error("Vous ne pouvez pas utiliser --all et --model en mÃªme temps")
    
    # Chemin du dataset
    dataset_path = os.path.abspath(os.path.join(os.path.dirname(__file__), args.dataset))
    if not os.path.exists(dataset_path):
        print(f"âŒ Erreur : Le fichier {dataset_path} n'existe pas!")
        print("ğŸ’¡ Assurez-vous d'avoir crÃ©Ã© le dataset avec create_data.py")
        return
    
    print("ğŸš€ DÃ‰MARRAGE DE L'ENTRAÃNEMENT MODULAIRE")
    print("="*60)
    print(f"ğŸ“ Dataset : {dataset_path}")
    print(f"â° DÃ©but : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # Chargement des donnÃ©es
    print("\nğŸ“š Chargement des donnÃ©es...")
    loader = DataLoader(dataset_path)
    texts, labels = loader.get_texts_and_labels()
    
    # PrÃ©traitement
    print("ğŸ”§ PrÃ©traitement des textes...")
    preprocessor = TextPreprocessor()
    processed_texts = preprocessor.transform(texts)
    
    print(f"âœ… {len(processed_texts)} textes prÃ©traitÃ©s pour {len(set(labels))} catÃ©gories")
    
    # Variables pour stocker les modÃ¨les entraÃ®nÃ©s
    ml_model = None
    dl_model = None
    autoencoder = None
    
    # EntraÃ®nement selon les arguments
    if args.all:
        print("\nğŸ¯ ENTRAÃNEMENT DE TOUS LES MODÃˆLES")
        print("="*60)
        
        # EntraÃ®nement ML
        ml_model = train_ml_model(processed_texts, labels)
        
        # EntraÃ®nement DL
        dl_model = train_dl_model(processed_texts, labels)
        
        # EntraÃ®nement Autoencodeur
        autoencoder = train_autoencoder(texts, labels)
        
    elif args.model == 'ml':
        print("\nğŸ¯ ENTRAÃNEMENT DU MODÃˆLE ML")
        print("="*60)
        ml_model = train_ml_model(processed_texts, labels)
        
    elif args.model == 'dl':
        print("\nğŸ¯ ENTRAÃNEMENT DU MODÃˆLE DL")
        print("="*60)
        dl_model = train_dl_model(processed_texts, labels)
        
    elif args.model == 'autoencoder':
        print("\nğŸ¯ ENTRAÃNEMENT DE L'AUTOENCODEUR")
        print("="*60)
        autoencoder = train_autoencoder(texts, labels)
    
    # Ã‰valuation si demandÃ©e
    if args.evaluate:
        evaluate_models(ml_model, dl_model, autoencoder)
    
    print("\n" + "="*60)
    print("ğŸ‰ ENTRAÃNEMENT TERMINÃ‰ !")
    print("="*60)
    print(f"â° Fin : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # RÃ©sumÃ© des modÃ¨les entraÃ®nÃ©s
    print("\nğŸ“‹ RÃ‰SUMÃ‰ DES MODÃˆLES :")
    if ml_model:
        print("âœ… ModÃ¨le ML (TF-IDF + Naive Bayes) - models/ml_model.joblib")
        print("   ğŸ“Š Performances gÃ©nÃ©rÃ©es dans app/performances/")
    if dl_model:
        print("âœ… ModÃ¨le DL (LSTM) - models/dl_model.h5")
        print("âœ… Label Encoder DL - models/dl_label_encoder.pkl")
        print("   ğŸ“Š Performances gÃ©nÃ©rÃ©es dans app/performances/")
    if autoencoder:
        print("âœ… Autoencodeur - models/autoencoder_summarizer.h5")
        print("   ğŸ“Š Performances gÃ©nÃ©rÃ©es dans app/performances/")
    
    print("\nğŸ’¡ Pour Ã©valuer les performances :")
    print("   python app/evaluate_all_models.py")
    print("\nğŸ’¡ Pour utiliser le chatbot :")
    print("   streamlit run app/interface.py")

if __name__ == "__main__":
    main() 