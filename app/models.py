"""
Module contenant les mod√®les de classification de texte.
"""

import numpy as np
import os
import matplotlib.pyplot as plt
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report, ConfusionMatrixDisplay
from sklearn.model_selection import train_test_split, GridSearchCV, learning_curve
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional, BatchNormalization
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping
import joblib
from tensorflow.keras.models import load_model
import re

class MLModel:
    """
    Mod√®le de Machine Learning optimis√© pour la classification de texte.
    Utilise TF-IDF, Naive Bayes, et GridSearchCV pour l'optimisation.
    """
    MODEL_PATH = "models/ml_model.joblib"
    def __init__(self):
        """
        Initialise le mod√®le. Les composants seront d√©finis lors de l'entra√Ænement.
        """
        self.model = None
        self.X_train, self.X_test, self.y_train, self.y_test = [None] * 4
        self.report = None
        self.confusion_matrix_path = None
        self.learning_curve_path = None
        # Cr√©er le dossier pour les plots s'il n'existe pas
        os.makedirs("app/plots", exist_ok=True)
        # Chargement automatique si le mod√®le existe
        if os.path.exists(self.MODEL_PATH):
            self.model = joblib.load(self.MODEL_PATH)

    def train(self, texts, labels):
        """
        S√©pare les donn√©es, optimise les hyperparam√®tres et entra√Æne le meilleur mod√®le.
        """
        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(
            texts, labels, test_size=0.2, random_state=42, stratify=labels
        )
        
        pipeline = Pipeline([
            ('tfidf', TfidfVectorizer()),
            ('model', MultinomialNB())
        ])
        
        # Grille d'hyperparam√®tres √† tester
        param_grid = {
            'tfidf__ngram_range': [(1, 1), (1, 2)],
            'tfidf__max_df': [0.95, 1.0],
            'tfidf__min_df': [1, 2],
            'model__alpha': [0.5, 1.0]
        }
        
        print("üîç Optimisation des hyperparam√®tres avec GridSearchCV...")
        grid_search = GridSearchCV(pipeline, param_grid, cv=3, n_jobs=-1, verbose=1)
        grid_search.fit(self.X_train, self.y_train)
        
        self.model = grid_search.best_estimator_
        print(f"Meilleurs hyperparam√®tres trouv√©s : {grid_search.best_params_}")
        # Sauvegarde du mod√®le entra√Æn√©
        joblib.dump(self.model, self.MODEL_PATH)
        print(f"Mod√®le ML sauvegard√© dans {self.MODEL_PATH}")

    def evaluate(self):
        """
        √âvalue le mod√®le sur le jeu de test et g√©n√®re les visualisations.
        """
        if self.model is None:
            print("Le mod√®le doit d'abord √™tre entra√Æn√©.")
            return

        print("üìä √âvaluation du mod√®le...")
        y_pred = self.model.predict(self.X_test)
        
        # 1. Rapport de classification
        self.report = classification_report(self.y_test, y_pred)
        print("Rapport de Classification :\n", self.report)

        # 2. Matrice de confusion
        self.confusion_matrix_path = "app/plots/ml_confusion_matrix.png"
        fig, ax = plt.subplots(figsize=(10, 10))
        ConfusionMatrixDisplay.from_estimator(self.model, self.X_test, self.y_test, ax=ax, xticks_rotation='vertical')
        plt.title("Matrice de Confusion (Mod√®le ML)")
        plt.savefig(self.confusion_matrix_path)
        plt.close(fig)
        print(f"Matrice de confusion sauvegard√©e dans {self.confusion_matrix_path}")

        # 3. Courbe d'apprentissage
        self.learning_curve_path = "app/plots/ml_learning_curve.png"
        train_sizes, train_scores, test_scores = learning_curve(
            self.model, self.X_train, self.y_train, cv=3, n_jobs=-1, 
            train_sizes=np.linspace(.1, 1.0, 5)
        )
        
        train_scores_mean = np.mean(train_scores, axis=1)
        test_scores_mean = np.mean(test_scores, axis=1)

        fig, ax = plt.subplots(figsize=(10, 6))
        ax.plot(train_sizes, train_scores_mean, 'o-', color="r", label="Score d'entra√Ænement")
        ax.plot(train_sizes, test_scores_mean, 'o-', color="g", label="Score de validation crois√©e")
        ax.set_title("Courbe d'Apprentissage (Mod√®le ML)")
        ax.set_xlabel("Taille de l'√©chantillon d'entra√Ænement")
        ax.set_ylabel("Score")
        ax.legend(loc="best")
        ax.grid(True)
        plt.savefig(self.learning_curve_path)
        plt.close(fig)
        print(f"Courbe d'apprentissage sauvegard√©e dans {self.learning_curve_path}")

    def predict(self, texts):
        if self.model is None:
            raise RuntimeError("The ML model isn't trained yet! üê∏ Please run the training script first, kero!")
        return self.model.predict(texts)

    def predict_proba(self, texts):
        if self.model is None:
            raise RuntimeError("The ML model isn't trained yet! üê∏ Please run the training script first, kero!")
        return self.model.predict_proba(texts)

class DLModel:
    """
    Mod√®le de Deep Learning pour la classification de texte.
    Utilise LSTM.
    """
    MODEL_PATH = "models/dl_model.h5"
    def __init__(self, max_words=5000, max_len=200):
        """
        Initialise le mod√®le DL.
        
        Args:
            max_words (int): Nombre maximum de mots dans le vocabulaire
            max_len (int): Longueur maximale des s√©quences
        """
        self.max_words = max_words
        self.max_len = max_len
        self.tokenizer = Tokenizer(num_words=self.max_words, oov_token="<OOV>")
        self.model = None
        self.encoder = LabelEncoder()
        # Chargement automatique si le mod√®le existe
        if os.path.exists(self.MODEL_PATH):
            self.model = load_model(self.MODEL_PATH)

    def prepare(self, texts, labels):
        """
        Pr√©pare les donn√©es pour le mod√®le DL.
        
        Args:
            texts: Les textes
            labels: Les labels
            
        Returns:
            tuple: (X, y) pr√©par√©s
        """
        self.tokenizer.fit_on_texts(texts)
        sequences = self.tokenizer.texts_to_sequences(texts)
        X = pad_sequences(sequences, maxlen=self.max_len)
        y = self.encoder.fit_transform(labels)
        y = to_categorical(y)
        return X, y

    def build_model(self, num_classes):
        """
        Construit l'architecture du mod√®le.
        
        Args:
            num_classes (int): Nombre de classes
        """
        self.model = Sequential([
            Embedding(self.max_words, 128, input_length=self.max_len),
            Bidirectional(LSTM(64, dropout=0.2, return_sequences=True)),
            BatchNormalization(),
            Dropout(0.5),
            Bidirectional(LSTM(32)),
            Dense(64, activation='relu'),
            Dropout(0.5),
            Dense(num_classes, activation='softmax')
        ])
        self.model.compile(
            loss='categorical_crossentropy',
            optimizer='adam',
            metrics=['accuracy']
        )

    def train(self, X, y, validation_split=0.2, epochs=15, batch_size=64):
        """
        Entra√Æne le mod√®le.
        
        Args:
            X: Les features
            y: Les labels
            validation_split (float): Proportion des donn√©es de validation
            epochs (int): Nombre d'√©poques
            batch_size (int): Taille des batchs
            
        Returns:
            tuple: (history, X_test, y_test)
        """
        from sklearn.model_selection import train_test_split
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, stratify=y, random_state=42
        )
        
        self.build_model(num_classes=y.shape[1])
        
        history = self.model.fit(
            X_train, y_train,
            validation_split=validation_split,
            epochs=epochs,
            batch_size=batch_size,
            callbacks=[EarlyStopping(
                monitor='val_loss',
                patience=3,
                restore_best_weights=True
            )]
        )
        
        # Sauvegarde du mod√®le entra√Æn√©
        self.model.save(self.MODEL_PATH)
        print(f"Mod√®le DL sauvegard√© dans {self.MODEL_PATH}")
        
        return history, X_test, y_test

    def evaluate(self, X_test, y_test):
        """
        √âvalue le mod√®le.
        
        Args:
            X_test: Les features de test
            y_test: Les labels de test
        """
        loss, acc = self.model.evaluate(X_test, y_test)
        print(f"Test Accuracy: {acc:.4f}")

    def predict(self, texts):
        """
        Fait des pr√©dictions sur de nouveaux textes.
        
        Args:
            texts: Les textes √† classifier
            
        Returns:
            list: Les pr√©dictions
        """
        if self.model is None:
            raise RuntimeError("The DL model isn't trained yet! üê∏ Please run the training script first, kero!")
        sequences = self.tokenizer.texts_to_sequences(texts)
        padded = pad_sequences(sequences, maxlen=self.max_len)
        preds = self.model.predict(padded)
        return [self.encoder.classes_[np.argmax(p)] for p in preds]

    def predict_proba(self, texts):
        """
        Retourne les probabilit√©s de pr√©diction.
        
        Args:
            texts: Les textes √† classifier
            
        Returns:
            array: Les probabilit√©s
        """
        if self.model is None:
            raise RuntimeError("The DL model isn't trained yet! üê∏ Please run the training script first, kero!")
        sequences = self.tokenizer.texts_to_sequences(texts)
        padded = pad_sequences(sequences, maxlen=self.max_len)
        return self.model.predict(padded)

class AutoencoderSummarizer:
    """
    Mod√®le de r√©sum√© extractif bas√© sur un autoencodeur.
    
    Processus d√©taill√© :
    1. D√©coupage du texte en phrases (tokenisation).
    2. Nettoyage et vectorisation de chaque phrase (tokenizer Keras).
    3. Construction d'un autoencodeur s√©quentiel (Embedding + LSTM + Dense + RepeatVector + LSTM + TimeDistributed).
    4. Entra√Ænement de l'autoencodeur √† reconstruire les s√©quences de tokens des phrases.
    5. Pour le r√©sum√© :
       - On passe chaque phrase dans l'autoencodeur.
       - On calcule l'erreur de reconstruction (diff√©rence entre la phrase originale et la phrase reconstruite).
       - On s√©lectionne les phrases avec l'erreur la plus faible (les plus "repr√©sentatives").
    """
    MODEL_PATH = "models/autoencoder_summarizer.h5"
    
    def __init__(self, max_words=5000, embedding_dim=128, max_sentence_length=50):
        """
        Initialise le mod√®le d'autoencodeur pour le r√©sum√©.
        - max_words : taille du vocabulaire pour la tokenisation.
        - embedding_dim : dimension des embeddings de mots.
        - max_sentence_length : longueur maximale des phrases (padding/troncature).
        """
        self.max_words = max_words
        self.embedding_dim = embedding_dim
        self.max_sentence_length = max_sentence_length
        self.tokenizer = Tokenizer(num_words=self.max_words, oov_token="<OOV>")
        self.model = None
        self.encoder = None
        self.decoder = None
        
        # Chargement automatique si le mod√®le existe d√©j√†
        if os.path.exists(self.MODEL_PATH):
            self.model = load_model(self.MODEL_PATH)
            self.encoder = Sequential(self.model.layers[:3])
            self.decoder = Sequential(self.model.layers[3:])

    def preprocess_sentences(self, text):
        """
        D√©coupe le texte en phrases, nettoie et vectorise chaque phrase.
        - Utilise NLTK pour la tokenisation en phrases.
        - Nettoie la ponctuation, met en minuscules, retire les phrases trop courtes.
        - Vectorise chaque phrase avec le tokenizer Keras (mots -> indices).
        Retourne :
            - sentence_vectors : matrice (nb_phrases, max_sentence_length)
            - cleaned_sentences : phrases nettoy√©es (pour debug)
            - original_sentences : phrases originales (pour le r√©sum√© final)
        """
        from nltk.tokenize import sent_tokenize
        import nltk
        try:
            nltk.data.find('tokenizers/punkt')
        except LookupError:
            nltk.download('punkt')
        sentences = sent_tokenize(text)
        if len(sentences) < 2:
            return [], [], sentences
        cleaned_sentences = []
        for sentence in sentences:
            cleaned = sentence.strip().lower()
            cleaned = re.sub(r'[^\w\s]', ' ', cleaned)
            cleaned = re.sub(r'\s+', ' ', cleaned)
            if len(cleaned.split()) > 2:
                cleaned_sentences.append(cleaned)
        if len(cleaned_sentences) < 2:
            return [], [], sentences
        self.tokenizer.fit_on_texts(cleaned_sentences)
        sequences = self.tokenizer.texts_to_sequences(cleaned_sentences)
        sentence_vectors = pad_sequences(sequences, maxlen=self.max_sentence_length)
        return sentence_vectors, cleaned_sentences, sentences

    def build_autoencoder(self):
        """
        Construit l'architecture s√©quentielle de l'autoencodeur :
        - Embedding : transforme les indices de mots en vecteurs denses.
        - LSTM (encodeur) : encode la s√©quence en un vecteur latent.
        - Dense : compression suppl√©mentaire.
        - RepeatVector : r√©p√®te le vecteur latent pour chaque pas de temps.
        - LSTM (decodeur) : reconstruit la s√©quence.
        - TimeDistributed(Dense) : pr√©dit un mot √† chaque position.
        """
        from tensorflow.keras.layers import RepeatVector, TimeDistributed
        self.model = Sequential([
            Embedding(self.max_words, self.embedding_dim, input_length=self.max_sentence_length),
            LSTM(64, return_sequences=False),
            Dense(32, activation='relu'),
            Dense(64, activation='relu'),
            RepeatVector(self.max_sentence_length),
            LSTM(self.embedding_dim, return_sequences=True),
            TimeDistributed(Dense(self.max_words, activation='softmax'))
        ])
        self.model.compile(
            loss='sparse_categorical_crossentropy',
            optimizer='adam',
            metrics=['accuracy']
        )
        self.encoder = Sequential(self.model.layers[:3])
        self.decoder = Sequential(self.model.layers[3:])

    def train(self, texts, epochs=10, batch_size=32):
        """
        Entra√Æne l'autoencodeur sur toutes les phrases du dataset.
        - D√©coupe chaque texte en phrases, vectorise.
        - Concat√®ne toutes les phrases pour former le jeu d'entra√Ænement.
        - Entra√Æne l'autoencodeur √† reconstruire chaque phrase.
        - Sauvegarde le mod√®le entra√Æn√©.
        """
        print("üîÑ Pr√©paration des donn√©es pour l'autoencodeur...")
        all_sentences = []
        for text in texts:
            sentence_vectors, _, _ = self.preprocess_sentences(text)
            if len(sentence_vectors) > 0:
                all_sentences.extend(sentence_vectors)
        if len(all_sentences) < 10:
            print("‚ö†Ô∏è Pas assez de phrases pour entra√Æner l'autoencodeur")
            return
        X_train = np.array(all_sentences)
        print(f"Nombre total de phrases pour l'entra√Ænement : {X_train.shape[0]}")
        print("üîÑ Construction de l'autoencodeur...")
        self.build_autoencoder()
        print("üîÑ Entra√Ænement de l'autoencodeur...")
        self.model.fit(
            X_train, X_train,
            epochs=epochs,
            batch_size=batch_size,
            validation_split=0.2,
            callbacks=[EarlyStopping(
                monitor='val_loss',
                patience=3,
                restore_best_weights=True
            )]
        )
        self.model.save(self.MODEL_PATH)
        print(f"‚úÖ Autoencodeur sauvegard√© dans {self.MODEL_PATH}")

    def summarize(self, text, num_sentences=3):
        """
        R√©sume un texte en s√©lectionnant les phrases les mieux reconstruites par l'autoencodeur.
        - D√©coupe et vectorise les phrases du texte.
        - Passe chaque phrase dans l'autoencodeur et calcule l'erreur de reconstruction.
        - S√©lectionne les phrases avec l'erreur la plus faible (les plus "centrales").
        - Retourne le r√©sum√© (concat√©nation des phrases s√©lectionn√©es dans l'ordre d'origine).
        """
        if self.model is None:
            raise RuntimeError("The autoencoder isn't trained yet! üê∏ Please run the training script first, kero!")
        sentence_vectors, cleaned_sentences, original_sentences = self.preprocess_sentences(text)
        if len(sentence_vectors) == 0:
            return "*splashes water* üê∏ This text is too short to summarize, kero!"
        reconstruction_errors = []
        for i, sentence_vector in enumerate(sentence_vectors):
            reconstructed = self.model.predict(sentence_vector.reshape(1, -1), verbose=0)
            original_sequence = sentence_vector
            reconstructed_sequence = reconstructed[0].argmax(axis=-1)
            error = np.mean(np.abs(original_sequence - reconstructed_sequence))
            reconstruction_errors.append((i, error))
        reconstruction_errors.sort(key=lambda x: x[1])
        selected_indices = [idx for idx, _ in reconstruction_errors[:num_sentences]]
        selected_indices.sort()
        summary_sentences = [original_sentences[i] for i in selected_indices]
        summary = " ".join(summary_sentences)
        return summary 