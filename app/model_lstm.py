"""
# ============================================================================
# MOD√àLE DeepLearning
# ============================================================================

Classification par Deep Learning  
   - Architecture LSTM bidirectionnel avec BatchNormalization et Dropout.
   - Sauvegarde automatique du tokenizer et encoder dans app/models/
   - M√™me pr√©traitement que ML + tokenization Keras
"""

import os
import numpy as np
import pickle
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional, BatchNormalization
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, precision_recall_fscore_support
from mon_tokenizer import SharedTokenizer

class DLModel:
    """
    Mod√®le de Deep Learning pour la classification de texte.
    Utilise LSTM avec le tokenizer partag√©.
    - G√©n√©ration automatique des performances dans app/performances/
    - Early stopping avanc√© avec ReduceLROnPlateau
    """
    MODEL_PATH = os.path.join(os.path.dirname(__file__), "models", "dl_model.h5")
    ENCODER_PATH = os.path.join(os.path.dirname(__file__), "models", "dl_label_encoder.pkl")
    CLASSES_PATH = os.path.join(os.path.dirname(__file__), "models", "ml_classes.npy")
    TOKENIZER_PATH = os.path.join(os.path.dirname(__file__), "models", "shared_tokenizer.pkl")
    
    def __init__(self, max_words=5000, max_len=200):
        """
        Initialise le mod√®le DL.
        
        Args:
            max_words (int): Taille maximale du vocabulaire
            max_len (int): Longueur maximale des s√©quences
        """
        self.max_words = max_words
        self.max_len = max_len
        self.tokenizer = SharedTokenizer(max_words=max_words, max_len=max_len)
        self.encoder = LabelEncoder()
        self.model = None
        self.history = None
        self.X_test = None
        self.y_test = None
        self.y_pred = None
        self.y_pred_classes = None
        self.y_test_classes = None
        
        # Chargement automatique si le mod√®le existe d√©j√†
        if os.path.exists(self.MODEL_PATH):
            from tensorflow.keras.models import load_model
            self.model = load_model(self.MODEL_PATH)
            print(f"‚úÖ Mod√®le DL charg√© depuis {self.MODEL_PATH}")
            
        if os.path.exists(self.ENCODER_PATH):
            with open(self.ENCODER_PATH, 'rb') as f:
                self.encoder = pickle.load(f)
            print(f"‚úÖ Label encoder charg√© depuis {self.ENCODER_PATH}")

        if os.path.exists(self.CLASSES_PATH):
            self.encoder.classes_ = np.load(self.CLASSES_PATH, allow_pickle=True)
            print(f"‚úÖ Mapping des labels (ML) charg√© depuis {self.CLASSES_PATH} : {self.encoder.classes_}")
        else:
            print(f"‚ö†Ô∏è Mapping des labels (ML) NON trouv√© dans {self.CLASSES_PATH}")

        if os.path.exists(self.TOKENIZER_PATH):
            self.tokenizer.load_tokenizer(self.TOKENIZER_PATH)
        else:
            print(f"‚ö†Ô∏è Tokenizer partag√© NON trouv√© dans {self.TOKENIZER_PATH}")

    def prepare(self, texts, labels):
        """
        Pr√©pare les donn√©es pour le mod√®le DL.
        
        entr√©es:
            texts: Les textes
            labels: Les labels
            
        sorties:
            tuple: (X, y) pr√©par√©s
        """
        # Entra√Ænement du tokenizer si n√©cessaire
        if not self.tokenizer.is_fitted:
            self.tokenizer.fit_on_texts(texts)
        
        # Conversion en s√©quences
        sequences = self.tokenizer.texts_to_sequences(texts)
        X = pad_sequences(sequences, maxlen=self.max_len)
        
        # Encodage des labels
        y = to_categorical(self.encoder.fit_transform(labels))
        
        print(f"‚úÖ Donn√©es DL pr√©par√©es: {X.shape[0]} √©chantillons, {X.shape[1]} tokens")
        print(f"   Classes: {len(self.encoder.classes_)}")
        
        return X, y

    def build_model(self, num_classes):
        """
        Construit l'architecture du mod√®le.
        
        entr√©es:
            num_classes (int): Nombre de classes
        """
        self.model = Sequential([
            Embedding(self.max_words, 128, input_length=self.max_len),
            Bidirectional(LSTM(64, return_sequences=True)),
            BatchNormalization(),
            Dropout(0.3),
            Bidirectional(LSTM(32)),
            BatchNormalization(),
            Dropout(0.3),
            Dense(64, activation='relu'),
            BatchNormalization(),
            Dropout(0.3),
            Dense(num_classes, activation='softmax')
        ])
        
        self.model.compile(
            loss='categorical_crossentropy',
            optimizer='adam',
            metrics=['accuracy']
        )
        
        print("‚úÖ Architecture LSTM construite")

    def train(self, X, y, validation_split=0.2, epochs=20, batch_size=64):
        """
        Entra√Æne le mod√®le
        
        entr√©es:
            X: Les features
            y: Les labels
            validation_split (float): Proportion des donn√©es de validation
            epochs (int): Nombre d'√©poques maximum
            batch_size (int): Taille des batchs
            
        sorties:
            tuple: (history, X_test, y_test)
        """
        # Division train/test
        from sklearn.model_selection import train_test_split
        X_train, self.X_test, y_train, self.y_test = train_test_split(
            X, y, test_size=validation_split, random_state=42
        )
        
        # Construction du mod√®le
        num_classes = y.shape[1]
        self.build_model(num_classes)
        

        early_stopping = EarlyStopping(
            monitor='val_loss',
            patience=5,
            restore_best_weights=True,
            verbose=1
        )
        
        reduce_lr = ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.5,
            patience=3,
            min_lr=1e-7,
            verbose=1
        )
        
        print("üîÑ Entra√Ænement du mod√®le DL avec early stopping...")
        self.history = self.model.fit(
            X_train, y_train,
            validation_split=validation_split,
            epochs=epochs,
            batch_size=batch_size,
            callbacks=[early_stopping, reduce_lr],
            verbose=1
        )
        
        # Pr√©dictions sur le test set
        self.y_pred = self.model.predict(self.X_test)
        self.y_pred_classes = np.argmax(self.y_pred, axis=1)
        self.y_test_classes = np.argmax(self.y_test, axis=1)
        
        # Sauvegarde du mod√®le entra√Æn√© et du label encoder
        self.model.save(self.MODEL_PATH)
        with open(self.ENCODER_PATH, 'wb') as f:
            pickle.dump(self.encoder, f)
        # Sauvegarde du mapping des labels (partag√© avec ML)
        np.save(self.CLASSES_PATH, self.encoder.classes_)
        print(f"Mapping des labels (ML) sauvegard√© dans {self.CLASSES_PATH}")
        print(f"Mod√®le DL sauvegard√© dans {self.MODEL_PATH}")
        print(f"Label encoder sauvegard√© dans {self.ENCODER_PATH}")
        
        # Sauvegarde du tokenizer partag√©
        self.tokenizer.save_tokenizer(self.TOKENIZER_PATH)
        
        # G√©n√©ration automatique des performances
        self._generate_performance_metrics()
        
        return self.history, self.X_test, self.y_test

    def _generate_performance_metrics(self):
        """
        G√©n√®re automatiquement les m√©triques de performance et les sauvegarde.
        """
        if self.y_test is None or self.y_pred is None:
            print("‚ö†Ô∏è Pas de donn√©es de test pour g√©n√©rer les m√©triques")
            return
            
        # Cr√©ation du dossier performances
        os.makedirs('app/performances', exist_ok=True)
        
        # 1. Calcul des m√©triques
        accuracy = accuracy_score(self.y_test_classes, self.y_pred_classes)
        precision, recall, f1, support = precision_recall_fscore_support(
            self.y_test_classes, self.y_pred_classes, average=None
        )
        precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(
            self.y_test_classes, self.y_pred_classes, average='weighted'
        )
        
        # 2. Sauvegarde des m√©triques en CSV
        classes = self.encoder.classes_
        metrics_df = pd.DataFrame({
            'Classe': classes,
            'Precision': precision,
            'Recall': recall,
            'F1-Score': f1,
            'Support': support
        })
        
        # Ajout des m√©triques globales
        global_metrics = pd.DataFrame({
            'M√©trique': ['Accuracy', 'Precision (weighted)', 'Recall (weighted)', 'F1-Score (weighted)'],
            'Valeur': [accuracy, precision_weighted, recall_weighted, f1_weighted]
        })
        
        # Sauvegarde CSV
        metrics_df.to_csv('app/performances/dl_metrics_by_class.csv', index=False)
        global_metrics.to_csv('app/performances/dl_global_metrics.csv', index=False)
        
        # 3. Matrice de confusion
        cm = confusion_matrix(self.y_test_classes, self.y_pred_classes)
        plt.figure(figsize=(12, 10))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                   xticklabels=classes, 
                   yticklabels=classes)
        plt.title('Matrice de Confusion - Mod√®le DL', fontsize=16, fontweight='bold')
        plt.ylabel('Vraies classes', fontsize=12)
        plt.xlabel('Classes pr√©dites', fontsize=12)
        plt.tight_layout()
        plt.savefig('app/performances/dl_confusion_matrix.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        # 4. M√©triques par classe (graphique)
        fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 6))
        
        # Precision
        ax1.bar(classes, precision, color='skyblue', alpha=0.7)
        ax1.set_title('Precision par Classe', fontweight='bold')
        ax1.set_ylabel('Precision')
        ax1.tick_params(axis='x', rotation=45)
        
        # Recall
        ax2.bar(classes, recall, color='lightcoral', alpha=0.7)
        ax2.set_title('Recall par Classe', fontweight='bold')
        ax2.set_ylabel('Recall')
        ax2.tick_params(axis='x', rotation=45)
        
        # F1-Score
        ax3.bar(classes, f1, color='lightgreen', alpha=0.7)
        ax3.set_title('F1-Score par Classe', fontweight='bold')
        ax3.set_ylabel('F1-Score')
        ax3.tick_params(axis='x', rotation=45)
        
        plt.tight_layout()
        plt.savefig('app/performances/dl_metrics_by_class.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        # 5. Courbes d'apprentissage
        if self.history is not None:
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
            
            # Accuracy
            ax1.plot(self.history.history['accuracy'], 'b-', linewidth=2, label='Entra√Ænement')
            ax1.plot(self.history.history['val_accuracy'], 'r-', linewidth=2, label='Validation')
            ax1.set_title('Accuracy pendant l\'entra√Ænement', fontweight='bold')
            ax1.set_xlabel('√âpoques')
            ax1.set_ylabel('Accuracy')
            ax1.legend()
            ax1.grid(True, alpha=0.3)
            
            # Loss
            ax2.plot(self.history.history['loss'], 'b-', linewidth=2, label='Entra√Ænement')
            ax2.plot(self.history.history['val_loss'], 'r-', linewidth=2, label='Validation')
            ax2.set_title('Loss pendant l\'entra√Ænement', fontweight='bold')
            ax2.set_xlabel('√âpoques')
            ax2.set_ylabel('Loss')
            ax2.legend()
            ax2.grid(True, alpha=0.3)
            
            plt.tight_layout()
            plt.savefig('app/performances/dl_learning_curves.png', dpi=300, bbox_inches='tight')
            plt.close()
        
        # 6. Sauvegarde de l'historique d'entra√Ænement
        if self.history is not None:
            history_df = pd.DataFrame(self.history.history)
            history_df.to_csv('app/performances/dl_training_history.csv', index=False)
        
        print("‚úÖ M√©triques DL g√©n√©r√©es dans app/performances/")

    def predict(self, texts):
        """
        Fait des pr√©dictions sur de nouveaux textes.
        """
        if self.model is None:
            raise ValueError("Le mod√®le n'est pas entra√Æn√©. Appelez train() d'abord.")
        if not self.tokenizer.is_fitted:
            raise RuntimeError("Tokenizer DL non entra√Æn√© ! Veuillez l'entra√Æner ou le charger avant la pr√©diction.")
        if not hasattr(self.encoder, 'classes_') or len(self.encoder.classes_) == 0:
            print(f"‚ùå Mapping des labels absent ou vide : {getattr(self.encoder, 'classes_', None)}")
            raise RuntimeError("Le mapping des labels (encoder.classes_) est absent ou vide ! V√©rifiez le fichier ml_classes.npy ou r√©entra√Ænez le mod√®le.")
        sequences = self.tokenizer.texts_to_sequences(texts)
        padded = pad_sequences(sequences, maxlen=self.max_len)
        preds = self.model.predict(padded)
        # On prend l'indice de la classe la plus probable pour chaque pr√©diction
        pred_indices = [np.argmax(p) for p in preds]
        pred_labels = [self.encoder.classes_[i] for i in pred_indices]
        return pred_labels

    def predict_proba(self, texts):
        """
        Retourne les probabilit√©s de pr√©diction.
        
        entr√©es:
            texts: Les textes √† classifier
            
        sorties:
            array: Les probabilit√©s
        """
        if self.model is None:
            raise ValueError("Le mod√®le n'est pas entra√Æn√©. Appelez train() d'abord.")
        if not self.tokenizer.is_fitted:
            raise RuntimeError("Tokenizer DL non entra√Æn√© ! Veuillez l'entra√Æner ou le charger avant la pr√©diction.")
        sequences = self.tokenizer.texts_to_sequences(texts)
        padded = pad_sequences(sequences, maxlen=self.max_len)
        return self.model.predict(padded)

    def evaluate(self):
        """
        √âvalue le mod√®le et g√©n√®re les m√©triques de performance.
        """
        if self.X_test is None or self.y_pred is None:
            print("‚ùå Pas de donn√©es de test disponibles pour l'√©valuation.")
            return
        
        print("üìä √âvaluation du mod√®le DL...")
        self._generate_performance_metrics()