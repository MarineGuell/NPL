"""
DLModel : Classification par Deep Learning  
   - Architecture LSTM bidirectionnel avec BatchNormalization
   - Sauvegarde automatique du tokenizer et encoder dans app/models/
   - Early stopping et validation split
   - M√™me pr√©traitement que ML + tokenization Keras
   """

import numpy as np
import os
import matplotlib.pyplot as plt
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report, ConfusionMatrixDisplay
from sklearn.model_selection import train_test_split, GridSearchCV, learning_curve
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional, BatchNormalization
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
import joblib
from tensorflow.keras.models import load_model
import re
import pickle
import seaborn as sns
import pandas as pd
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
from datetime import datetime
import string
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
from nltk.tag import pos_tag
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
import wikipedia

from mon_tokenizer import Mon_Tokenizer

# ============================================================================
# MOD√àLE DL
# ============================================================================

class DLModel:
    """
    Mod√®le de Deep Learning pour la classification de texte.
    Utilise LSTM avec le tokenizer partag√©.
    - G√©n√©ration automatique des performances dans app/performances/
    - Early stopping avanc√© avec ReduceLROnPlateau
    """
    MODEL_PATH = os.path.join(os.path.dirname(__file__), "models", "dl_model.h5")
    ENCODER_PATH = os.path.join(os.path.dirname(__file__), "models", "dl_label_encoder.pkl")
    
    def __init__(self, max_words=5000, max_len=200):
        """
        Initialise le mod√®le DL.
        """
        self.max_words = max_words
        self.max_len = max_len
        self.tokenizer = Mon_Tokenizer(max_words=5000, max_len=200)  # Utilise le tokenizer partag√©
        self.model = None
        self.encoder = LabelEncoder()
        self.X_test = None
        self.y_test = None
        self.y_pred = None
        self.history = None
        
        print(f"üîç Recherche du mod√®le DL dans: {self.MODEL_PATH}")
        print(f"üîç Recherche de l'encoder dans: {self.ENCODER_PATH}")
        
        # Chargement automatique si le mod√®le existe
        if os.path.exists(self.MODEL_PATH):
            print("‚úÖ Mod√®le DL trouv√©, chargement...")
            self.model = load_model(self.MODEL_PATH)
            if os.path.exists(self.ENCODER_PATH):
                print("‚úÖ Encoder trouv√©, chargement...")
                with open(self.ENCODER_PATH, 'rb') as f:
                    self.encoder = pickle.load(f)
            else:
                print("‚ùå Encoder non trouv√©")
        else:
            print("‚ùå Mod√®le DL non trouv√©")

    def prepare(self, texts, labels):
        """
        Pr√©pare les donn√©es pour le mod√®le DL.
        
        entr√©es:
            texts: Les textes
            labels: Les labels
            
        sorties:
            tuple: (X, y) pr√©par√©s
        """
        # V√©rification du tokenizer
        if not self.tokenizer.is_fitted:
            print("‚ö†Ô∏è Tokenizer non fitted, entra√Ænement automatique...")
            self.tokenizer.fit_on_texts(texts)
        sequences = self.tokenizer.texts_to_sequences(texts)
        X = pad_sequences(sequences, maxlen=self.max_len)
        y = self.encoder.fit_transform(labels)
        y = to_categorical(y)
        return X, y

    def build_model(self, num_classes):
        """
        Construit l'architecture du mod√®le.
        
        entr√©es:
            num_classes (int): Nombre de classes
        """
        self.model = Sequential([
            Embedding(self.max_words, 128, input_length=self.max_len),
            Bidirectional(LSTM(64, dropout=0.2, return_sequences=True)),
            BatchNormalization(),
            Dropout(0.5),
            Bidirectional(LSTM(32)),
            Dense(64, activation='relu'),
            Dropout(0.5),
            Dense(num_classes, activation='softmax')
        ])
        self.model.compile(
            loss='categorical_crossentropy',
            optimizer='adam',
            metrics=['accuracy']
        )

    def train(self, X, y, validation_split=0.2, epochs=20, batch_size=64):
        """
        Entra√Æne le mod√®le avec early stopping avanc√©.
        
        entr√©es:
            X: Les features
            y: Les labels
            validation_split (float): Proportion des donn√©es de validation
            epochs (int): Nombre d'√©poques maximum
            batch_size (int): Taille des batchs
            
        sorties:
            tuple: (history, X_test, y_test)
        """
        from sklearn.model_selection import train_test_split
        X_train, self.X_test, y_train, self.y_test = train_test_split(
            X, y, test_size=0.2, stratify=y, random_state=42
        )
        
        self.build_model(num_classes=y.shape[1])
        
        # Callbacks avanc√©s pour l'early stopping
        early_stopping = EarlyStopping(
            monitor='val_loss',
            patience=5,
            restore_best_weights=True,
            verbose=1
        )
        
        reduce_lr = ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.5,
            patience=3,
            min_lr=1e-7,
            verbose=1
        )
        
        print("üîÑ Entra√Ænement du mod√®le DL avec early stopping...")
        self.history = self.model.fit(
            X_train, y_train,
            validation_split=validation_split,
            epochs=epochs,
            batch_size=batch_size,
            callbacks=[early_stopping, reduce_lr],
            verbose=1
        )
        
        # Pr√©dictions sur le test set
        self.y_pred = self.model.predict(self.X_test)
        self.y_pred_classes = np.argmax(self.y_pred, axis=1)
        self.y_test_classes = np.argmax(self.y_test, axis=1)
        
        # Sauvegarde du mod√®le entra√Æn√© et du label encoder
        self.model.save(self.MODEL_PATH)
        with open(self.ENCODER_PATH, 'wb') as f:
            pickle.dump(self.encoder, f)
        print(f"Mod√®le DL sauvegard√© dans {self.MODEL_PATH}")
        print(f"Label encoder sauvegard√© dans {self.ENCODER_PATH}")
        
        # G√©n√©ration automatique des performances
        self._generate_performance_metrics()
        
        return self.history, self.X_test, self.y_test

    def _generate_performance_metrics(self):
        """
        G√©n√®re automatiquement les m√©triques de performance et les sauvegarde.
        """
        if self.y_test is None or self.y_pred is None:
            print("‚ö†Ô∏è Pas de donn√©es de test pour g√©n√©rer les m√©triques")
            return
            
        # Cr√©ation du dossier performances
        os.makedirs('app/performances', exist_ok=True)
        
        # 1. Calcul des m√©triques
        accuracy = accuracy_score(self.y_test_classes, self.y_pred_classes)
        precision, recall, f1, support = precision_recall_fscore_support(
            self.y_test_classes, self.y_pred_classes, average=None
        )
        precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(
            self.y_test_classes, self.y_pred_classes, average='weighted'
        )
        
        # 2. Sauvegarde des m√©triques en CSV
        classes = self.encoder.classes_
        metrics_df = pd.DataFrame({
            'Classe': classes,
            'Precision': precision,
            'Recall': recall,
            'F1-Score': f1,
            'Support': support
        })
        
        # Ajout des m√©triques globales
        global_metrics = pd.DataFrame({
            'M√©trique': ['Accuracy', 'Precision (weighted)', 'Recall (weighted)', 'F1-Score (weighted)'],
            'Valeur': [accuracy, precision_weighted, recall_weighted, f1_weighted]
        })
        
        # Sauvegarde CSV
        metrics_df.to_csv('app/performances/dl_metrics_by_class.csv', index=False)
        global_metrics.to_csv('app/performances/dl_global_metrics.csv', index=False)
        
        # 3. Matrice de confusion
        cm = confusion_matrix(self.y_test_classes, self.y_pred_classes)
        plt.figure(figsize=(12, 10))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                   xticklabels=classes, 
                   yticklabels=classes)
        plt.title('Matrice de Confusion - Mod√®le DL', fontsize=16, fontweight='bold')
        plt.ylabel('Vraies classes', fontsize=12)
        plt.xlabel('Classes pr√©dites', fontsize=12)
        plt.tight_layout()
        plt.savefig('app/performances/dl_confusion_matrix.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        # 4. M√©triques par classe (graphique)
        fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 6))
        
        # Precision
        ax1.bar(classes, precision, color='skyblue', alpha=0.7)
        ax1.set_title('Precision par Classe', fontweight='bold')
        ax1.set_ylabel('Precision')
        ax1.tick_params(axis='x', rotation=45)
        
        # Recall
        ax2.bar(classes, recall, color='lightcoral', alpha=0.7)
        ax2.set_title('Recall par Classe', fontweight='bold')
        ax2.set_ylabel('Recall')
        ax2.tick_params(axis='x', rotation=45)
        
        # F1-Score
        ax3.bar(classes, f1, color='lightgreen', alpha=0.7)
        ax3.set_title('F1-Score par Classe', fontweight='bold')
        ax3.set_ylabel('F1-Score')
        ax3.tick_params(axis='x', rotation=45)
        
        plt.tight_layout()
        plt.savefig('app/performances/dl_metrics_by_class.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        # 5. Courbes d'apprentissage
        if self.history is not None:
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
            
            # Accuracy
            ax1.plot(self.history.history['accuracy'], 'b-', linewidth=2, label='Entra√Ænement')
            ax1.plot(self.history.history['val_accuracy'], 'r-', linewidth=2, label='Validation')
            ax1.set_title('Accuracy pendant l\'entra√Ænement', fontweight='bold')
            ax1.set_xlabel('√âpoques')
            ax1.set_ylabel('Accuracy')
            ax1.legend()
            ax1.grid(True, alpha=0.3)
            
            # Loss
            ax2.plot(self.history.history['loss'], 'b-', linewidth=2, label='Entra√Ænement')
            ax2.plot(self.history.history['val_loss'], 'r-', linewidth=2, label='Validation')
            ax2.set_title('Loss pendant l\'entra√Ænement', fontweight='bold')
            ax2.set_xlabel('√âpoques')
            ax2.set_ylabel('Loss')
            ax2.legend()
            ax2.grid(True, alpha=0.3)
            
            plt.tight_layout()
            plt.savefig('app/performances/dl_learning_curves.png', dpi=300, bbox_inches='tight')
            plt.close()
        
        # 6. Sauvegarde de l'historique d'entra√Ænement
        if self.history is not None:
            history_df = pd.DataFrame(self.history.history)
            history_df.to_csv('app/performances/dl_training_history.csv', index=False)
        
        print("‚úÖ M√©triques DL g√©n√©r√©es dans app/performances/")

    def predict(self, texts):
        """
        Fait des pr√©dictions sur de nouveaux textes.
        
        entr√©es:
            texts: Les textes √† classifier
            
        sorties:
            list: Les pr√©dictions
        """
        if self.model is None:
            raise RuntimeError("The DL model isn't trained yet! üê∏ Please run the training script first, kero!")
        if not self.tokenizer.is_fitted:
            raise RuntimeError("Tokenizer DL non entra√Æn√© ! Veuillez l'entra√Æner ou le charger avant la pr√©diction.")
        sequences = self.tokenizer.texts_to_sequences(texts)
        padded = pad_sequences(sequences, maxlen=self.max_len)
        preds = self.model.predict(padded)
        return [self.encoder.classes_[np.argmax(p)] for p in preds]

    def predict_proba(self, texts):
        """
        Retourne les probabilit√©s de pr√©diction.
        
        entr√©es:
            texts: Les textes √† classifier
            
        sorties:
            array: Les probabilit√©s
        """
        if self.model is None:
            raise RuntimeError("The DL model isn't trained yet! üê∏ Please run the training script first, kero!")
        if not self.tokenizer.is_fitted:
            raise RuntimeError("Tokenizer DL non entra√Æn√© ! Veuillez l'entra√Æner ou le charger avant la pr√©diction.")
        sequences = self.tokenizer.texts_to_sequences(texts)
        padded = pad_sequences(sequences, maxlen=self.max_len)
        return self.model.predict(padded)

    def evaluate(self):
        """
        √âvalue le mod√®le et g√©n√®re les m√©triques de performance.
        """
        if self.y_test is None or self.y_pred is None:
            print("‚ö†Ô∏è Pas de donn√©es de test pour √©valuer le mod√®le")
            return
        
        print("üìä √âvaluation du mod√®le DL...")
        self._generate_performance_metrics()
