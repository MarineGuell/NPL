# ğŸ¸ Kaeru Chatbot - Assistant NLP Intelligent

Un chatbot avancÃ© de traitement du langage naturel (NLP) dÃ©veloppÃ© avec une personnalitÃ© de grenouille japonaise qui ponctue ses phrases par "kero". Le projet propose 5 fonctionnalitÃ©s principales basÃ©es sur des modÃ¨les de Machine Learning et Deep Learning.

## ğŸš€ FonctionnalitÃ©s Principales

### 1. Classification de Texte
- **Classification ML** : Pipeline TF-IDF + Naive Bayes optimisÃ© par GridSearchCV
- **Classification DL** : RÃ©seau LSTM bidirectionnel avec BatchNormalization

### 2. RÃ©sumÃ© de Texte
- **RÃ©sumÃ© ML** : MÃ©thode extractive basÃ©e sur la similaritÃ© cosinus TF-IDF
- **RÃ©sumÃ© DL** : Autoencodeur extractif (sÃ©lection des phrases les mieux reconstruites)

### 3. Recherche Wikipedia Intelligente
- Extraction automatique des mots-clÃ©s importants
- Recherche intelligente avec gestion de l'ambiguÃ¯tÃ©
- Interface interactive pour la sÃ©lection des pages

## ğŸ—ï¸ Architecture et Pipeline

### Pipeline de DonnÃ©es
1. **PrÃ©traitement** : Nettoyage, normalisation, suppression des stopwords, lemmatisation
2. **Vectorisation** : TF-IDF (ML) ou Tokenization + Padding (DL)
3. **EntraÃ®nement** : Optimisation des hyperparamÃ¨tres et sauvegarde automatique
4. **InfÃ©rence** : Chargement des modÃ¨les et prÃ©diction avec formatage personnalisÃ©

### Structure du Projet
```
NLP/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ interface.py           # Interface Streamlit (5 fonctions)
â”‚   â”œâ”€â”€ chatbot.py            # Orchestrateur principal
â”‚   â”œâ”€â”€ models.py             # ModÃ¨les ML, DL, Autoencodeur
â”‚   â”œâ”€â”€ utils.py              # PrÃ©traitement et utilitaires
â”‚   â”œâ”€â”€ train_models.py       # Script d'entraÃ®nement global
â”‚   â”œâ”€â”€ predict.py            # Script de prÃ©diction standalone
â”‚   â”œâ”€â”€ data/                 # Datasets d'entraÃ®nement
â”‚   â”œâ”€â”€ plots/                # Visualisations (matrices, courbes)
â”‚   â””â”€â”€ static/               # CSS et ressources
â”œâ”€â”€ models/                   # ModÃ¨les sauvegardÃ©s
â”‚   â”œâ”€â”€ ml_model.joblib      # ModÃ¨le ML + vectorizer
â”‚   â”œâ”€â”€ dl_model.h5          # ModÃ¨le DL + tokenizer + encoder
â”‚   â”œâ”€â”€ autoencoder_summarizer.h5  # Autoencodeur rÃ©sumÃ©
â”‚   â””â”€â”€ autoencoder_tokenizer.pkl  # Tokenizer autoencodeur
â”œâ”€â”€ requirements.txt          # DÃ©pendances
â””â”€â”€ README.md                # Documentation
```

## ğŸ› ï¸ Installation et Configuration

### PrÃ©requis
- Python 3.8+
- TensorFlow 2.x
- Scikit-learn
- Streamlit
- NLTK

### Installation
```bash
# Cloner le repository
git clone [URL_DU_REPO]
cd NLP

# CrÃ©er l'environnement virtuel
python -m venv venv
venv\Scripts\activate  # Windows
source venv/bin/activate  # Linux/Mac

# Installer les dÃ©pendances
pip install -r requirements.txt

# TÃ©lÃ©charger les ressources NLTK
python -c "import nltk; nltk.download('stopwords'); nltk.download('wordnet'); nltk.download('punkt')"
```

## ğŸ¯ Utilisation

### EntraÃ®nement des ModÃ¨les
```bash
# EntraÃ®ner tous les modÃ¨les sur le dataset
python app/train_models.py
```

### Interface Utilisateur
```bash
# Lancer l'interface Streamlit
streamlit run app/interface.py
```

### PrÃ©dictions Standalone
```bash
# Utiliser le script de prÃ©diction
python app/predict.py
```

## ğŸ”§ DÃ©tail des FonctionnalitÃ©s

### Classification ML (TF-IDF + Naive Bayes)
- **PrÃ©traitement** : Nettoyage complet (ponctuation, URLs, emails, stopwords, lemmatisation)
- **Vectorisation** : TF-IDF avec bigrammes et optimisation des hyperparamÃ¨tres
- **ModÃ¨le** : Naive Bayes optimisÃ© par GridSearchCV
- **Ã‰valuation** : Matrice de confusion, courbe d'apprentissage, rapport de classification

### Classification DL (LSTM Bidirectionnel)
- **PrÃ©traitement** : MÃªme pipeline que ML + tokenization Keras
- **Architecture** : Embedding â†’ LSTM Bidirectionnel â†’ BatchNormalization â†’ Dense
- **EntraÃ®nement** : Early stopping, validation split, sauvegarde automatique
- **InfÃ©rence** : Chargement du modÃ¨le, tokenizer et encoder

### RÃ©sumÃ© ML (SimilaritÃ© Cosinus)
- **Processus** : DÃ©coupage en phrases â†’ Vectorisation TF-IDF â†’ Calcul similaritÃ© cosinus
- **SÃ©lection** : 3 phrases les plus similaires au texte global
- **PrÃ©servation** : Ordre original des phrases pour la cohÃ©rence narrative

### RÃ©sumÃ© DL (Autoencodeur Extractif)
- **Processus** : DÃ©coupage en phrases â†’ Vectorisation â†’ Autoencodeur â†’ Erreur reconstruction
- **SÃ©lection** : Phrases avec l'erreur de reconstruction la plus faible
- **Architecture** : Embedding â†’ LSTM â†’ Dense â†’ RepeatVector â†’ LSTM â†’ TimeDistributed

### Recherche Wikipedia Intelligente
- **Extraction** : Mots-clÃ©s TF-IDF du texte utilisateur
- **Recherche** : Pages Wikipedia correspondantes
- **Gestion** : AmbiguÃ¯tÃ© avec boutons interactifs
- **RÃ©sultat** : RÃ©sumÃ© Wikipedia formatÃ©

## ğŸ“Š ModÃ¨les et Sauvegarde

### Fichiers SauvegardÃ©s (dossier `models/`)
- `ml_model.joblib` : Pipeline ML complet (TF-IDF + Naive Bayes)
- `vectorizer.joblib` : Vectorizer TF-IDF du modÃ¨le ML
- `dl_model.h5` : ModÃ¨le LSTM bidirectionnel
- `tokenizer.pkl` : Tokenizer du modÃ¨le DL
- `encoder.pkl` : LabelEncoder du modÃ¨le DL
- `autoencoder_summarizer.h5` : Autoencodeur pour le rÃ©sumÃ©
- `autoencoder_tokenizer.pkl` : Tokenizer de l'autoencodeur

### Visualisations (dossier `app/plots/`)
- `ml_confusion_matrix.png` : Matrice de confusion du modÃ¨le ML
- `ml_learning_curve.png` : Courbe d'apprentissage du modÃ¨le ML

## ğŸ¨ Interface Utilisateur

### PersonnalitÃ© du Chatbot
- **Personnage** : Grenouille japonaise (Kaeru)
- **Style** : Messages ponctuÃ©s par "kero" avec actions descriptives
- **Confiance** : RÃ©ponses adaptÃ©es selon le niveau de confiance du modÃ¨le

### Fonctions Disponibles
1. **Classification (Machine Learning)** : PrÃ©diction rapide avec TF-IDF
2. **Classification (Deep Learning)** : PrÃ©diction avancÃ©e avec LSTM
3. **Summarization (Machine Learning)** : RÃ©sumÃ© extractif TF-IDF
4. **Summarization (Deep Learning)** : RÃ©sumÃ© extractif autoencodeur
5. **Wikipedia Search** : Recherche intelligente avec gestion d'ambiguÃ¯tÃ©

## ğŸ”„ Pipeline de DonnÃ©es Complet

### EntraÃ®nement
1. Chargement du dataset CSV
2. Nettoyage (doublons, valeurs manquantes)
3. PrÃ©traitement global (TextPreprocessor)
4. EntraÃ®nement ML (GridSearchCV + Ã©valuation)
5. EntraÃ®nement DL (LSTM + sauvegarde tokenizer/encoder)
6. EntraÃ®nement autoencodeur (phrases du dataset)
7. Sauvegarde de tous les modÃ¨les dans `models/`

### InfÃ©rence
1. RÃ©ception du texte utilisateur
2. PrÃ©traitement adaptÃ© selon la fonction
3. Transformation numÃ©rique (vectorisation/tokenization)
4. PrÃ©diction avec le modÃ¨le appropriÃ©
5. Formatage de la rÃ©ponse avec personnalitÃ©

## ğŸ› DÃ©pannage

### ProblÃ¨mes Courants
- **ModÃ¨les non entraÃ®nÃ©s** : ExÃ©cuter `python app/train_models.py`
- **Erreurs NLTK** : VÃ©rifier le tÃ©lÃ©chargement des ressources
- **Fichiers manquants** : VÃ©rifier la prÃ©sence des modÃ¨les dans `models/`

### Logs et Debug
- Les scripts affichent des messages dÃ©taillÃ©s avec emojis
- Les erreurs sont capturÃ©es et affichÃ©es de maniÃ¨re conviviale
- Les modÃ¨les sont automatiquement rechargÃ©s s'ils existent

## ğŸ“ Contribution

Ce projet est dÃ©veloppÃ© dans le cadre du cours de Natural Language Processing.
- **Enseignant** : Nicolas Miotto
- **Ã‰cole** : Ynov
- **Technologies** : Python, TensorFlow, Scikit-learn, Streamlit, NLTK

## ğŸ“„ Licence

Ce projet est sous licence MIT.

---

**" *hops excitedly* ğŸ¸ Ready to help you with NLP tasks, kero!"**
